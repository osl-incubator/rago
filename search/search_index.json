{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#rago","title":"Rago","text":"<p>Rago is a lightweight framework for RAG</p> <ul> <li>License: BSD 3 Clause</li> <li>Documentation: https://rago.github.io</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li> <p>The security of our code: Bandit is a powerful tool that we use in our Python   project to ensure its security. This tool analyzes the code and detects   potential vulnerabilities. Some of the key features of Bandit are its ease of   use, its ability to integrate with other tools, and its support for multiple   Python versions. If you want to know about bandit you can check its   documentation.</p> </li> <li> <p>Finds unused code: Vulture is   useful for cleaning up and finding errors in large code bases in Python.</p> </li> <li> <p>Complexity of functions and modules: We use   McCabe to identify the complexity in our   Python code that may be difficult to maintain or understand. By identifying   complex code at the outset, we as developers can refactor it to make it easier   to maintain and understand. In summary, McCabe helps us to improve the quality   of our code and make it easier to maintain. If you would like to learn more   about McCabe and code complexity, you can visit   McCabe - Code Complexity Checker.   This tool is included with Flake8.</p> </li> <li> <p>TODO</p> </li> </ul>"},{"location":"#credits","title":"Credits","text":"<p>This package was created with Cookiecutter and the osl-incubator/scicookie project template.</p>"},{"location":"changelog/","title":"Release Notes","text":""},{"location":"changelog/#0110-2025-01-21","title":"0.11.0 (2025-01-21)","text":""},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Add support for extra parameters for RAG generation (#32) (e7a57c1)</li> </ul>"},{"location":"changelog/#0101-2025-01-19","title":"0.10.1 (2025-01-19)","text":""},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix caching (#30) (1ed642c)</li> </ul>"},{"location":"changelog/#0100-2025-01-16","title":"0.10.0 (2025-01-16)","text":""},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li>Add support for caching (#29) (a3bb556)</li> </ul>"},{"location":"changelog/#090-2024-11-21","title":"0.9.0 (2024-11-21)","text":""},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li>Add initial support for structured output (#25) (64856e1)</li> </ul>"},{"location":"changelog/#081-2024-11-19","title":"0.8.1 (2024-11-19)","text":""},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Rename Spacy to SpaCy (#24) (d611271)</li> </ul>"},{"location":"changelog/#080-2024-11-19","title":"0.8.0 (2024-11-19)","text":""},{"location":"changelog/#features_3","title":"Features","text":"<ul> <li>Add an attribute for logs to keep more information for all the steps (#22) (89b19bb)</li> <li>aug: Add support for SpaCy augmented class (#23) (14b4dd5)</li> </ul>"},{"location":"changelog/#071-2024-11-15","title":"0.7.1 (2024-11-15)","text":""},{"location":"changelog/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Remove experimental augmented classes (#21) (e878012)</li> </ul>"},{"location":"changelog/#070-2024-11-14","title":"0.7.0 (2024-11-14)","text":""},{"location":"changelog/#features_4","title":"Features","text":"<ul> <li>Add PDF Retrieval and Text Splitter (#20) (6e80756)</li> </ul>"},{"location":"changelog/#060-2024-11-07","title":"0.6.0 (2024-11-07)","text":""},{"location":"changelog/#features_5","title":"Features","text":"<ul> <li>Add OpenAIAug with embeddings (#19) (227c535)</li> </ul>"},{"location":"changelog/#051-2024-11-04","title":"0.5.1 (2024-11-04)","text":""},{"location":"changelog/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>Remove unnecessary usage of abc.abstractmethod (#18) (565ec05)</li> </ul>"},{"location":"changelog/#050-2024-11-01","title":"0.5.0 (2024-11-01)","text":""},{"location":"changelog/#features_6","title":"Features","text":"<ul> <li>Add attribute for storing intermediate results from the pipeline (#17) (b00c4ae)</li> </ul>"},{"location":"changelog/#040-2024-10-31","title":"0.4.0 (2024-10-31)","text":""},{"location":"changelog/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>Fix packaging issues with torch and fix issues with langdetect (#10) (e4ade58)</li> </ul>"},{"location":"changelog/#features_7","title":"Features","text":"<ul> <li>Implement Gemini Model Integration for Augmentation and Generation (#16) (b418c9b)</li> <li>model: Integrate GPT-4 text generation in Rago (#13) (42078a3)</li> </ul>"},{"location":"changelog/#030-2024-10-24","title":"0.3.0 (2024-10-24)","text":""},{"location":"changelog/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>Fix packaging for gpu and cpu (#8) (42e0716)</li> <li>Improve the usage of typeguard (#5) (25782c1)</li> </ul>"},{"location":"changelog/#features_8","title":"Features","text":"<ul> <li>Add parameter for selecting device (cpu and gpu) (#6) (552d912)</li> <li>Implement type checking with typeguard (#4) (ae7958e)</li> </ul>"},{"location":"changelog/#020-2024-10-23","title":"0.2.0 (2024-10-23)","text":""},{"location":"changelog/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>fix the semantic release workflow (#3) (b238e46)</li> </ul>"},{"location":"changelog/#features_9","title":"Features","text":"<ul> <li>Add support for llama 3.2 generation (#2) (770dd0c)</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>In order to be able to contribute, it is important that you understand the project layout.</p> <p>This project uses the src layout, which means that the package code is located at <code>./src/rago</code>.</p> <p>For my information, check the official documentation: https://packaging.python.org/en/latest/discussions/src-layout-vs-flat-layout/</p> <p>In addition, you should know that to build our package we use Poetry, it's a Python package management tool that simplifies the process of building and publishing Python packages. It allows us to easily manage dependencies, virtual environments and package versions. Poetry also includes features such as dependency resolution, lock files and publishing to PyPI. Overall, Poetry streamlines the process of managing Python packages, making it easier for us to create and share our code with others.</p> <p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/osl-incubator/rago.git/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \u201cbug\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \u201cenhancement\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>Rago could always use more documentation, whether as part of the official Rago docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/osl-incubator/rago.git/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are   welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started","text":"<p>Ready to contribute? Here\u2019s how to set up <code>rago</code> for local development.</p> <ol> <li>Fork the <code>rago</code> repo on GitHub.</li> <li>Clone your fork locally and change to the directory of your project:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/rago.git\n$ cd rago/\n</code></pre> <p>Also, create a remote to the upstream repository, you will need that later:</p> <pre><code>$ git remote add upstream https://github.com/osl-incubator/rago.git\n$ git fetch --all\n</code></pre>"},{"location":"contributing/#prepare-and-use-virtual-environment","title":"Prepare and use virtual environment","text":"<p>If you don't have yet conda installed in your machine, you can check the installation steps here: conda-forge/miniforge?tab=readme-ov-file#download After that, ensure that conda is already available in your terminal session and run:</p> <pre><code>$ conda env create env create --file conda/dev.yaml\n$ conda activate rago\n</code></pre> <p>Note: you can use <code>mamba env create</code> instead, if you have it already installed, in order to boost the installation step.</p>"},{"location":"contributing/#install-the-dependencies","title":"Install the dependencies","text":"<p>Now, you can already install the dependencies for the project:</p> <pre><code>$ poetry install\n</code></pre> <ul> <li><code>rago</code> uses a set of <code>pre-commit</code> hooks to improve code quality. The hooks can   be installed locally using:</li> </ul> <pre><code>$ pre-commit install\n</code></pre> <p>This would run the checks every time a <code>git commit</code> is executed locally. Usually, the verification will only run on the files modified by that commit, but the verification can also be triggered for all the files using:</p> <pre><code>$ pre-commit run --all-files\n</code></pre> <p>If you would like to skip the failing checks and push the code for further discussion, use the <code>--no-verify</code> option with <code>git commit</code>.</p> <p>This project uses <code>pytest</code> as a testing tool. <code>pytest</code> is responsible for testing the code, whose configuration is available in pyproject.toml. Additionally, this project also uses <code>pytest-cov</code> to calculate the coverage of these unit tests. For more information, check the section about tests later in this document.</p>"},{"location":"contributing/#commit-your-changes-and-push-your-branch-to-github","title":"Commit your changes and push your branch to GitHub","text":"<pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ul> <li>Submit a pull request through the GitHub website.</li> </ul>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put your    new functionality into a function with a docstring, and add the feature to    the list in README.rst.</li> <li>The pull request should work for Python &gt;= 3.8.</li> </ol>"},{"location":"contributing/#running-tests-locally","title":"Running tests locally","text":"<p>The tests can be executed using the <code>test</code> dependencies of <code>rago</code> in the following way:</p> <pre><code>$ python -m pytest\n</code></pre>"},{"location":"contributing/#running-tests-with-coverage-locally","title":"Running tests with coverage locally","text":"<p>The coverage value can be obtained while running the tests using <code>pytest-cov</code> in the following way:</p> <pre><code>$ python -m pytest --cov=rago tests/\n</code></pre> <p>A much more detailed guide on testing with <code>pytest</code> is available here.</p>"},{"location":"contributing/#automation-tasks-with-makim","title":"Automation Tasks with Makim","text":"<p>This project uses <code>makim</code> as an automation tool. Please, check the <code>.makim.yaml</code> file to check all the tasks available or run:</p> <pre><code>$ makim --help\n</code></pre>"},{"location":"contributing/#release","title":"Release","text":"<p>This project uses semantic-release in order to cut a new release based on the commit-message.</p>"},{"location":"contributing/#commit-message-format","title":"Commit message format","text":"<p>semantic-release uses the commit messages to determine the consumer impact of changes in the codebase. Following formalized conventions for commit messages, semantic-release automatically determines the next semantic version number, generates a changelog and publishes the release.</p> <p>By default, semantic-release uses Angular Commit Message Conventions. The commit message format can be changed with the <code>preset</code> or <code>config</code> options_ of the @semantic-release/commit-analyzer and @semantic-release/release-notes-generator plugins.</p> <p>Tools such as commitizen or commitlint can be used to help contributors and enforce valid commit messages.</p> <p>The table below shows which commit message gets you which release type when <code>semantic-release</code> runs (using the default configuration):</p> Commit message Release type <code>fix(pencil): stop graphite breaking when pressure is applied</code> Fix Release <code>feat(pencil): add 'graphiteWidth' option</code> Feature Release <code>perf(pencil): remove graphiteWidth option</code> Chore <code>feat(pencil)!: The graphiteWidth option has been removed</code> Breaking Release <p>Note: For a breaking change release, uses <code>!</code> at the end of the message prefix.</p> <p>source: https://github.com/semantic-release/semantic-release/blob/master/README.md#commit-message-format</p> <p>As this project uses the <code>squash and merge</code> strategy, ensure to apply the commit message format to the PR's title.</p>"},{"location":"example/","title":"Rago","text":"In\u00a0[\u00a0]: Copied! <pre>import rago\n</pre> import rago In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/#rago","title":"Rago\u00b6","text":"<p>Rago is Python library that aims to do ...</p>"},{"location":"example/#getting-started","title":"Getting Started\u00b6","text":"<p>First, check our documentation about the installation.</p> <p>Now, let's import our library:</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install Rago, run this command in your terminal:</p> <pre><code>pip install rago\n</code></pre> <p>This is the preferred method to install Rago, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>The sources for Rago can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>git clone https://github.com/osl-incubator/rago.git\n</code></pre> <p>Or download the tarball:</p> <pre><code>curl -OJL https://github.com/osl-incubator/rago.git/tarball/main\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>poetry install\n</code></pre>"},{"location":"api/","title":"Index","text":""},{"location":"api/#rago","title":"rago","text":"<p>Rago.</p> <p>Modules:</p> <ul> <li> <code>augmented</code>           \u2013            <p>Augmented package.</p> </li> <li> <code>base</code>           \u2013            <p>Provide base interfaces.</p> </li> <li> <code>core</code>           \u2013            <p>Rago is Retrieval Augmented Generation lightweight framework.</p> </li> <li> <code>extensions</code>           \u2013            <p>Extra tools for supporting RAG.</p> </li> <li> <code>generation</code>           \u2013            <p>RAG Generation package.</p> </li> <li> <code>retrieval</code>           \u2013            <p>RAG Retrieval package.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>Rago</code>           \u2013            <p>RAG class.</p> </li> </ul>"},{"location":"api/#rago.Rago","title":"Rago","text":"<pre><code>Rago(\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n)\n</code></pre> <p>RAG class.</p> <p>Parameters:</p> <ul> <li> <code>retrieval</code>               (<code>RetrievalBase</code>)           \u2013            <p>The retrieval component used to fetch relevant data based on the query.</p> </li> <li> <code>augmented</code>               (<code>AugmentedBase</code>)           \u2013            <p>The augmentation module responsible for enriching the retrieved data.</p> </li> <li> <code>generation</code>               (<code>GenerationBase</code>)           \u2013            <p>The text generation model used to generate a response based on the query and augmented data.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>prompt</code>             \u2013              <p>Run the pipeline for a specific prompt.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def __init__(\n    self,\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n) -&gt; None:\n    \"\"\"Initialize the RAG structure.\n\n    Parameters\n    ----------\n    retrieval : RetrievalBase\n        The retrieval component used to fetch relevant data based\n        on the query.\n    augmented : AugmentedBase\n        The augmentation module responsible for enriching the\n        retrieved data.\n    generation : GenerationBase\n        The text generation model used to generate a response based\n        on the query and augmented data.\n    \"\"\"\n    self.retrieval = retrieval\n    self.augmented = augmented\n    self.generation = generation\n    self.logs: dict[str, dict[str, Any]] = {\n        'retrieval': retrieval.logs,\n        'augmented': augmented.logs,\n        'generation': generation.logs,\n    }\n</code></pre>"},{"location":"api/#rago.Rago.prompt","title":"prompt","text":"<pre><code>prompt(query: str, device: str = 'auto') -&gt; str | BaseModel\n</code></pre> <p>Run the pipeline for a specific prompt.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query or prompt from the user.</p> </li> <li> <code>device</code>               (<code>str (default 'auto')</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device for generation (e.g., 'auto', 'cpu', 'cuda'), by default 'auto'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on the query and augmented data.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def prompt(self, query: str, device: str = 'auto') -&gt; str | BaseModel:\n    \"\"\"Run the pipeline for a specific prompt.\n\n    Parameters\n    ----------\n    query : str\n        The query or prompt from the user.\n    device : str (default 'auto')\n        Device for generation (e.g., 'auto', 'cpu', 'cuda'), by\n        default 'auto'.\n\n    Returns\n    -------\n    str\n        Generated text based on the query and augmented data.\n    \"\"\"\n    ret_data = self.retrieval.get(query)\n    self.logs['retrieval']['result'] = ret_data\n\n    aug_data = self.augmented.search(query, ret_data)\n    self.logs['augmented']['result'] = aug_data\n\n    gen_data = self.generation.generate(query, context=aug_data)\n    self.logs['generation']['result'] = gen_data\n\n    return gen_data\n</code></pre>"},{"location":"api/#rago.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the program version.</p> Source code in <code>src/rago/__init__.py</code> <pre><code>def get_version() -&gt; str:\n    \"\"\"Return the program version.\"\"\"\n    try:\n        return importlib_metadata.version(__name__)\n    except importlib_metadata.PackageNotFoundError:  # pragma: no cover\n        return '0.11.0'  # semantic-release\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li> rago<ul> <li> augmented<ul> <li> base</li> <li> db<ul> <li> base</li> <li> faiss</li> </ul> </li> <li> openai</li> <li> sentence_transformer</li> <li> spacy</li> </ul> </li> <li> base</li> <li> core</li> <li> extensions<ul> <li> base</li> <li> cache</li> </ul> </li> <li> generation<ul> <li> base</li> <li> gemini</li> <li> hugging_face</li> <li> llama</li> <li> openai</li> </ul> </li> <li> retrieval<ul> <li> base</li> <li> file</li> <li> text_splitter<ul> <li> base</li> <li> langchain</li> </ul> </li> <li> tools<ul> <li> pdf</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"api/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/base/#rago.base","title":"base","text":"<p>Provide base interfaces.</p> <p>Classes:</p> <ul> <li> <code>RagoBase</code>           \u2013            <p>Define base interface for RAG step classes.</p> </li> </ul>"},{"location":"api/base/#rago.base.RagoBase","title":"RagoBase","text":"<pre><code>RagoBase(\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = {},\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Define base interface for RAG step classes.</p> Source code in <code>src/rago/base.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = {},\n) -&gt; None:\n    self.api_key = api_key\n    self.cache = cache\n    self.logs = logs\n</code></pre>"},{"location":"api/core/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> core","text":""},{"location":"api/core/#rago.core","title":"core","text":"<p>Rago is Retrieval Augmented Generation lightweight framework.</p> <p>Classes:</p> <ul> <li> <code>Rago</code>           \u2013            <p>RAG class.</p> </li> </ul>"},{"location":"api/core/#rago.core.Rago","title":"Rago","text":"<pre><code>Rago(\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n)\n</code></pre> <p>RAG class.</p> <p>Parameters:</p> <ul> <li> <code>retrieval</code>               (<code>RetrievalBase</code>)           \u2013            <p>The retrieval component used to fetch relevant data based on the query.</p> </li> <li> <code>augmented</code>               (<code>AugmentedBase</code>)           \u2013            <p>The augmentation module responsible for enriching the retrieved data.</p> </li> <li> <code>generation</code>               (<code>GenerationBase</code>)           \u2013            <p>The text generation model used to generate a response based on the query and augmented data.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>prompt</code>             \u2013              <p>Run the pipeline for a specific prompt.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def __init__(\n    self,\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n) -&gt; None:\n    \"\"\"Initialize the RAG structure.\n\n    Parameters\n    ----------\n    retrieval : RetrievalBase\n        The retrieval component used to fetch relevant data based\n        on the query.\n    augmented : AugmentedBase\n        The augmentation module responsible for enriching the\n        retrieved data.\n    generation : GenerationBase\n        The text generation model used to generate a response based\n        on the query and augmented data.\n    \"\"\"\n    self.retrieval = retrieval\n    self.augmented = augmented\n    self.generation = generation\n    self.logs: dict[str, dict[str, Any]] = {\n        'retrieval': retrieval.logs,\n        'augmented': augmented.logs,\n        'generation': generation.logs,\n    }\n</code></pre>"},{"location":"api/core/#rago.core.Rago.prompt","title":"prompt","text":"<pre><code>prompt(query: str, device: str = 'auto') -&gt; str | BaseModel\n</code></pre> <p>Run the pipeline for a specific prompt.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query or prompt from the user.</p> </li> <li> <code>device</code>               (<code>str (default 'auto')</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device for generation (e.g., 'auto', 'cpu', 'cuda'), by default 'auto'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on the query and augmented data.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def prompt(self, query: str, device: str = 'auto') -&gt; str | BaseModel:\n    \"\"\"Run the pipeline for a specific prompt.\n\n    Parameters\n    ----------\n    query : str\n        The query or prompt from the user.\n    device : str (default 'auto')\n        Device for generation (e.g., 'auto', 'cpu', 'cuda'), by\n        default 'auto'.\n\n    Returns\n    -------\n    str\n        Generated text based on the query and augmented data.\n    \"\"\"\n    ret_data = self.retrieval.get(query)\n    self.logs['retrieval']['result'] = ret_data\n\n    aug_data = self.augmented.search(query, ret_data)\n    self.logs['augmented']['result'] = aug_data\n\n    gen_data = self.generation.generate(query, context=aug_data)\n    self.logs['generation']['result'] = gen_data\n\n    return gen_data\n</code></pre>"},{"location":"api/augmented/","title":"Index","text":""},{"location":"api/augmented/#rago.augmented","title":"augmented","text":"<p>Augmented package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for the augmented step.</p> </li> <li> <code>db</code>           \u2013            <p>Rago DB package.</p> </li> <li> <code>openai</code>           \u2013            <p>Classes for augmentation with OpenAI embeddings.</p> </li> <li> <code>sentence_transformer</code>           \u2013            <p>Classes for augmentation with hugging face.</p> </li> <li> <code>spacy</code>           \u2013            <p>Classes for augmentation with SpaCy embeddings.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>AugmentedBase</code>           \u2013            <p>Define the base structure for Augmented classes.</p> </li> <li> <code>OpenAIAug</code>           \u2013            <p>Class for augmentation with OpenAI embeddings.</p> </li> <li> <code>SentenceTransformerAug</code>           \u2013            <p>Class for augmentation with Hugging Face.</p> </li> <li> <code>SpaCyAug</code>           \u2013            <p>Class for augmentation with SpaCy embeddings.</p> </li> </ul>"},{"location":"api/augmented/#rago.augmented.AugmentedBase","title":"AugmentedBase","text":"<pre><code>AugmentedBase(\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Define the base structure for Augmented classes.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k or self.default_top_k\n    self.model_name = model_name or self.default_model_name\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.AugmentedBase.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    raise Exception('Method not implemented.')\n</code></pre>"},{"location":"api/augmented/#rago.augmented.AugmentedBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(\n    query: str, documents: Any, top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self,\n    query: str,\n    documents: Any,\n    top_k: int = 0,\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/#rago.augmented.OpenAIAug","title":"OpenAIAug","text":"<pre><code>OpenAIAug(\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with OpenAI embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k or self.default_top_k\n    self.model_name = model_name or self.default_model_name\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.OpenAIAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(openai.OpenAI, self.model)\n    response = model.embeddings.create(\n        input=content, model=self.model_name\n    )\n    result = np.array(response.data[0].embedding)\n    result = result.reshape(1, result.size)\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/augmented/#rago.augmented.OpenAIAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    # Encode the documents and query\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SentenceTransformerAug","title":"SentenceTransformerAug","text":"<pre><code>SentenceTransformerAug(\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Hugging Face.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k or self.default_top_k\n    self.model_name = model_name or self.default_model_name\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SentenceTransformerAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/sentence_transformer.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    model = cast(SentenceTransformer, self.model)\n    return model.encode(content)\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SentenceTransformerAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: Any, top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/sentence_transformer.py</code> <pre><code>def search(self, query: str, documents: Any, top_k: int = 0) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    retrieved_docs = [documents[i] for i in indices]\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SpaCyAug","title":"SpaCyAug","text":"<pre><code>SpaCyAug(\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with SpaCy embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using SpaCy.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k or self.default_top_k\n    self.model_name = model_name or self.default_model_name\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SpaCyAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: List[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using SpaCy.</p> Source code in <code>src/rago/augmented/spacy.py</code> <pre><code>def get_embedding(self, content: List[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using SpaCy.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(spacy.language.Language, self.model)\n    embeddings = []\n    for text in content:\n        doc = model(text)\n        embeddings.append(doc.vector)\n    result = np.array(embeddings)\n    self._save_cache(cache_key, result)\n    return result\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SpaCyAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/spacy.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    # Encode the documents and query\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/augmented/base/#rago.augmented.base","title":"base","text":"<p>Base classes for the augmented step.</p> <p>Classes:</p> <ul> <li> <code>AugmentedBase</code>           \u2013            <p>Define the base structure for Augmented classes.</p> </li> </ul>"},{"location":"api/augmented/base/#rago.augmented.base.AugmentedBase","title":"AugmentedBase","text":"<pre><code>AugmentedBase(\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Define the base structure for Augmented classes.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k or self.default_top_k\n    self.model_name = model_name or self.default_model_name\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/base/#rago.augmented.base.AugmentedBase.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    raise Exception('Method not implemented.')\n</code></pre>"},{"location":"api/augmented/base/#rago.augmented.base.AugmentedBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(\n    query: str, documents: Any, top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self,\n    query: str,\n    documents: Any,\n    top_k: int = 0,\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/openai/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> openai","text":""},{"location":"api/augmented/openai/#rago.augmented.openai","title":"openai","text":"<p>Classes for augmentation with OpenAI embeddings.</p> <p>Classes:</p> <ul> <li> <code>OpenAIAug</code>           \u2013            <p>Class for augmentation with OpenAI embeddings.</p> </li> </ul>"},{"location":"api/augmented/openai/#rago.augmented.openai.OpenAIAug","title":"OpenAIAug","text":"<pre><code>OpenAIAug(\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with OpenAI embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k or self.default_top_k\n    self.model_name = model_name or self.default_model_name\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/openai/#rago.augmented.openai.OpenAIAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(openai.OpenAI, self.model)\n    response = model.embeddings.create(\n        input=content, model=self.model_name\n    )\n    result = np.array(response.data[0].embedding)\n    result = result.reshape(1, result.size)\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/augmented/openai/#rago.augmented.openai.OpenAIAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    # Encode the documents and query\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/sentence_transformer/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> sentence_transformer","text":""},{"location":"api/augmented/sentence_transformer/#rago.augmented.sentence_transformer","title":"sentence_transformer","text":"<p>Classes for augmentation with hugging face.</p> <p>Classes:</p> <ul> <li> <code>SentenceTransformerAug</code>           \u2013            <p>Class for augmentation with Hugging Face.</p> </li> </ul>"},{"location":"api/augmented/sentence_transformer/#rago.augmented.sentence_transformer.SentenceTransformerAug","title":"SentenceTransformerAug","text":"<pre><code>SentenceTransformerAug(\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Hugging Face.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k or self.default_top_k\n    self.model_name = model_name or self.default_model_name\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/sentence_transformer/#rago.augmented.sentence_transformer.SentenceTransformerAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/sentence_transformer.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    model = cast(SentenceTransformer, self.model)\n    return model.encode(content)\n</code></pre>"},{"location":"api/augmented/sentence_transformer/#rago.augmented.sentence_transformer.SentenceTransformerAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: Any, top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/sentence_transformer.py</code> <pre><code>def search(self, query: str, documents: Any, top_k: int = 0) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    retrieved_docs = [documents[i] for i in indices]\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/spacy/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> spacy","text":""},{"location":"api/augmented/spacy/#rago.augmented.spacy","title":"spacy","text":"<p>Classes for augmentation with SpaCy embeddings.</p> <p>Classes:</p> <ul> <li> <code>SpaCyAug</code>           \u2013            <p>Class for augmentation with SpaCy embeddings.</p> </li> </ul>"},{"location":"api/augmented/spacy/#rago.augmented.spacy.SpaCyAug","title":"SpaCyAug","text":"<pre><code>SpaCyAug(\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with SpaCy embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using SpaCy.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    db: DBBase = FaissDB(),\n    top_k: int = 0,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k or self.default_top_k\n    self.model_name = model_name or self.default_model_name\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/spacy/#rago.augmented.spacy.SpaCyAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: List[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using SpaCy.</p> Source code in <code>src/rago/augmented/spacy.py</code> <pre><code>def get_embedding(self, content: List[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using SpaCy.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(spacy.language.Language, self.model)\n    embeddings = []\n    for text in content:\n        doc = model(text)\n        embeddings.append(doc.vector)\n    result = np.array(embeddings)\n    self._save_cache(cache_key, result)\n    return result\n</code></pre>"},{"location":"api/augmented/spacy/#rago.augmented.spacy.SpaCyAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/spacy.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    # Encode the documents and query\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/db/","title":"Index","text":""},{"location":"api/augmented/db/#rago.augmented.db","title":"db","text":"<p>Rago DB package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for database.</p> </li> <li> <code>faiss</code>           \u2013            <p>Module for faiss database.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>DBBase</code>           \u2013            <p>Base class for vector database.</p> </li> <li> <code>FaissDB</code>           \u2013            <p>Faiss Database.</p> </li> </ul>"},{"location":"api/augmented/db/#rago.augmented.db.DBBase","title":"DBBase","text":"<p>Base class for vector database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search a query from documents.</p> </li> </ul>"},{"location":"api/augmented/db/#rago.augmented.db.DBBase.embed","title":"embed  <code>abstractmethod</code>","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/base.py</code> <pre><code>@abstractmethod\ndef embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/db/#rago.augmented.db.DBBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search a query from documents.</p> Source code in <code>src/rago/augmented/db/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search a query from documents.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/db/#rago.augmented.db.FaissDB","title":"FaissDB","text":"<p>               Bases: <code>DBBase</code></p> <p>Faiss Database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul>"},{"location":"api/augmented/db/#rago.augmented.db.FaissDB.embed","title":"embed","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/faiss.py</code> <pre><code>def embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    self.index = faiss.IndexFlatL2(documents.shape[1])\n    self.index.add(documents)\n</code></pre>"},{"location":"api/augmented/db/#rago.augmented.db.FaissDB.search","title":"search","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/db/faiss.py</code> <pre><code>def search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    distances, indices = self.index.search(query_encoded, top_k)\n    return distances, indices[0]\n</code></pre>"},{"location":"api/augmented/db/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/augmented/db/base/#rago.augmented.db.base","title":"base","text":"<p>Base classes for database.</p> <p>Classes:</p> <ul> <li> <code>DBBase</code>           \u2013            <p>Base class for vector database.</p> </li> </ul>"},{"location":"api/augmented/db/base/#rago.augmented.db.base.DBBase","title":"DBBase","text":"<p>Base class for vector database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search a query from documents.</p> </li> </ul>"},{"location":"api/augmented/db/base/#rago.augmented.db.base.DBBase.embed","title":"embed  <code>abstractmethod</code>","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/base.py</code> <pre><code>@abstractmethod\ndef embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/db/base/#rago.augmented.db.base.DBBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search a query from documents.</p> Source code in <code>src/rago/augmented/db/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search a query from documents.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/db/faiss/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> faiss","text":""},{"location":"api/augmented/db/faiss/#rago.augmented.db.faiss","title":"faiss","text":"<p>Module for faiss database.</p> <p>Classes:</p> <ul> <li> <code>FaissDB</code>           \u2013            <p>Faiss Database.</p> </li> </ul>"},{"location":"api/augmented/db/faiss/#rago.augmented.db.faiss.FaissDB","title":"FaissDB","text":"<p>               Bases: <code>DBBase</code></p> <p>Faiss Database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul>"},{"location":"api/augmented/db/faiss/#rago.augmented.db.faiss.FaissDB.embed","title":"embed","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/faiss.py</code> <pre><code>def embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    self.index = faiss.IndexFlatL2(documents.shape[1])\n    self.index.add(documents)\n</code></pre>"},{"location":"api/augmented/db/faiss/#rago.augmented.db.faiss.FaissDB.search","title":"search","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/db/faiss.py</code> <pre><code>def search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    distances, indices = self.index.search(query_encoded, top_k)\n    return distances, indices[0]\n</code></pre>"},{"location":"api/extensions/","title":"Index","text":""},{"location":"api/extensions/#rago.extensions","title":"extensions","text":"<p>Extra tools for supporting RAG.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Extension base module.</p> </li> <li> <code>cache</code>           \u2013            <p>Provide an extension for caching.</p> </li> </ul>"},{"location":"api/extensions/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/extensions/base/#rago.extensions.base","title":"base","text":"<p>Extension base module.</p> <p>Classes:</p> <ul> <li> <code>Extension</code>           \u2013            <p>Define base interface for RAG Extension classes.</p> </li> </ul>"},{"location":"api/extensions/base/#rago.extensions.base.Extension","title":"Extension","text":"<p>               Bases: <code>ABC</code></p> <p>Define base interface for RAG Extension classes.</p>"},{"location":"api/extensions/cache/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> cache","text":""},{"location":"api/extensions/cache/#rago.extensions.cache","title":"cache","text":"<p>Provide an extension for caching.</p> <p>Classes:</p> <ul> <li> <code>Cache</code>           \u2013            <p>Define an extension base for caching.</p> </li> <li> <code>CacheFile</code>           \u2013            <p>Define a extra step for caching.</p> </li> </ul>"},{"location":"api/extensions/cache/#rago.extensions.cache.Cache","title":"Cache","text":"<p>               Bases: <code>Extension</code></p> <p>Define an extension base for caching.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load the cache for given key.</p> </li> <li> <code>save</code>             \u2013              <p>Save the cache for given key.</p> </li> </ul>"},{"location":"api/extensions/cache/#rago.extensions.cache.Cache.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(key: Any) -&gt; Any\n</code></pre> <p>Load the cache for given key.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>@abstractmethod\ndef load(self, key: Any) -&gt; Any:\n    \"\"\"Load the cache for given key.\"\"\"\n    raise Exception(f'Load method is not implemented: {key}')\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.Cache.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(key: Any, data: Any) -&gt; None\n</code></pre> <p>Save the cache for given key.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>@abstractmethod\ndef save(self, key: Any, data: Any) -&gt; None:\n    \"\"\"Save the cache for given key.\"\"\"\n    raise Exception(f'Save method is not implemented: {key}')\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.CacheFile","title":"CacheFile","text":"<pre><code>CacheFile(target_dir: Path)\n</code></pre> <p>               Bases: <code>Cache</code></p> <p>Define a extra step for caching.</p> <p>Methods:</p> <ul> <li> <code>get_file_path</code>             \u2013              <p>Return the file path.</p> </li> <li> <code>load</code>             \u2013              <p>Load the cache for given key.</p> </li> <li> <code>save</code>             \u2013              <p>Load the cache for given key.</p> </li> </ul> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>def __init__(self, target_dir: Path) -&gt; None:\n    self.target_dir = target_dir\n    self.target_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.CacheFile.get_file_path","title":"get_file_path","text":"<pre><code>get_file_path(key: Any) -&gt; Path\n</code></pre> <p>Return the file path.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>def get_file_path(self, key: Any) -&gt; Path:\n    \"\"\"Return the file path.\"\"\"\n    ref = sha256(str(key).encode('utf-8')).hexdigest()\n    return self.target_dir / f'{ref}.pkl'\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.CacheFile.load","title":"load","text":"<pre><code>load(key: Any) -&gt; Any\n</code></pre> <p>Load the cache for given key.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>def load(self, key: Any) -&gt; Any:\n    \"\"\"Load the cache for given key.\"\"\"\n    file_path = self.get_file_path(key)\n    if not file_path.exists():\n        return\n    return joblib.load(file_path)\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.CacheFile.save","title":"save","text":"<pre><code>save(key: Any, data: Any) -&gt; None\n</code></pre> <p>Load the cache for given key.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>def save(self, key: Any, data: Any) -&gt; None:\n    \"\"\"Load the cache for given key.\"\"\"\n    file_path = self.get_file_path(key)\n    joblib.dump(data, file_path)\n</code></pre>"},{"location":"api/generation/","title":"Index","text":""},{"location":"api/generation/#rago.generation","title":"generation","text":"<p>RAG Generation package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for generation.</p> </li> <li> <code>gemini</code>           \u2013            <p>GeminiGen class for text generation using Google's Gemini model.</p> </li> <li> <code>hugging_face</code>           \u2013            <p>Hugging Face classes for text generation.</p> </li> <li> <code>llama</code>           \u2013            <p>Llama generation module.</p> </li> <li> <code>openai</code>           \u2013            <p>OpenAI Generation Model class for flexible GPT-based text generation.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>GeminiGen</code>           \u2013            <p>Gemini generation model for text generation.</p> </li> <li> <code>GenerationBase</code>           \u2013            <p>Generic Generation class.</p> </li> <li> <code>HuggingFaceGen</code>           \u2013            <p>HuggingFaceGen.</p> </li> <li> <code>LlamaGen</code>           \u2013            <p>Llama Generation class.</p> </li> <li> <code>OpenAIGen</code>           \u2013            <p>OpenAI generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/#rago.generation.GeminiGen","title":"GeminiGen","text":"<pre><code>GeminiGen(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Gemini generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Gemini model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.GeminiGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Gemini model support.</p> Source code in <code>src/rago/generation/gemini.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Gemini model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.structured_output:\n        models_params_gen = {'contents': input_text}\n        response = self.model.generate_content(**models_params_gen)\n        self.logs['model_params'] = models_params_gen\n        return cast(str, response.text.strip())\n\n    messages = [\n        {'role': 'user', 'content': input_text},\n    ]\n    model_params = {\n        'messages': messages,\n        'response_model': self.structured_output,\n    }\n\n    response = self.model.create(\n        **model_params,\n    )\n\n    self.logs['model_params'] = model_params\n\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/#rago.generation.GenerationBase","title":"GenerationBase","text":"<pre><code>GenerationBase(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Generic Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text with optional language parameter.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.GenerationBase.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text with optional language parameter.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The input query or prompt.</p> </li> <li> <code>context</code>               (<code>list[str]</code>)           \u2013            <p>Additional context information for the generation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on query and context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text with optional language parameter.\n\n    Parameters\n    ----------\n    query : str\n        The input query or prompt.\n    context : list[str]\n        Additional context information for the generation.\n\n    Returns\n    -------\n    str\n        Generated text based on query and context.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/generation/#rago.generation.HuggingFaceGen","title":"HuggingFaceGen","text":"<pre><code>HuggingFaceGen(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>HuggingFaceGen.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate the text from the query and augmented context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.HuggingFaceGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate the text from the query and augmented context.</p> Source code in <code>src/rago/generation/hugging_face.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate the text from the query and augmented context.\"\"\"\n    with torch.no_grad():\n        input_text = self.prompt_template.format(\n            query=query, context=' '.join(context)\n        )\n        input_ids = self.tokenizer.encode(\n            input_text,\n            return_tensors='pt',\n            truncation=True,\n            max_length=512,\n        ).to(self.device_name)\n\n        model_params = dict(\n            inputs=input_ids,\n            max_length=self.output_max_length,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n\n        outputs = self.model.generate(**model_params)\n\n        self.logs['model_params'] = model_params\n\n        response = self.tokenizer.decode(\n            outputs[0], skip_special_tokens=True\n        )\n\n    if self.device_name == 'cuda':\n        torch.cuda.empty_cache()\n\n    return str(response)\n</code></pre>"},{"location":"api/generation/#rago.generation.LlamaGen","title":"LlamaGen","text":"<pre><code>LlamaGen(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Llama Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Llama model with language support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.LlamaGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Llama model with language support.</p> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Llama model with language support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    # Detect and set the language code for multilingual models (optional)\n    language = str(detect(query)) or 'en'\n    self.tokenizer.lang_code = language\n\n    # Generate the response with adjusted parameters\n\n    model_params = dict(\n        text_inputs=input_text,\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n        top_k=50,  # todo: check if it is necessary\n        top_p=0.95,  # todo: check if it is necessary\n        num_return_sequences=1,\n        eos_token_id=self.tokenizer.eos_token_id,\n    )\n    response = self.generator(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    # Extract and return the answer only\n    answer = str(response[0].get('generated_text', ''))\n    # Strip off any redundant text after the answer itself\n    return answer.split('Answer:')[-1].strip()\n</code></pre>"},{"location":"api/generation/#rago.generation.OpenAIGen","title":"OpenAIGen","text":"<pre><code>OpenAIGen(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>OpenAI generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using OpenAI's API with dynamic model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.OpenAIGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using OpenAI's API with dynamic model support.</p> Source code in <code>src/rago/generation/openai.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text using OpenAI's API with dynamic model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    api_params = (\n        self.api_params if self.api_params else self.default_api_params\n    )\n\n    model_params = dict(\n        model=self.model_name,\n        messages=[{'role': 'user', 'content': input_text}],\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n        **api_params,\n    )\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n\n    response = self.model.chat.completions.create(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    has_choices = hasattr(response, 'choices')\n\n    if has_choices and isinstance(response.choices, list):\n        return cast(str, response.choices[0].message.content.strip())\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/generation/base/#rago.generation.base","title":"base","text":"<p>Base classes for generation.</p> <p>Classes:</p> <ul> <li> <code>GenerationBase</code>           \u2013            <p>Generic Generation class.</p> </li> </ul>"},{"location":"api/generation/base/#rago.generation.base.GenerationBase","title":"GenerationBase","text":"<pre><code>GenerationBase(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Generic Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text with optional language parameter.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/base/#rago.generation.base.GenerationBase.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text with optional language parameter.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The input query or prompt.</p> </li> <li> <code>context</code>               (<code>list[str]</code>)           \u2013            <p>Additional context information for the generation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on query and context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text with optional language parameter.\n\n    Parameters\n    ----------\n    query : str\n        The input query or prompt.\n    context : list[str]\n        Additional context information for the generation.\n\n    Returns\n    -------\n    str\n        Generated text based on query and context.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/generation/gemini/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> gemini","text":""},{"location":"api/generation/gemini/#rago.generation.gemini","title":"gemini","text":"<p>GeminiGen class for text generation using Google's Gemini model.</p> <p>Classes:</p> <ul> <li> <code>GeminiGen</code>           \u2013            <p>Gemini generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/gemini/#rago.generation.gemini.GeminiGen","title":"GeminiGen","text":"<pre><code>GeminiGen(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Gemini generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Gemini model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/gemini/#rago.generation.gemini.GeminiGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Gemini model support.</p> Source code in <code>src/rago/generation/gemini.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Gemini model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.structured_output:\n        models_params_gen = {'contents': input_text}\n        response = self.model.generate_content(**models_params_gen)\n        self.logs['model_params'] = models_params_gen\n        return cast(str, response.text.strip())\n\n    messages = [\n        {'role': 'user', 'content': input_text},\n    ]\n    model_params = {\n        'messages': messages,\n        'response_model': self.structured_output,\n    }\n\n    response = self.model.create(\n        **model_params,\n    )\n\n    self.logs['model_params'] = model_params\n\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/hugging_face/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> hugging_face","text":""},{"location":"api/generation/hugging_face/#rago.generation.hugging_face","title":"hugging_face","text":"<p>Hugging Face classes for text generation.</p> <p>Classes:</p> <ul> <li> <code>HuggingFaceGen</code>           \u2013            <p>HuggingFaceGen.</p> </li> </ul>"},{"location":"api/generation/hugging_face/#rago.generation.hugging_face.HuggingFaceGen","title":"HuggingFaceGen","text":"<pre><code>HuggingFaceGen(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>HuggingFaceGen.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate the text from the query and augmented context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/hugging_face/#rago.generation.hugging_face.HuggingFaceGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate the text from the query and augmented context.</p> Source code in <code>src/rago/generation/hugging_face.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate the text from the query and augmented context.\"\"\"\n    with torch.no_grad():\n        input_text = self.prompt_template.format(\n            query=query, context=' '.join(context)\n        )\n        input_ids = self.tokenizer.encode(\n            input_text,\n            return_tensors='pt',\n            truncation=True,\n            max_length=512,\n        ).to(self.device_name)\n\n        model_params = dict(\n            inputs=input_ids,\n            max_length=self.output_max_length,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n\n        outputs = self.model.generate(**model_params)\n\n        self.logs['model_params'] = model_params\n\n        response = self.tokenizer.decode(\n            outputs[0], skip_special_tokens=True\n        )\n\n    if self.device_name == 'cuda':\n        torch.cuda.empty_cache()\n\n    return str(response)\n</code></pre>"},{"location":"api/generation/llama/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> llama","text":""},{"location":"api/generation/llama/#rago.generation.llama","title":"llama","text":"<p>Llama generation module.</p> <p>Classes:</p> <ul> <li> <code>LlamaGen</code>           \u2013            <p>Llama Generation class.</p> </li> </ul>"},{"location":"api/generation/llama/#rago.generation.llama.LlamaGen","title":"LlamaGen","text":"<pre><code>LlamaGen(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Llama Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Llama model with language support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/llama/#rago.generation.llama.LlamaGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Llama model with language support.</p> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Llama model with language support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    # Detect and set the language code for multilingual models (optional)\n    language = str(detect(query)) or 'en'\n    self.tokenizer.lang_code = language\n\n    # Generate the response with adjusted parameters\n\n    model_params = dict(\n        text_inputs=input_text,\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n        top_k=50,  # todo: check if it is necessary\n        top_p=0.95,  # todo: check if it is necessary\n        num_return_sequences=1,\n        eos_token_id=self.tokenizer.eos_token_id,\n    )\n    response = self.generator(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    # Extract and return the answer only\n    answer = str(response[0].get('generated_text', ''))\n    # Strip off any redundant text after the answer itself\n    return answer.split('Answer:')[-1].strip()\n</code></pre>"},{"location":"api/generation/openai/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> openai","text":""},{"location":"api/generation/openai/#rago.generation.openai","title":"openai","text":"<p>OpenAI Generation Model class for flexible GPT-based text generation.</p> <p>Classes:</p> <ul> <li> <code>OpenAIGen</code>           \u2013            <p>OpenAI generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/openai/#rago.generation.openai.OpenAIGen","title":"OpenAIGen","text":"<pre><code>OpenAIGen(\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>OpenAI generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using OpenAI's API with dynamic model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = model_name or self.default_model_name\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = temperature or self.default_temperature\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/openai/#rago.generation.openai.OpenAIGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using OpenAI's API with dynamic model support.</p> Source code in <code>src/rago/generation/openai.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text using OpenAI's API with dynamic model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    api_params = (\n        self.api_params if self.api_params else self.default_api_params\n    )\n\n    model_params = dict(\n        model=self.model_name,\n        messages=[{'role': 'user', 'content': input_text}],\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n        **api_params,\n    )\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n\n    response = self.model.chat.completions.create(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    has_choices = hasattr(response, 'choices')\n\n    if has_choices and isinstance(response.choices, list):\n        return cast(str, response.choices[0].message.content.strip())\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/retrieval/","title":"Index","text":""},{"location":"api/retrieval/#rago.retrieval","title":"retrieval","text":"<p>RAG Retrieval package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for retrieval.</p> </li> <li> <code>file</code>           \u2013            <p>Base classes for retrieval.</p> </li> <li> <code>text_splitter</code>           \u2013            <p>Package for classes about text splitter.</p> </li> <li> <code>tools</code>           \u2013            <p>Tools for support retrieval classes.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>PDFPathRet</code>           \u2013            <p>PDFPathRet Retrieval class.</p> </li> <li> <code>RetrievalBase</code>           \u2013            <p>Base Retrieval class.</p> </li> <li> <code>StringRet</code>           \u2013            <p>String Retrieval class.</p> </li> </ul>"},{"location":"api/retrieval/#rago.retrieval.PDFPathRet","title":"PDFPathRet","text":"<pre><code>PDFPathRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>FilePathRet</code></p> <p>PDFPathRet Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.PDFPathRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/file.py</code> <pre><code>def get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    cache_key = self.source\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(Iterable[str], cached)\n\n    text = extract_text_from_pdf(self.source)\n\n    self.logs['text'] = text\n\n    result = self.splitter.split(text)\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.RetrievalBase","title":"RetrievalBase","text":"<pre><code>RetrievalBase(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Base Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.RetrievalBase.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>@abstractmethod\ndef get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.StringRet","title":"StringRet","text":"<pre><code>StringRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RetrievalBase</code></p> <p>String Retrieval class.</p> <p>This is a very generic class that assumes that the input (source) is already a list of strings.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the sources.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.StringRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the sources.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the sources.\"\"\"\n    return cast(list[str], self.source)\n</code></pre>"},{"location":"api/retrieval/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/retrieval/base/#rago.retrieval.base","title":"base","text":"<p>Base classes for retrieval.</p> <p>Classes:</p> <ul> <li> <code>RetrievalBase</code>           \u2013            <p>Base Retrieval class.</p> </li> <li> <code>StringRet</code>           \u2013            <p>String Retrieval class.</p> </li> </ul>"},{"location":"api/retrieval/base/#rago.retrieval.base.RetrievalBase","title":"RetrievalBase","text":"<pre><code>RetrievalBase(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Base Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/base/#rago.retrieval.base.RetrievalBase.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>@abstractmethod\ndef get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/base/#rago.retrieval.base.StringRet","title":"StringRet","text":"<pre><code>StringRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RetrievalBase</code></p> <p>String Retrieval class.</p> <p>This is a very generic class that assumes that the input (source) is already a list of strings.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the sources.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/base/#rago.retrieval.base.StringRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the sources.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the sources.\"\"\"\n    return cast(list[str], self.source)\n</code></pre>"},{"location":"api/retrieval/file/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> file","text":""},{"location":"api/retrieval/file/#rago.retrieval.file","title":"file","text":"<p>Base classes for retrieval.</p> <p>Classes:</p> <ul> <li> <code>FilePathRet</code>           \u2013            <p>File Retrieval class.</p> </li> <li> <code>PDFPathRet</code>           \u2013            <p>PDFPathRet Retrieval class.</p> </li> </ul>"},{"location":"api/retrieval/file/#rago.retrieval.file.FilePathRet","title":"FilePathRet","text":"<pre><code>FilePathRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RetrievalBase</code></p> <p>File Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/file/#rago.retrieval.file.FilePathRet.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>@abstractmethod\ndef get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/file/#rago.retrieval.file.PDFPathRet","title":"PDFPathRet","text":"<pre><code>PDFPathRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>FilePathRet</code></p> <p>PDFPathRet Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/file/#rago.retrieval.file.PDFPathRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/file.py</code> <pre><code>def get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    cache_key = self.source\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(Iterable[str], cached)\n\n    text = extract_text_from_pdf(self.source)\n\n    self.logs['text'] = text\n\n    result = self.splitter.split(text)\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/retrieval/text_splitter/","title":"Index","text":""},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter","title":"text_splitter","text":"<p>Package for classes about text splitter.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>The base classes for text splitter.</p> </li> <li> <code>langchain</code>           \u2013            <p>Support langchain text splitter.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>LangChainTextSplitter</code>           \u2013            <p>LangChain Text Splitter class.</p> </li> <li> <code>TextSplitterBase</code>           \u2013            <p>The base text splitter class.</p> </li> </ul>"},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter.LangChainTextSplitter","title":"LangChainTextSplitter","text":"<pre><code>LangChainTextSplitter(\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n)\n</code></pre> <p>               Bases: <code>TextSplitterBase</code></p> <p>LangChain Text Splitter class.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split text into smaller chunks for processing.</p> </li> </ul> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>def __init__(\n    self,\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n) -&gt; None:\n    \"\"\"Initialize the text splitter class.\"\"\"\n    self.chunk_size = chunk_size or self.default_chunk_size\n    self.chunk_overlap = chunk_overlap or self.default_chunk_overlap\n    self.splitter_name = splitter_name or self.default_splitter_name\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter.LangChainTextSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; list[str]\n</code></pre> <p>Split text into smaller chunks for processing.</p> Source code in <code>src/rago/retrieval/text_splitter/langchain.py</code> <pre><code>def split(self, text: str) -&gt; list[str]:\n    \"\"\"Split text into smaller chunks for processing.\"\"\"\n    text_splitter = self.splitter(\n        chunk_size=self.chunk_size,\n        chunk_overlap=self.chunk_overlap,\n        length_function=len,\n        is_separator_regex=True,\n    )\n    return cast(List[str], text_splitter.split_text(text))\n</code></pre>"},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter.TextSplitterBase","title":"TextSplitterBase","text":"<pre><code>TextSplitterBase(\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n)\n</code></pre> <p>The base text splitter class.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split a text into chunks.</p> </li> </ul> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>def __init__(\n    self,\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n) -&gt; None:\n    \"\"\"Initialize the text splitter class.\"\"\"\n    self.chunk_size = chunk_size or self.default_chunk_size\n    self.chunk_overlap = chunk_overlap or self.default_chunk_overlap\n    self.splitter_name = splitter_name or self.default_splitter_name\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter.TextSplitterBase.split","title":"split  <code>abstractmethod</code>","text":"<pre><code>split(text: str) -&gt; Iterable[str]\n</code></pre> <p>Split a text into chunks.</p> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>@abstractmethod\ndef split(self, text: str) -&gt; Iterable[str]:\n    \"\"\"Split a text into chunks.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/text_splitter/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/retrieval/text_splitter/base/#rago.retrieval.text_splitter.base","title":"base","text":"<p>The base classes for text splitter.</p> <p>Classes:</p> <ul> <li> <code>TextSplitterBase</code>           \u2013            <p>The base text splitter class.</p> </li> </ul>"},{"location":"api/retrieval/text_splitter/base/#rago.retrieval.text_splitter.base.TextSplitterBase","title":"TextSplitterBase","text":"<pre><code>TextSplitterBase(\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n)\n</code></pre> <p>The base text splitter class.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split a text into chunks.</p> </li> </ul> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>def __init__(\n    self,\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n) -&gt; None:\n    \"\"\"Initialize the text splitter class.\"\"\"\n    self.chunk_size = chunk_size or self.default_chunk_size\n    self.chunk_overlap = chunk_overlap or self.default_chunk_overlap\n    self.splitter_name = splitter_name or self.default_splitter_name\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/text_splitter/base/#rago.retrieval.text_splitter.base.TextSplitterBase.split","title":"split  <code>abstractmethod</code>","text":"<pre><code>split(text: str) -&gt; Iterable[str]\n</code></pre> <p>Split a text into chunks.</p> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>@abstractmethod\ndef split(self, text: str) -&gt; Iterable[str]:\n    \"\"\"Split a text into chunks.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/text_splitter/langchain/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> langchain","text":""},{"location":"api/retrieval/text_splitter/langchain/#rago.retrieval.text_splitter.langchain","title":"langchain","text":"<p>Support langchain text splitter.</p> <p>Classes:</p> <ul> <li> <code>LangChainTextSplitter</code>           \u2013            <p>LangChain Text Splitter class.</p> </li> </ul>"},{"location":"api/retrieval/text_splitter/langchain/#rago.retrieval.text_splitter.langchain.LangChainTextSplitter","title":"LangChainTextSplitter","text":"<pre><code>LangChainTextSplitter(\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n)\n</code></pre> <p>               Bases: <code>TextSplitterBase</code></p> <p>LangChain Text Splitter class.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split text into smaller chunks for processing.</p> </li> </ul> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>def __init__(\n    self,\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n) -&gt; None:\n    \"\"\"Initialize the text splitter class.\"\"\"\n    self.chunk_size = chunk_size or self.default_chunk_size\n    self.chunk_overlap = chunk_overlap or self.default_chunk_overlap\n    self.splitter_name = splitter_name or self.default_splitter_name\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/text_splitter/langchain/#rago.retrieval.text_splitter.langchain.LangChainTextSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; list[str]\n</code></pre> <p>Split text into smaller chunks for processing.</p> Source code in <code>src/rago/retrieval/text_splitter/langchain.py</code> <pre><code>def split(self, text: str) -&gt; list[str]:\n    \"\"\"Split text into smaller chunks for processing.\"\"\"\n    text_splitter = self.splitter(\n        chunk_size=self.chunk_size,\n        chunk_overlap=self.chunk_overlap,\n        length_function=len,\n        is_separator_regex=True,\n    )\n    return cast(List[str], text_splitter.split_text(text))\n</code></pre>"},{"location":"api/retrieval/tools/","title":"Index","text":""},{"location":"api/retrieval/tools/#rago.retrieval.tools","title":"tools","text":"<p>Tools for support retrieval classes.</p> <p>Modules:</p> <ul> <li> <code>pdf</code>           \u2013            <p>PDF tools.</p> </li> </ul>"},{"location":"api/retrieval/tools/pdf/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> pdf","text":""},{"location":"api/retrieval/tools/pdf/#rago.retrieval.tools.pdf","title":"pdf","text":"<p>PDF tools.</p> <p>Functions:</p> <ul> <li> <code>extract_text_from_pdf</code>             \u2013              <p>Extract text from a PDF file using pypdf.</p> </li> <li> <code>is_pdf</code>             \u2013              <p>Check if a file is a PDF by reading its header.</p> </li> </ul>"},{"location":"api/retrieval/tools/pdf/#rago.retrieval.tools.pdf.extract_text_from_pdf","title":"extract_text_from_pdf","text":"<pre><code>extract_text_from_pdf(file_path: str) -&gt; str\n</code></pre> <p>Extract text from a PDF file using pypdf.</p> <p>The result is the same as the one returned by PyPDFLoader.</p> Source code in <code>src/rago/retrieval/tools/pdf.py</code> <pre><code>def extract_text_from_pdf(file_path: str) -&gt; str:\n    \"\"\"\n    Extract text from a PDF file using pypdf.\n\n    The result is the same as the one returned by PyPDFLoader.\n    \"\"\"\n    reader = PdfReader(file_path)\n    pages = []\n    for page in reader.pages:\n        text = page.extract_text()\n        if text:\n            pages.append(text)\n    return ' '.join(pages)\n</code></pre>"},{"location":"api/retrieval/tools/pdf/#rago.retrieval.tools.pdf.is_pdf","title":"is_pdf","text":"<pre><code>is_pdf(file_path: str | Path) -&gt; bool\n</code></pre> <p>Check if a file is a PDF by reading its header.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>Path to the file to be checked.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the file is a PDF, False otherwise.</p> </li> </ul> Source code in <code>src/rago/retrieval/tools/pdf.py</code> <pre><code>def is_pdf(file_path: str | Path) -&gt; bool:\n    \"\"\"\n    Check if a file is a PDF by reading its header.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the file to be checked.\n\n    Returns\n    -------\n    bool\n        True if the file is a PDF, False otherwise.\n    \"\"\"\n    try:\n        with open(file_path, 'rb') as file:\n            header = file.read(4)\n            return header == b'%PDF'\n    except IOError:\n        return False\n</code></pre>"}]}