{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Rago","text":"<p>Rago is a lightweight framework for RAG.</p> <ul> <li>Software License: BSD 3 Clause</li> <li>Documentation: https://osl-incubator.github.io/rago</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Vector Database support</li> <li>FAISS</li> <li>Retrieval features</li> <li>Support pdf extraction via langchain</li> <li>Augmentation (Embedding + Vector Database Search)</li> <li>Support for Sentence Transformer (Hugging Face)</li> <li>Support for Open AI</li> <li>Support for SpaCy</li> <li>Generation (LLM)</li> <li>Support for Hugging Face</li> <li>Support for llama (Huggin FAce)</li> <li>Support for OpenAI</li> <li>Support for Gemini</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":""},{"location":"#1-add-new-backends","title":"1. Add new Backends","text":"<p>As noted in several GitHub issues, our initial goal is to support as many backends as possible. This approach will provide valuable insights into user needs and inform the structure for the next phase.</p>"},{"location":"#2-declarative-api-for-rago","title":"2. Declarative API for Rago","text":""},{"location":"#objective","title":"Objective","text":"<p>To simplify and streamline the user experience in configuring RAG by introducing a declarative, composable API\u2014similar to how Plotnine or Altair allows users to build visualizations.</p>"},{"location":"#overview","title":"Overview","text":"<p>The current procedural approach in Rago requires users to instantiate and connect individual components (retrieval, augmentation, generation, etc.) manually. This can become cumbersome as support for multiple backends grows. We propose a new declarative interface that lets users define their entire RAG steps in a single, fluent expression using operator overloading.</p>"},{"location":"#proposed-syntax-example","title":"Proposed Syntax Example","text":"<pre><code>from rago import Rago, Retrieval, Augmentation, Generation, DB, Cache\n\ndatasource = ...\n\nrag = (\n    Rago()\n    + DB(backend=\"faiss\", top_k=5)\n    + Cache(backend=\"file\")\n    + Retrieval(backend=\"dummy\")\n    + Augmentation(backend=\"openai\", model=\"text-embedding-3-small\")\n    + Generation(\n        backend=\"openai\",\n        model=\"gpt-4o-mini\",\n        prompt_template=\"Question: {query}\\nContext: {context}\\nAnswer:\"\n    )\n)\n\nresult = rag.run(query=\"What is the capital of France?\", data=datasource)\n</code></pre>"},{"location":"#key-benefits","title":"Key Benefits","text":"<ul> <li>Intuitive Composition: Users can build complex pipelines by simply adding   layers together.</li> <li>Modularity: Each component is encapsulated, making it easy to swap or   extend backends without altering the overall architecture.</li> <li>Reduced Boilerplate: The declarative syntax minimizes the need for   repetitive setup code, focusing on the \"what\" rather than the \"how.\"</li> <li>Enhanced Readability: The pipeline\u2019s structure becomes immediately clear,   promoting easier maintenance and collaboration.</li> </ul>"},{"location":"#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Define Base Classes: Develop abstract base classes for each component    (DB, Cache, Retrieval, Augmentation, Generation) to standardize interfaces    and facilitate future extensions.</li> <li>Operator Overloading: Implement the <code>__add__</code> method in the main <code>Rago</code>    class to allow chaining of components, effectively building the pipeline    through a fluent interface.</li> <li>Configuration and Defaults: Integrate sensible defaults and validation    (using tools like Pydantic) so that users can override only when necessary.</li> <li>Documentation and Examples: Provide comprehensive documentation and    examples to illustrate the new declarative syntax and usage scenarios.</li> </ol>"},{"location":"#installation","title":"Installation","text":"<p>If you want to install it for <code>cpu</code> only, you can run:</p> <pre><code>$ pip install rago[cpu]\n</code></pre> <p>But, if you want to install it for <code>gpu</code> (cuda), you can run:</p> <pre><code>$ pip install rago[gpu]\n</code></pre>"},{"location":"#setup","title":"Setup","text":""},{"location":"#llama-3","title":"Llama 3","text":"<p>In order to use a llama model, visit its page on huggingface and request your access in its form, for example: https://huggingface.co/meta-llama/Llama-3.2-1B.</p> <p>After you are granted access to the desired model, you will be able to use it with Rago.</p> <p>You will also need to provide a hugging face token in order to download the models locally, for example:</p> <pre><code>from rago import Rago\nfrom rago.generation import LlamaGen\nfrom rago.retrieval import StringRet\nfrom rago.augmented import SentenceTransformerAug\n\n# For Gated LLMs\nHF_TOKEN = 'YOUR_HUGGING_FACE_TOKEN'\n\nanimals_data = [\n    \"The Blue Whale is the largest animal ever known to have existed, even \"\n    \"bigger than the largest dinosaurs.\",\n    \"The Peregrine Falcon is renowned as the fastest animal on the planet, \"\n    \"capable of reaching speeds over 240 miles per hour.\",\n    \"The Giant Panda is a bear species endemic to China, easily recognized by \"\n    \"its distinctive black-and-white coat.\",\n    \"The Cheetah is the world's fastest land animal, capable of sprinting at \"\n    \"speeds up to 70 miles per hour in short bursts covering distances up to \"\n    \"500 meters.\",\n    \"The Komodo Dragon is the largest living species of lizard, found on \"\n    \"several Indonesian islands, including its namesake, Komodo.\",\n]\n\nrag = Rago(\n    retrieval=StringRet(animals_data),\n    augmented=SentenceTransformerAug(top_k=2),\n    generation=LlamaGen(api_key=HF_TOKEN),\n)\nrag.prompt('What is the faster animal on Earth?')\n</code></pre>"},{"location":"#ollama","title":"Ollama","text":"<p>For testing the generation with Ollama, run first the following commands:</p> <pre><code>$ ollama pull llama3.2:1b\n$ ollama serve\n</code></pre>"},{"location":"changelog/","title":"Release Notes","text":""},{"location":"changelog/#0142-2025-08-17","title":"0.14.2 (2025-08-17)","text":""},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>packaging and releasing steps (#100) (f8237a1)</li> </ul>"},{"location":"changelog/#0141-2025-08-17","title":"0.14.1 (2025-08-17)","text":""},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>fix packaging and replace poetry by setuptools (#99) (339cebf)</li> </ul>"},{"location":"changelog/#0140-2025-04-18","title":"0.14.0 (2025-04-18)","text":""},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Add <code>microsoft/phi-2</code> for Text Generation (#91) (9b2399f)</li> <li>Add support for fireworks-ai (#74) (db852ae)</li> <li>Add support for Ollama (#97) (5cae045)</li> <li>augmented: add ChromaBD backend support (#54) (22105c2)</li> <li>generation: add backend support for Groq (#86) (3cda11b)</li> <li>generation: add backend support for HuggingFace using inference (#85) (0be567f)</li> <li>generation: add backend support for Together (#79) (30c13ad)</li> </ul>"},{"location":"changelog/#0130-2025-03-13","title":"0.13.0 (2025-03-13)","text":""},{"location":"changelog/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>pkg: Add support for MacOS (#75) (e6a33b0)</li> </ul>"},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li>Add cohere backend support (#62) (6817ba0)</li> <li>generation: add backend for DeepSeek's generation class (#49) (47947d6)</li> </ul>"},{"location":"changelog/#0120-2025-02-11","title":"0.12.0 (2025-02-11)","text":""},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li>Add support system message (LLM/Gen) (#37) (eb6e499)</li> </ul>"},{"location":"changelog/#0113-2025-02-07","title":"0.11.3 (2025-02-07)","text":""},{"location":"changelog/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>improve and fix documentation (#36) (3ab061b)</li> </ul>"},{"location":"changelog/#0112-2025-01-22","title":"0.11.2 (2025-01-22)","text":""},{"location":"changelog/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>Fix get_embedding in the augmented classes (#34) (906ee2a)</li> </ul>"},{"location":"changelog/#0111-2025-01-22","title":"0.11.1 (2025-01-22)","text":""},{"location":"changelog/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>Fix default values (#33) (bb9b869)</li> </ul>"},{"location":"changelog/#0110-2025-01-21","title":"0.11.0 (2025-01-21)","text":""},{"location":"changelog/#features_3","title":"Features","text":"<ul> <li>Add support for extra parameters for RAG generation (#32) (e7a57c1)</li> </ul>"},{"location":"changelog/#0101-2025-01-19","title":"0.10.1 (2025-01-19)","text":""},{"location":"changelog/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>Fix caching (#30) (1ed642c)</li> </ul>"},{"location":"changelog/#0100-2025-01-16","title":"0.10.0 (2025-01-16)","text":""},{"location":"changelog/#features_4","title":"Features","text":"<ul> <li>Add support for caching (#29) (a3bb556)</li> </ul>"},{"location":"changelog/#090-2024-11-21","title":"0.9.0 (2024-11-21)","text":""},{"location":"changelog/#features_5","title":"Features","text":"<ul> <li>Add initial support for structured output (#25) (64856e1)</li> </ul>"},{"location":"changelog/#081-2024-11-19","title":"0.8.1 (2024-11-19)","text":""},{"location":"changelog/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>Rename Spacy to SpaCy (#24) (d611271)</li> </ul>"},{"location":"changelog/#080-2024-11-19","title":"0.8.0 (2024-11-19)","text":""},{"location":"changelog/#features_6","title":"Features","text":"<ul> <li>Add an attribute for logs to keep more information for all the steps (#22) (89b19bb)</li> <li>aug: Add support for SpaCy augmented class (#23) (14b4dd5)</li> </ul>"},{"location":"changelog/#071-2024-11-15","title":"0.7.1 (2024-11-15)","text":""},{"location":"changelog/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>Remove experimental augmented classes (#21) (e878012)</li> </ul>"},{"location":"changelog/#070-2024-11-14","title":"0.7.0 (2024-11-14)","text":""},{"location":"changelog/#features_7","title":"Features","text":"<ul> <li>Add PDF Retrieval and Text Splitter (#20) (6e80756)</li> </ul>"},{"location":"changelog/#060-2024-11-07","title":"0.6.0 (2024-11-07)","text":""},{"location":"changelog/#features_8","title":"Features","text":"<ul> <li>Add OpenAIAug with embeddings (#19) (227c535)</li> </ul>"},{"location":"changelog/#051-2024-11-04","title":"0.5.1 (2024-11-04)","text":""},{"location":"changelog/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>Remove unnecessary usage of abc.abstractmethod (#18) (565ec05)</li> </ul>"},{"location":"changelog/#050-2024-11-01","title":"0.5.0 (2024-11-01)","text":""},{"location":"changelog/#features_9","title":"Features","text":"<ul> <li>Add attribute for storing intermediate results from the pipeline (#17) (b00c4ae)</li> </ul>"},{"location":"changelog/#040-2024-10-31","title":"0.4.0 (2024-10-31)","text":""},{"location":"changelog/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>Fix packaging issues with torch and fix issues with langdetect (#10) (e4ade58)</li> </ul>"},{"location":"changelog/#features_10","title":"Features","text":"<ul> <li>Implement Gemini Model Integration for Augmentation and Generation (#16) (b418c9b)</li> <li>model: Integrate GPT-4 text generation in Rago (#13) (42078a3)</li> </ul>"},{"location":"changelog/#030-2024-10-24","title":"0.3.0 (2024-10-24)","text":""},{"location":"changelog/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>Fix packaging for gpu and cpu (#8) (42e0716)</li> <li>Improve the usage of typeguard (#5) (25782c1)</li> </ul>"},{"location":"changelog/#features_11","title":"Features","text":"<ul> <li>Add parameter for selecting device (cpu and gpu) (#6) (552d912)</li> <li>Implement type checking with typeguard (#4) (ae7958e)</li> </ul>"},{"location":"changelog/#020-2024-10-23","title":"0.2.0 (2024-10-23)","text":""},{"location":"changelog/#bug-fixes_12","title":"Bug Fixes","text":"<ul> <li>fix the semantic release workflow (#3) (b238e46)</li> </ul>"},{"location":"changelog/#features_12","title":"Features","text":"<ul> <li>Add support for llama 3.2 generation (#2) (770dd0c)</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>In order to be able to contribute, it is important that you understand the project layout.</p> <p>This project uses the src layout, which means that the package code is located at <code>./src/rago</code>.</p> <p>For my information, check the official documentation: https://packaging.python.org/en/latest/discussions/src-layout-vs-flat-layout/</p> <p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/osl-incubator/rago.git/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \u201cbug\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \u201cenhancement\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>Rago could always use more documentation, whether as part of the official Rago docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/osl-incubator/rago.git/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are   welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started","text":"<p>Ready to contribute? Here\u2019s how to set up <code>rago</code> for local development.</p> <ol> <li>Fork the <code>rago</code> repo on GitHub.</li> <li>Clone your fork locally and change to the directory of your project:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/rago.git\n$ cd rago/\n</code></pre> <p>Also, create a remote to the upstream repository, you will need that later:</p> <pre><code>$ git remote add upstream https://github.com/osl-incubator/rago.git\n$ git fetch --all\n</code></pre>"},{"location":"contributing/#prepare-and-use-virtual-environment","title":"Prepare and use virtual environment","text":"<p>If you don't have yet conda installed in your machine, you can check the installation steps here: conda-forge/miniforge?tab=readme-ov-file#download After that, ensure that conda is already available in your terminal session and run:</p> <pre><code>$ conda env create --file conda/dev.yaml\n$ conda activate rago\n</code></pre> <p>Note: you can use <code>mamba env create</code> instead, if you have it already installed, in order to boost the installation step.</p>"},{"location":"contributing/#install-the-dependencies","title":"Install the dependencies","text":"<p>Now, you can already install the dependencies for the project:</p> <pre><code>$ ./scripts/install-dev.sh\n</code></pre> <ul> <li><code>rago</code> uses a set of <code>pre-commit</code> hooks to improve code quality. The hooks can   be installed locally using:</li> </ul> <pre><code>$ pre-commit install\n</code></pre> <p>This would run the checks every time a <code>git commit</code> is executed locally. Usually, the verification will only run on the files modified by that commit, but the verification can also be triggered for all the files using:</p> <pre><code>$ pre-commit run --all-files\n</code></pre> <p>If you would like to skip the failing checks and push the code for further discussion, use the <code>--no-verify</code> option with <code>git commit</code>.</p> <pre><code>$ git commit -m \"Your commit message\" --no-verify\n</code></pre> <p>This project uses <code>pytest</code> as a testing tool. <code>pytest</code> is responsible for testing the code, whose configuration is available in pyproject.toml. Additionally, this project also uses <code>pytest-cov</code> to calculate the coverage of these unit tests. For more information, check the section about tests later in this document.</p>"},{"location":"contributing/#commit-your-changes-and-push-your-branch-to-github","title":"Commit your changes and push your branch to GitHub","text":"<pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ul> <li>Submit a pull request through the GitHub website.</li> </ul>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put your    new functionality into a function with a docstring, and add the feature to    the list in README.rst.</li> <li>The pull request should work for Python &gt;= 3.8.</li> </ol>"},{"location":"contributing/#running-tests-locally","title":"Running tests locally","text":"<p>The tests can be executed using the <code>test</code> dependencies of <code>rago</code> in the following way:</p> <pre><code>$ python -m pytest\n</code></pre>"},{"location":"contributing/#running-tests-with-coverage-locally","title":"Running tests with coverage locally","text":"<p>The coverage value can be obtained while running the tests using <code>pytest-cov</code> in the following way:</p> <pre><code>$ python -m pytest --cov=rago tests/\n</code></pre> <p>A much more detailed guide on testing with <code>pytest</code> is available here.</p>"},{"location":"contributing/#automation-tasks-with-makim","title":"Automation Tasks with Makim","text":"<p>This project uses <code>makim</code> as an automation tool. Please, check the <code>.makim.yaml</code> file to check all the tasks available or run:</p> <pre><code>$ makim --help\n</code></pre>"},{"location":"contributing/#release","title":"Release","text":"<p>This project uses semantic-release in order to cut a new release based on the commit-message.</p>"},{"location":"contributing/#commit-message-format","title":"Commit message format","text":"<p>semantic-release uses the commit messages to determine the consumer impact of changes in the codebase. Following formalized conventions for commit messages, semantic-release automatically determines the next semantic version number, generates a changelog and publishes the release.</p> <p>By default, semantic-release uses Angular Commit Message Conventions. The commit message format can be changed with the <code>preset</code> or <code>config</code> options_ of the @semantic-release/commit-analyzer and @semantic-release/release-notes-generator plugins.</p> <p>Tools such as commitizen or commitlint can be used to help contributors and enforce valid commit messages.</p> <p>The table below shows which commit message gets you which release type when <code>semantic-release</code> runs (using the default configuration):</p> Commit message Release type <code>fix(pencil): stop graphite breaking when pressure is applied</code> Fix Release <code>feat(pencil): add 'graphiteWidth' option</code> Feature Release <code>perf(pencil): remove graphiteWidth option</code> Chore <code>feat(pencil)!: The graphiteWidth option has been removed</code> Breaking Release <p>Note: For a breaking change release, uses <code>!</code> at the end of the message prefix.</p> <p>source: https://github.com/semantic-release/semantic-release/blob/master/README.md#commit-message-format</p> <p>As this project uses the <code>squash and merge</code> strategy, ensure to apply the commit message format to the PR's title.</p>"},{"location":"example/","title":"Rago","text":"In\u00a0[\u00a0]: Copied! <pre>import rago\n</pre> import rago In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/#rago","title":"Rago\u00b6","text":"<p>Rago is Python library that aims to do ...</p>"},{"location":"example/#getting-started","title":"Getting Started\u00b6","text":"<p>First, check our documentation about the installation.</p> <p>Now, let's import our library:</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install Rago, run this command in your terminal:</p> <pre><code>pip install rago\n</code></pre> <p>This is the preferred method to install Rago, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>The sources for Rago can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>git clone https://github.com/osl-incubator/rago.git\n</code></pre> <p>Or download the tarball:</p> <pre><code>curl -OJL https://github.com/osl-incubator/rago.git/tarball/main\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>./scripts/install-dev.sh\n</code></pre>"},{"location":"api/","title":"Index","text":""},{"location":"api/#rago","title":"rago","text":"<p>Rago.</p> <p>Modules:</p> <ul> <li> <code>augmented</code>           \u2013            <p>Augmented package.</p> </li> <li> <code>base</code>           \u2013            <p>Provide base interfaces.</p> </li> <li> <code>core</code>           \u2013            <p>Rago is Retrieval Augmented Generation lightweight framework.</p> </li> <li> <code>extensions</code>           \u2013            <p>Extra tools for supporting RAG.</p> </li> <li> <code>generation</code>           \u2013            <p>RAG Generation package.</p> </li> <li> <code>retrieval</code>           \u2013            <p>RAG Retrieval package.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>Rago</code>           \u2013            <p>RAG class.</p> </li> </ul>"},{"location":"api/#rago.Rago","title":"Rago","text":"<pre><code>Rago(\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n)\n</code></pre> <p>RAG class.</p> <p>Parameters:</p> <ul> <li> <code>retrieval</code>               (<code>RetrievalBase</code>)           \u2013            <p>The retrieval component used to fetch relevant data based on the query.</p> </li> <li> <code>augmented</code>               (<code>AugmentedBase</code>)           \u2013            <p>The augmentation module responsible for enriching the retrieved data.</p> </li> <li> <code>generation</code>               (<code>GenerationBase</code>)           \u2013            <p>The text generation model used to generate a response based on the query and augmented data.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>prompt</code>             \u2013              <p>Run the pipeline for a specific prompt.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def __init__(\n    self,\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n) -&gt; None:\n    \"\"\"Initialize the RAG structure.\n\n    Parameters\n    ----------\n    retrieval : RetrievalBase\n        The retrieval component used to fetch relevant data based\n        on the query.\n    augmented : AugmentedBase\n        The augmentation module responsible for enriching the\n        retrieved data.\n    generation : GenerationBase\n        The text generation model used to generate a response based\n        on the query and augmented data.\n    \"\"\"\n    self.retrieval = retrieval\n    self.augmented = augmented\n    self.generation = generation\n    self.logs: dict[str, dict[str, Any]] = {\n        'retrieval': retrieval.logs,\n        'augmented': augmented.logs,\n        'generation': generation.logs,\n    }\n</code></pre>"},{"location":"api/#rago.Rago.prompt","title":"prompt","text":"<pre><code>prompt(query: str, device: str = 'auto') -&gt; str | BaseModel\n</code></pre> <p>Run the pipeline for a specific prompt.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query or prompt from the user.</p> </li> <li> <code>device</code>               (<code>str (default 'auto')</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device for generation (e.g., 'auto', 'cpu', 'cuda'), by default 'auto'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on the query and augmented data.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def prompt(self, query: str, device: str = 'auto') -&gt; str | BaseModel:\n    \"\"\"Run the pipeline for a specific prompt.\n\n    Parameters\n    ----------\n    query : str\n        The query or prompt from the user.\n    device : str (default 'auto')\n        Device for generation (e.g., 'auto', 'cpu', 'cuda'), by\n        default 'auto'.\n\n    Returns\n    -------\n    str\n        Generated text based on the query and augmented data.\n    \"\"\"\n    ret_data = self.retrieval.get(query)\n    self.logs['retrieval']['result'] = ret_data\n\n    aug_data = self.augmented.search(query, ret_data)\n    self.logs['augmented']['result'] = aug_data\n\n    gen_data = self.generation.generate(query, context=aug_data)\n    self.logs['generation']['result'] = gen_data\n\n    return gen_data\n</code></pre>"},{"location":"api/#rago.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the program version.</p> Source code in <code>src/rago/__init__.py</code> <pre><code>def get_version() -&gt; str:\n    \"\"\"Return the program version.\"\"\"\n    try:\n        return importlib_metadata.version(__name__)\n    except importlib_metadata.PackageNotFoundError:  # pragma: no cover\n        return '0.14.2'  # semantic-release\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li> rago<ul> <li> augmented<ul> <li> base</li> <li> cohere</li> <li> db<ul> <li> base</li> <li> chroma</li> <li> faiss</li> </ul> </li> <li> fireworks</li> <li> openai</li> <li> sentence_transformer</li> <li> spacy</li> <li> together</li> </ul> </li> <li> base</li> <li> core</li> <li> extensions<ul> <li> base</li> <li> cache</li> </ul> </li> <li> generation<ul> <li> base</li> <li> cohere</li> <li> deepseek</li> <li> fireworks</li> <li> gemini</li> <li> groq</li> <li> hugging_face</li> <li> hugging_face_inf</li> <li> llama</li> <li> openai</li> <li> phi</li> <li> together</li> </ul> </li> <li> retrieval<ul> <li> base</li> <li> file</li> <li> text_splitter<ul> <li> base</li> <li> langchain</li> </ul> </li> <li> tools<ul> <li> pdf</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"api/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/base/#rago.base","title":"base","text":"<p>Provide base interfaces.</p> <p>Classes:</p> <ul> <li> <code>RagoBase</code>           \u2013            <p>Define base interface for RAG step classes.</p> </li> </ul>"},{"location":"api/base/#rago.base.RagoBase","title":"RagoBase","text":"<pre><code>RagoBase(\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = {},\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Define base interface for RAG step classes.</p> Source code in <code>src/rago/base.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = {},\n) -&gt; None:\n    self.api_key = api_key\n    self.cache = cache\n    self.logs = logs\n</code></pre>"},{"location":"api/core/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> core","text":""},{"location":"api/core/#rago.core","title":"core","text":"<p>Rago is Retrieval Augmented Generation lightweight framework.</p> <p>Classes:</p> <ul> <li> <code>Rago</code>           \u2013            <p>RAG class.</p> </li> </ul>"},{"location":"api/core/#rago.core.Rago","title":"Rago","text":"<pre><code>Rago(\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n)\n</code></pre> <p>RAG class.</p> <p>Parameters:</p> <ul> <li> <code>retrieval</code>               (<code>RetrievalBase</code>)           \u2013            <p>The retrieval component used to fetch relevant data based on the query.</p> </li> <li> <code>augmented</code>               (<code>AugmentedBase</code>)           \u2013            <p>The augmentation module responsible for enriching the retrieved data.</p> </li> <li> <code>generation</code>               (<code>GenerationBase</code>)           \u2013            <p>The text generation model used to generate a response based on the query and augmented data.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>prompt</code>             \u2013              <p>Run the pipeline for a specific prompt.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def __init__(\n    self,\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n) -&gt; None:\n    \"\"\"Initialize the RAG structure.\n\n    Parameters\n    ----------\n    retrieval : RetrievalBase\n        The retrieval component used to fetch relevant data based\n        on the query.\n    augmented : AugmentedBase\n        The augmentation module responsible for enriching the\n        retrieved data.\n    generation : GenerationBase\n        The text generation model used to generate a response based\n        on the query and augmented data.\n    \"\"\"\n    self.retrieval = retrieval\n    self.augmented = augmented\n    self.generation = generation\n    self.logs: dict[str, dict[str, Any]] = {\n        'retrieval': retrieval.logs,\n        'augmented': augmented.logs,\n        'generation': generation.logs,\n    }\n</code></pre>"},{"location":"api/core/#rago.core.Rago.prompt","title":"prompt","text":"<pre><code>prompt(query: str, device: str = 'auto') -&gt; str | BaseModel\n</code></pre> <p>Run the pipeline for a specific prompt.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query or prompt from the user.</p> </li> <li> <code>device</code>               (<code>str (default 'auto')</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device for generation (e.g., 'auto', 'cpu', 'cuda'), by default 'auto'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on the query and augmented data.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def prompt(self, query: str, device: str = 'auto') -&gt; str | BaseModel:\n    \"\"\"Run the pipeline for a specific prompt.\n\n    Parameters\n    ----------\n    query : str\n        The query or prompt from the user.\n    device : str (default 'auto')\n        Device for generation (e.g., 'auto', 'cpu', 'cuda'), by\n        default 'auto'.\n\n    Returns\n    -------\n    str\n        Generated text based on the query and augmented data.\n    \"\"\"\n    ret_data = self.retrieval.get(query)\n    self.logs['retrieval']['result'] = ret_data\n\n    aug_data = self.augmented.search(query, ret_data)\n    self.logs['augmented']['result'] = aug_data\n\n    gen_data = self.generation.generate(query, context=aug_data)\n    self.logs['generation']['result'] = gen_data\n\n    return gen_data\n</code></pre>"},{"location":"api/augmented/","title":"Index","text":""},{"location":"api/augmented/#rago.augmented","title":"augmented","text":"<p>Augmented package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for the augmented step.</p> </li> <li> <code>cohere</code>           \u2013            <p>Classes for augmentation with Cohere embeddings.</p> </li> <li> <code>db</code>           \u2013            <p>Rago DB package.</p> </li> <li> <code>fireworks</code>           \u2013            <p>Classes for augmentation with Fireworks embeddings.</p> </li> <li> <code>openai</code>           \u2013            <p>Classes for augmentation with OpenAI embeddings.</p> </li> <li> <code>sentence_transformer</code>           \u2013            <p>Classes for augmentation with hugging face.</p> </li> <li> <code>spacy</code>           \u2013            <p>Classes for augmentation with SpaCy embeddings.</p> </li> <li> <code>together</code>           \u2013            <p>Classes for augmentation with Together embeddings.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>AugmentedBase</code>           \u2013            <p>Define the base structure for Augmented classes.</p> </li> <li> <code>CohereAug</code>           \u2013            <p>Class for augmentation with Cohere embeddings.</p> </li> <li> <code>FireworksAug</code>           \u2013            <p>Class for augmentation with Fireworks embeddings.</p> </li> <li> <code>OpenAIAug</code>           \u2013            <p>Class for augmentation with OpenAI embeddings.</p> </li> <li> <code>SentenceTransformerAug</code>           \u2013            <p>Class for augmentation with Hugging Face.</p> </li> <li> <code>SpaCyAug</code>           \u2013            <p>Class for augmentation with SpaCy embeddings.</p> </li> <li> <code>TogetherAug</code>           \u2013            <p>Class for augmentation with Together embeddings.</p> </li> </ul>"},{"location":"api/augmented/#rago.augmented.AugmentedBase","title":"AugmentedBase","text":"<pre><code>AugmentedBase(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Define the base structure for Augmented classes.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.AugmentedBase.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    raise Exception('Method not implemented.')\n</code></pre>"},{"location":"api/augmented/#rago.augmented.AugmentedBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(\n    query: str, documents: Any, top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self,\n    query: str,\n    documents: Any,\n    top_k: int = 0,\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/#rago.augmented.CohereAug","title":"CohereAug","text":"<pre><code>CohereAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Cohere embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for given texts using Cohere API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.CohereAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for given texts using Cohere API.</p> Source code in <code>src/rago/augmented/cohere.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for given texts using Cohere API.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(cohere.Client, self.model)\n    response = model.embed(\n        texts=content,\n        model=self.model_name,\n        input_type='search_document',\n        embedding_types=['float'],\n    )\n    result = np.array(response.embeddings.float_, dtype=np.float32)  # type: ignore[union-attr]\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/augmented/#rago.augmented.CohereAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/cohere.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not getattr(self, 'db', None):\n        raise Exception('Vector database (db) is not initialized.')\n    document_encoded = self.get_embedding(documents)\n    model = cast(cohere.Client, self.model)\n    response = model.embed(\n        texts=[query],\n        model=self.model_name,\n        input_type='search_query',\n        embedding_types=['float'],\n    )\n    query_encoded = np.array(response.embeddings.float_, dtype=np.float32)  # type: ignore[union-attr]\n\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/#rago.augmented.FireworksAug","title":"FireworksAug","text":"<pre><code>FireworksAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Fireworks embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for given texts using the OpenAI client.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.FireworksAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for given texts using the OpenAI client.</p> Source code in <code>src/rago/augmented/fireworks.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for given texts using the OpenAI client.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    # Using the OpenAI embeddings API call for fireworks\n    response = self.openai_client.embeddings.create(\n        model=self.model_name,\n        input=content,\n    )\n    result = np.array(\n        [data.embedding for data in response.data], dtype=np.float32\n    )\n    self._save_cache(cache_key, result)\n    return result\n</code></pre>"},{"location":"api/augmented/#rago.augmented.FireworksAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/fireworks.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/#rago.augmented.OpenAIAug","title":"OpenAIAug","text":"<pre><code>OpenAIAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with OpenAI embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.OpenAIAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(openai.OpenAI, self.model)\n    response = model.embeddings.create(\n        input=content, model=self.model_name\n    )\n    result = np.array(\n        [data.embedding for data in response.data], dtype=np.float32\n    )\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/augmented/#rago.augmented.OpenAIAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    # Encode the documents and query\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SentenceTransformerAug","title":"SentenceTransformerAug","text":"<pre><code>SentenceTransformerAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Hugging Face.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SentenceTransformerAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/sentence_transformer.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    model = cast(SentenceTransformer, self.model)\n    return model.encode(content)\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SentenceTransformerAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: Any, top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/sentence_transformer.py</code> <pre><code>def search(self, query: str, documents: Any, top_k: int = 0) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    retrieved_docs = [documents[i] for i in indices]\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SpaCyAug","title":"SpaCyAug","text":"<pre><code>SpaCyAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with SpaCy embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using SpaCy.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SpaCyAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: List[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using SpaCy.</p> Source code in <code>src/rago/augmented/spacy.py</code> <pre><code>def get_embedding(self, content: List[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using SpaCy.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(spacy.language.Language, self.model)\n    embeddings = []\n\n    for text in content:\n        doc = model(text)\n\n        # Ensure the model has proper vectors\n        if not doc.has_vector:\n            raise ValueError(f\"Text: '{text}' has no valid word vectors!\")\n\n        embeddings.append(doc.vector)\n\n    result = np.array(embeddings, dtype=np.float32)\n\n    # Ensure 2D shape (num_texts, embedding_dim)\n    if result.ndim == 1:\n        result = result.reshape(1, -1)\n\n    self._save_cache(cache_key, result)\n    return result\n</code></pre>"},{"location":"api/augmented/#rago.augmented.SpaCyAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/spacy.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    # Encode the documents and query\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/#rago.augmented.TogetherAug","title":"TogetherAug","text":"<pre><code>TogetherAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Together embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for given texts using Together API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.TogetherAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for given texts using Together API.</p> Source code in <code>src/rago/augmented/together.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for given texts using Together API.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    client = cast(Together, self.model)\n    all_embeddings = []\n    for text in content:\n        response = client.embeddings.create(\n            model=self.model_name, input=text\n        )\n        embedding = response.data[0].embedding\n        all_embeddings.append(embedding)\n    result = np.array(all_embeddings, dtype=np.float32)\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/augmented/#rago.augmented.TogetherAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/together.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/augmented/base/#rago.augmented.base","title":"base","text":"<p>Base classes for the augmented step.</p> <p>Classes:</p> <ul> <li> <code>AugmentedBase</code>           \u2013            <p>Define the base structure for Augmented classes.</p> </li> </ul>"},{"location":"api/augmented/base/#rago.augmented.base.AugmentedBase","title":"AugmentedBase","text":"<pre><code>AugmentedBase(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Define the base structure for Augmented classes.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/base/#rago.augmented.base.AugmentedBase.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    raise Exception('Method not implemented.')\n</code></pre>"},{"location":"api/augmented/base/#rago.augmented.base.AugmentedBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(\n    query: str, documents: Any, top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self,\n    query: str,\n    documents: Any,\n    top_k: int = 0,\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/cohere/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> cohere","text":""},{"location":"api/augmented/cohere/#rago.augmented.cohere","title":"cohere","text":"<p>Classes for augmentation with Cohere embeddings.</p> <p>Classes:</p> <ul> <li> <code>CohereAug</code>           \u2013            <p>Class for augmentation with Cohere embeddings.</p> </li> </ul>"},{"location":"api/augmented/cohere/#rago.augmented.cohere.CohereAug","title":"CohereAug","text":"<pre><code>CohereAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Cohere embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for given texts using Cohere API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/cohere/#rago.augmented.cohere.CohereAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for given texts using Cohere API.</p> Source code in <code>src/rago/augmented/cohere.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for given texts using Cohere API.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(cohere.Client, self.model)\n    response = model.embed(\n        texts=content,\n        model=self.model_name,\n        input_type='search_document',\n        embedding_types=['float'],\n    )\n    result = np.array(response.embeddings.float_, dtype=np.float32)  # type: ignore[union-attr]\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/augmented/cohere/#rago.augmented.cohere.CohereAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/cohere.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not getattr(self, 'db', None):\n        raise Exception('Vector database (db) is not initialized.')\n    document_encoded = self.get_embedding(documents)\n    model = cast(cohere.Client, self.model)\n    response = model.embed(\n        texts=[query],\n        model=self.model_name,\n        input_type='search_query',\n        embedding_types=['float'],\n    )\n    query_encoded = np.array(response.embeddings.float_, dtype=np.float32)  # type: ignore[union-attr]\n\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/fireworks/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> fireworks","text":""},{"location":"api/augmented/fireworks/#rago.augmented.fireworks","title":"fireworks","text":"<p>Classes for augmentation with Fireworks embeddings.</p> <p>Classes:</p> <ul> <li> <code>FireworksAug</code>           \u2013            <p>Class for augmentation with Fireworks embeddings.</p> </li> </ul>"},{"location":"api/augmented/fireworks/#rago.augmented.fireworks.FireworksAug","title":"FireworksAug","text":"<pre><code>FireworksAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Fireworks embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for given texts using the OpenAI client.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/fireworks/#rago.augmented.fireworks.FireworksAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for given texts using the OpenAI client.</p> Source code in <code>src/rago/augmented/fireworks.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for given texts using the OpenAI client.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    # Using the OpenAI embeddings API call for fireworks\n    response = self.openai_client.embeddings.create(\n        model=self.model_name,\n        input=content,\n    )\n    result = np.array(\n        [data.embedding for data in response.data], dtype=np.float32\n    )\n    self._save_cache(cache_key, result)\n    return result\n</code></pre>"},{"location":"api/augmented/fireworks/#rago.augmented.fireworks.FireworksAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/fireworks.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/openai/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> openai","text":""},{"location":"api/augmented/openai/#rago.augmented.openai","title":"openai","text":"<p>Classes for augmentation with OpenAI embeddings.</p> <p>Classes:</p> <ul> <li> <code>OpenAIAug</code>           \u2013            <p>Class for augmentation with OpenAI embeddings.</p> </li> </ul>"},{"location":"api/augmented/openai/#rago.augmented.openai.OpenAIAug","title":"OpenAIAug","text":"<pre><code>OpenAIAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with OpenAI embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/openai/#rago.augmented.openai.OpenAIAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(openai.OpenAI, self.model)\n    response = model.embeddings.create(\n        input=content, model=self.model_name\n    )\n    result = np.array(\n        [data.embedding for data in response.data], dtype=np.float32\n    )\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/augmented/openai/#rago.augmented.openai.OpenAIAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    # Encode the documents and query\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/sentence_transformer/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> sentence_transformer","text":""},{"location":"api/augmented/sentence_transformer/#rago.augmented.sentence_transformer","title":"sentence_transformer","text":"<p>Classes for augmentation with hugging face.</p> <p>Classes:</p> <ul> <li> <code>SentenceTransformerAug</code>           \u2013            <p>Class for augmentation with Hugging Face.</p> </li> </ul>"},{"location":"api/augmented/sentence_transformer/#rago.augmented.sentence_transformer.SentenceTransformerAug","title":"SentenceTransformerAug","text":"<pre><code>SentenceTransformerAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Hugging Face.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using OpenAI API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/sentence_transformer/#rago.augmented.sentence_transformer.SentenceTransformerAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using OpenAI API.</p> Source code in <code>src/rago/augmented/sentence_transformer.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using OpenAI API.\"\"\"\n    model = cast(SentenceTransformer, self.model)\n    return model.encode(content)\n</code></pre>"},{"location":"api/augmented/sentence_transformer/#rago.augmented.sentence_transformer.SentenceTransformerAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: Any, top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/sentence_transformer.py</code> <pre><code>def search(self, query: str, documents: Any, top_k: int = 0) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    retrieved_docs = [documents[i] for i in indices]\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/spacy/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> spacy","text":""},{"location":"api/augmented/spacy/#rago.augmented.spacy","title":"spacy","text":"<p>Classes for augmentation with SpaCy embeddings.</p> <p>Classes:</p> <ul> <li> <code>SpaCyAug</code>           \u2013            <p>Class for augmentation with SpaCy embeddings.</p> </li> </ul>"},{"location":"api/augmented/spacy/#rago.augmented.spacy.SpaCyAug","title":"SpaCyAug","text":"<pre><code>SpaCyAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with SpaCy embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for a given text using SpaCy.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/spacy/#rago.augmented.spacy.SpaCyAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: List[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for a given text using SpaCy.</p> Source code in <code>src/rago/augmented/spacy.py</code> <pre><code>def get_embedding(self, content: List[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for a given text using SpaCy.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    model = cast(spacy.language.Language, self.model)\n    embeddings = []\n\n    for text in content:\n        doc = model(text)\n\n        # Ensure the model has proper vectors\n        if not doc.has_vector:\n            raise ValueError(f\"Text: '{text}' has no valid word vectors!\")\n\n        embeddings.append(doc.vector)\n\n    result = np.array(embeddings, dtype=np.float32)\n\n    # Ensure 2D shape (num_texts, embedding_dim)\n    if result.ndim == 1:\n        result = result.reshape(1, -1)\n\n    self._save_cache(cache_key, result)\n    return result\n</code></pre>"},{"location":"api/augmented/spacy/#rago.augmented.spacy.SpaCyAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/spacy.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n\n    # Encode the documents and query\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/together/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> together","text":""},{"location":"api/augmented/together/#rago.augmented.together","title":"together","text":"<p>Classes for augmentation with Together embeddings.</p> <p>Classes:</p> <ul> <li> <code>TogetherAug</code>           \u2013            <p>Class for augmentation with Together embeddings.</p> </li> </ul>"},{"location":"api/augmented/together/#rago.augmented.together.TogetherAug","title":"TogetherAug","text":"<pre><code>TogetherAug(\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Together embeddings.</p> <p>Methods:</p> <ul> <li> <code>get_embedding</code>             \u2013              <p>Retrieve the embedding for given texts using Together API.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    db: DBBase = FaissDB(),\n    top_k: Optional[int] = None,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.db = db\n\n    self.top_k = top_k if top_k is not None else self.default_top_k\n    self.model_name = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/together/#rago.augmented.together.TogetherAug.get_embedding","title":"get_embedding","text":"<pre><code>get_embedding(content: list[str]) -&gt; EmbeddingType\n</code></pre> <p>Retrieve the embedding for given texts using Together API.</p> Source code in <code>src/rago/augmented/together.py</code> <pre><code>def get_embedding(self, content: list[str]) -&gt; EmbeddingType:\n    \"\"\"Retrieve the embedding for given texts using Together API.\"\"\"\n    cache_key = sha256(''.join(content).encode('utf-8')).hexdigest()\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(EmbeddingType, cached)\n\n    client = cast(Together, self.model)\n    all_embeddings = []\n    for text in content:\n        response = client.embeddings.create(\n            model=self.model_name, input=text\n        )\n        embedding = response.data[0].embedding\n        all_embeddings.append(embedding)\n    result = np.array(all_embeddings, dtype=np.float32)\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/augmented/together/#rago.augmented.together.TogetherAug.search","title":"search","text":"<pre><code>search(\n    query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/together.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], top_k: int = 0\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not hasattr(self, 'db') or not self.db:\n        raise Exception('Vector database (db) is not initialized.')\n    document_encoded = self.get_embedding(documents)\n    query_encoded = self.get_embedding([query])\n    top_k = top_k or self.top_k or self.default_top_k or 1\n\n    self.db.embed(document_encoded)\n    scores, indices = self.db.search(query_encoded, top_k=top_k)\n\n    self.logs['indices'] = indices\n    self.logs['scores'] = scores\n    self.logs['search_params'] = {\n        'query_encoded': query_encoded,\n        'top_k': top_k,\n    }\n\n    retrieved_docs = [documents[i] for i in indices if i &gt;= 0]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/db/","title":"Index","text":""},{"location":"api/augmented/db/#rago.augmented.db","title":"db","text":"<p>Rago DB package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for database.</p> </li> <li> <code>chroma</code>           \u2013            <p>ChromaDB implementation for vector database.</p> </li> <li> <code>faiss</code>           \u2013            <p>Module for faiss database.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>ChromaDB</code>           \u2013            <p>ChromaDB implementation for vector database.</p> </li> <li> <code>DBBase</code>           \u2013            <p>Base class for vector database.</p> </li> <li> <code>FaissDB</code>           \u2013            <p>Faiss Database.</p> </li> </ul>"},{"location":"api/augmented/db/#rago.augmented.db.ChromaDB","title":"ChromaDB","text":"<pre><code>ChromaDB(client: ClientAPI, collection_name: str = 'rago')\n</code></pre> <p>               Bases: <code>DBBase</code></p> <p>ChromaDB implementation for vector database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search a query from documents.</p> </li> </ul> Source code in <code>src/rago/augmented/db/chroma.py</code> <pre><code>def __init__(\n    self,\n    client: ClientAPI,\n    collection_name: str = 'rago',\n) -&gt; None:\n    \"\"\"Initialize ChromaDB.\"\"\"\n    self.client = client\n    self.collection_name = collection_name\n    self._setup()\n</code></pre>"},{"location":"api/augmented/db/#rago.augmented.db.ChromaDB.embed","title":"embed","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/chroma.py</code> <pre><code>def embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    if not isinstance(documents, tuple) or len(documents) != 2:\n        raise ValueError(\n            'documents format must be: (List[str], List[List[float]])'\n        )\n\n    documents_list: List[str] = documents[0]\n    embeddings_list: List[List[float]] = documents[1]\n\n    # Convert embeddings to numpy array\n    embeddings = np.array(embeddings_list, dtype=np.float32)\n\n    self.collection.add(\n        documents=documents_list,\n        embeddings=embeddings,\n        ids=[str(i) for i in range(len(documents_list))],\n    )\n</code></pre>"},{"location":"api/augmented/db/#rago.augmented.db.ChromaDB.search","title":"search","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; Tuple[List[float], List[str]]\n</code></pre> <p>Search a query from documents.</p> Source code in <code>src/rago/augmented/db/chroma.py</code> <pre><code>def search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; Tuple[List[float], List[str]]:\n    \"\"\"Search a query from documents.\"\"\"\n    # Convert query_encoded to numpy array\n    query_encoded_np = np.array([query_encoded], dtype=np.float32)\n\n    results = self.collection.query(\n        query_embeddings=query_encoded_np.tolist(),\n        n_results=top_k,\n    )\n\n    # Check if keys exist before accessing them\n    distances = results.get('distances', [[]])\n    ids = results.get('ids', [[]])\n\n    # Ensure distances and ids are not None before indexing\n    distances_list: List[float] = distances[0] if distances else []\n    ids_list: List[str] = ids[0] if ids else []\n\n    return distances_list, ids_list\n</code></pre>"},{"location":"api/augmented/db/#rago.augmented.db.DBBase","title":"DBBase","text":"<p>Base class for vector database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search a query from documents.</p> </li> </ul>"},{"location":"api/augmented/db/#rago.augmented.db.DBBase.embed","title":"embed  <code>abstractmethod</code>","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/base.py</code> <pre><code>@abstractmethod\ndef embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/db/#rago.augmented.db.DBBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; tuple[\n    Iterable[float], Union[Iterable[str], Iterable[int]]\n]\n</code></pre> <p>Search a query from documents.</p> Source code in <code>src/rago/augmented/db/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Union[Iterable[str], Iterable[int]]]:\n    \"\"\"Search a query from documents.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/db/#rago.augmented.db.FaissDB","title":"FaissDB","text":"<p>               Bases: <code>DBBase</code></p> <p>Faiss Database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul>"},{"location":"api/augmented/db/#rago.augmented.db.FaissDB.embed","title":"embed","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/faiss.py</code> <pre><code>def embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    self.index = faiss.IndexFlatL2(documents.shape[1])\n    self.index.add(documents)\n</code></pre>"},{"location":"api/augmented/db/#rago.augmented.db.FaissDB.search","title":"search","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/db/faiss.py</code> <pre><code>def search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    distances, indices = self.index.search(query_encoded, top_k)\n    return distances, indices[0]\n</code></pre>"},{"location":"api/augmented/db/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/augmented/db/base/#rago.augmented.db.base","title":"base","text":"<p>Base classes for database.</p> <p>Classes:</p> <ul> <li> <code>DBBase</code>           \u2013            <p>Base class for vector database.</p> </li> </ul>"},{"location":"api/augmented/db/base/#rago.augmented.db.base.DBBase","title":"DBBase","text":"<p>Base class for vector database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search a query from documents.</p> </li> </ul>"},{"location":"api/augmented/db/base/#rago.augmented.db.base.DBBase.embed","title":"embed  <code>abstractmethod</code>","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/base.py</code> <pre><code>@abstractmethod\ndef embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/db/base/#rago.augmented.db.base.DBBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; tuple[\n    Iterable[float], Union[Iterable[str], Iterable[int]]\n]\n</code></pre> <p>Search a query from documents.</p> Source code in <code>src/rago/augmented/db/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Union[Iterable[str], Iterable[int]]]:\n    \"\"\"Search a query from documents.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/db/chroma/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> chroma","text":""},{"location":"api/augmented/db/chroma/#rago.augmented.db.chroma","title":"chroma","text":"<p>ChromaDB implementation for vector database.</p> <p>Classes:</p> <ul> <li> <code>ChromaDB</code>           \u2013            <p>ChromaDB implementation for vector database.</p> </li> </ul>"},{"location":"api/augmented/db/chroma/#rago.augmented.db.chroma.ChromaDB","title":"ChromaDB","text":"<pre><code>ChromaDB(client: ClientAPI, collection_name: str = 'rago')\n</code></pre> <p>               Bases: <code>DBBase</code></p> <p>ChromaDB implementation for vector database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search a query from documents.</p> </li> </ul> Source code in <code>src/rago/augmented/db/chroma.py</code> <pre><code>def __init__(\n    self,\n    client: ClientAPI,\n    collection_name: str = 'rago',\n) -&gt; None:\n    \"\"\"Initialize ChromaDB.\"\"\"\n    self.client = client\n    self.collection_name = collection_name\n    self._setup()\n</code></pre>"},{"location":"api/augmented/db/chroma/#rago.augmented.db.chroma.ChromaDB.embed","title":"embed","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/chroma.py</code> <pre><code>def embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    if not isinstance(documents, tuple) or len(documents) != 2:\n        raise ValueError(\n            'documents format must be: (List[str], List[List[float]])'\n        )\n\n    documents_list: List[str] = documents[0]\n    embeddings_list: List[List[float]] = documents[1]\n\n    # Convert embeddings to numpy array\n    embeddings = np.array(embeddings_list, dtype=np.float32)\n\n    self.collection.add(\n        documents=documents_list,\n        embeddings=embeddings,\n        ids=[str(i) for i in range(len(documents_list))],\n    )\n</code></pre>"},{"location":"api/augmented/db/chroma/#rago.augmented.db.chroma.ChromaDB.search","title":"search","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; Tuple[List[float], List[str]]\n</code></pre> <p>Search a query from documents.</p> Source code in <code>src/rago/augmented/db/chroma.py</code> <pre><code>def search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; Tuple[List[float], List[str]]:\n    \"\"\"Search a query from documents.\"\"\"\n    # Convert query_encoded to numpy array\n    query_encoded_np = np.array([query_encoded], dtype=np.float32)\n\n    results = self.collection.query(\n        query_embeddings=query_encoded_np.tolist(),\n        n_results=top_k,\n    )\n\n    # Check if keys exist before accessing them\n    distances = results.get('distances', [[]])\n    ids = results.get('ids', [[]])\n\n    # Ensure distances and ids are not None before indexing\n    distances_list: List[float] = distances[0] if distances else []\n    ids_list: List[str] = ids[0] if ids else []\n\n    return distances_list, ids_list\n</code></pre>"},{"location":"api/augmented/db/faiss/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> faiss","text":""},{"location":"api/augmented/db/faiss/#rago.augmented.db.faiss","title":"faiss","text":"<p>Module for faiss database.</p> <p>Classes:</p> <ul> <li> <code>FaissDB</code>           \u2013            <p>Faiss Database.</p> </li> </ul>"},{"location":"api/augmented/db/faiss/#rago.augmented.db.faiss.FaissDB","title":"FaissDB","text":"<p>               Bases: <code>DBBase</code></p> <p>Faiss Database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul>"},{"location":"api/augmented/db/faiss/#rago.augmented.db.faiss.FaissDB.embed","title":"embed","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/augmented/db/faiss.py</code> <pre><code>def embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    self.index = faiss.IndexFlatL2(documents.shape[1])\n    self.index.add(documents)\n</code></pre>"},{"location":"api/augmented/db/faiss/#rago.augmented.db.faiss.FaissDB.search","title":"search","text":"<pre><code>search(\n    query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/db/faiss.py</code> <pre><code>def search(\n    self, query_encoded: Any, top_k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    distances, indices = self.index.search(query_encoded, top_k)\n    return distances, indices[0]\n</code></pre>"},{"location":"api/extensions/","title":"Index","text":""},{"location":"api/extensions/#rago.extensions","title":"extensions","text":"<p>Extra tools for supporting RAG.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Extension base module.</p> </li> <li> <code>cache</code>           \u2013            <p>Provide an extension for caching.</p> </li> </ul>"},{"location":"api/extensions/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/extensions/base/#rago.extensions.base","title":"base","text":"<p>Extension base module.</p> <p>Classes:</p> <ul> <li> <code>Extension</code>           \u2013            <p>Define base interface for RAG Extension classes.</p> </li> </ul>"},{"location":"api/extensions/base/#rago.extensions.base.Extension","title":"Extension","text":"<p>               Bases: <code>ABC</code></p> <p>Define base interface for RAG Extension classes.</p>"},{"location":"api/extensions/cache/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> cache","text":""},{"location":"api/extensions/cache/#rago.extensions.cache","title":"cache","text":"<p>Provide an extension for caching.</p> <p>Classes:</p> <ul> <li> <code>Cache</code>           \u2013            <p>Define an extension base for caching.</p> </li> <li> <code>CacheFile</code>           \u2013            <p>Define a extra step for caching.</p> </li> </ul>"},{"location":"api/extensions/cache/#rago.extensions.cache.Cache","title":"Cache","text":"<p>               Bases: <code>Extension</code></p> <p>Define an extension base for caching.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load the cache for given key.</p> </li> <li> <code>save</code>             \u2013              <p>Save the cache for given key.</p> </li> </ul>"},{"location":"api/extensions/cache/#rago.extensions.cache.Cache.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(key: Any) -&gt; Any\n</code></pre> <p>Load the cache for given key.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>@abstractmethod\ndef load(self, key: Any) -&gt; Any:\n    \"\"\"Load the cache for given key.\"\"\"\n    raise Exception(f'Load method is not implemented: {key}')\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.Cache.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(key: Any, data: Any) -&gt; None\n</code></pre> <p>Save the cache for given key.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>@abstractmethod\ndef save(self, key: Any, data: Any) -&gt; None:\n    \"\"\"Save the cache for given key.\"\"\"\n    raise Exception(f'Save method is not implemented: {key}')\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.CacheFile","title":"CacheFile","text":"<pre><code>CacheFile(target_dir: Path)\n</code></pre> <p>               Bases: <code>Cache</code></p> <p>Define a extra step for caching.</p> <p>Methods:</p> <ul> <li> <code>get_file_path</code>             \u2013              <p>Return the file path.</p> </li> <li> <code>load</code>             \u2013              <p>Load the cache for given key.</p> </li> <li> <code>save</code>             \u2013              <p>Load the cache for given key.</p> </li> </ul> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>def __init__(self, target_dir: Path) -&gt; None:\n    self.target_dir = target_dir\n    self.target_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.CacheFile.get_file_path","title":"get_file_path","text":"<pre><code>get_file_path(key: Any) -&gt; Path\n</code></pre> <p>Return the file path.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>def get_file_path(self, key: Any) -&gt; Path:\n    \"\"\"Return the file path.\"\"\"\n    ref = sha256(str(key).encode('utf-8')).hexdigest()\n    return self.target_dir / f'{ref}.pkl'\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.CacheFile.load","title":"load","text":"<pre><code>load(key: Any) -&gt; Any\n</code></pre> <p>Load the cache for given key.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>def load(self, key: Any) -&gt; Any:\n    \"\"\"Load the cache for given key.\"\"\"\n    file_path = self.get_file_path(key)\n    if not file_path.exists():\n        return\n    return joblib.load(file_path)\n</code></pre>"},{"location":"api/extensions/cache/#rago.extensions.cache.CacheFile.save","title":"save","text":"<pre><code>save(key: Any, data: Any) -&gt; None\n</code></pre> <p>Load the cache for given key.</p> Source code in <code>src/rago/extensions/cache.py</code> <pre><code>def save(self, key: Any, data: Any) -&gt; None:\n    \"\"\"Load the cache for given key.\"\"\"\n    file_path = self.get_file_path(key)\n    joblib.dump(data, file_path)\n</code></pre>"},{"location":"api/generation/","title":"Index","text":""},{"location":"api/generation/#rago.generation","title":"generation","text":"<p>RAG Generation package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for generation.</p> </li> <li> <code>cohere</code>           \u2013            <p>CohereGen class for text generation using Cohere's API.</p> </li> <li> <code>deepseek</code>           \u2013            <p>DeepSeek generation module.</p> </li> <li> <code>fireworks</code>           \u2013            <p>FireworksGen class for text generation using Fireworks API.</p> </li> <li> <code>gemini</code>           \u2013            <p>GeminiGen class for text generation using Google's Gemini model.</p> </li> <li> <code>groq</code>           \u2013            <p>Groq class for text generation.</p> </li> <li> <code>hugging_face</code>           \u2013            <p>Hugging Face classes for text generation.</p> </li> <li> <code>hugging_face_inf</code>           \u2013            <p>Hugging Face inferencing classes for text generation.</p> </li> <li> <code>llama</code>           \u2013            <p>Llama generation module.</p> </li> <li> <code>openai</code>           \u2013            <p>OpenAI Generation Model class for flexible GPT-based text generation.</p> </li> <li> <code>phi</code>           \u2013            <p>Phi generation module.</p> </li> <li> <code>together</code>           \u2013            <p>TogetherGen class for text generation using Together AI's API.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>CohereGen</code>           \u2013            <p>Cohere generation model for text generation.</p> </li> <li> <code>DeepSeekGen</code>           \u2013            <p>DeepSeek Generation class.</p> </li> <li> <code>FireworksGen</code>           \u2013            <p>Fireworks AI generation model for text generation.</p> </li> <li> <code>GeminiGen</code>           \u2013            <p>Gemini generation model for text generation.</p> </li> <li> <code>GenerationBase</code>           \u2013            <p>Generic Generation class.</p> </li> <li> <code>GroqGen</code>           \u2013            <p>Groq generation model for text generation.</p> </li> <li> <code>HuggingFaceGen</code>           \u2013            <p>HuggingFaceGen.</p> </li> <li> <code>HuggingFaceInfGen</code>           \u2013            <p>HuggingFaceGen with InferenceClient.</p> </li> <li> <code>LlamaGen</code>           \u2013            <p>Llama Generation class.</p> </li> <li> <code>OllamaGen</code>           \u2013            <p>Ollama Generation class for local inference via ollama-python.</p> </li> <li> <code>OllamaOpenAIGen</code>           \u2013            <p>OllamaGen via the Ollama Python client.</p> </li> <li> <code>OpenAIGen</code>           \u2013            <p>OpenAI generation model for text generation.</p> </li> <li> <code>PhiGen</code>           \u2013            <p>Phi Generation class.</p> </li> <li> <code>TogetherGen</code>           \u2013            <p>Together AI generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/#rago.generation.CohereGen","title":"CohereGen","text":"<pre><code>CohereGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Cohere generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Cohere's API.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.CohereGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Cohere's API.</p> Source code in <code>src/rago/generation/cohere.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Cohere's API.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n    api_params = self.api_params or self.default_api_params\n\n    if self.structured_output:\n        messages = []\n        # Explicit instruction to generate JSON output.\n        system_instruction = (\n            'Generate a JSON object that strictly follows the provided  '\n            'JSON schema. Do not include any additional text.'\n        )\n        if self.system_message:\n            system_instruction += ' ' + self.system_message\n        messages.append({'role': 'system', 'content': system_instruction})\n        messages.append({'role': 'user', 'content': input_text})\n\n        response_format_config = {\n            'type': 'json_object',\n            'json_schema': (\n                self.structured_output\n                if isinstance(self.structured_output, dict)\n                else self.structured_output.model_json_schema()\n            ),\n        }\n        model_params = {\n            'messages': messages,\n            'max_tokens': self.output_max_length,\n            'temperature': self.temperature,\n            'model': self.model_name,\n            'response_format': response_format_config,\n            **api_params,\n        }\n\n        response = self.model.client.chat(**model_params)\n        self.logs['model_params'] = model_params\n        json_text = response.message.content[0].text\n        parsed_dict = json.loads(json_text)\n        parsed_model = self.structured_output(**parsed_dict)\n        return parsed_model\n\n    if self.system_message:\n        messages = [\n            {'role': 'system', 'content': self.system_message},\n            {'role': 'user', 'content': input_text},\n        ]\n        model_params = {\n            'model': self.model_name,\n            'messages': messages,\n            'max_tokens': self.output_max_length,\n            'temperature': self.temperature,\n            **api_params,\n        }\n        response = self.model.chat(**model_params)\n        self.logs['model_params'] = model_params\n        return cast(str, response.text)\n\n    model_params = {\n        'model': self.model_name,\n        'prompt': input_text,\n        'max_tokens': self.output_max_length,\n        'temperature': self.temperature,\n        **api_params,\n    }\n    response = self.model.generate(**model_params)\n    self.logs['model_params'] = model_params\n    return cast(str, response.generations[0].text.strip())\n</code></pre>"},{"location":"api/generation/#rago.generation.DeepSeekGen","title":"DeepSeekGen","text":"<pre><code>DeepSeekGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>DeepSeek Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using DeepSeek model with chat template.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.DeepSeekGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using DeepSeek model with chat template.</p> Source code in <code>src/rago/generation/deepseek.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using DeepSeek model with chat template.\"\"\"\n    messages = [\n        {\n            'role': 'user',\n            'content': f'{query}\\nContext: {\" \".join(context)}',\n        }\n    ]\n\n    input_tensor = self.tokenizer.apply_chat_template(\n        messages, add_generation_prompt=True, return_tensors='pt'\n    ).to(self.model.device)\n\n    model_params = dict(\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n    )\n\n    self.logs['model_params'] = model_params\n\n    outputs = self.model.generate(input_tensor, **model_params)\n\n    answer: str = str(\n        self.tokenizer.decode(\n            outputs[0][input_tensor.shape[1] :], skip_special_tokens=True\n        )\n    )\n\n    return answer.strip()\n</code></pre>"},{"location":"api/generation/#rago.generation.FireworksGen","title":"FireworksGen","text":"<pre><code>FireworksGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Fireworks AI generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Fireworks AI's API.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.FireworksGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Fireworks AI's API.</p> Source code in <code>src/rago/generation/fireworks.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Fireworks AI's API.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    api_params = self.api_params or self.default_api_params\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = {\n        'model': self.model_name,\n        'messages': messages,\n        'max_tokens': self.output_max_length,\n        'temperature': self.temperature,\n        **api_params,\n    }\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n        response = self.model.chat.completions.create(**model_params)\n        self.logs['model_params'] = model_params\n        return cast(BaseModel, response)\n\n    response = self.model.chat.completions.create(**model_params)\n    self.logs['model_params'] = model_params\n    return cast(str, response.choices[0].message.content.strip())\n</code></pre>"},{"location":"api/generation/#rago.generation.GeminiGen","title":"GeminiGen","text":"<pre><code>GeminiGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Gemini generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Gemini model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.GeminiGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Gemini model support.</p> Source code in <code>src/rago/generation/gemini.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Gemini model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.structured_output:\n        models_params_gen = {'contents': input_text}\n        response = self.model.generate_content(**models_params_gen)\n        self.logs['model_params'] = models_params_gen\n        return cast(str, response.text.strip())\n\n    api_params = (\n        self.api_params if self.api_params else self.default_api_params\n    )\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = {\n        'messages': messages,\n        'response_model': self.structured_output,\n        **api_params,\n    }\n\n    response = self.model.create(\n        **model_params,\n    )\n\n    self.logs['model_params'] = model_params\n\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/#rago.generation.GenerationBase","title":"GenerationBase","text":"<pre><code>GenerationBase(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Generic Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text with optional language parameter.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.GenerationBase.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text with optional language parameter.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The input query or prompt.</p> </li> <li> <code>context</code>               (<code>list[str]</code>)           \u2013            <p>Additional context information for the generation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on query and context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text with optional language parameter.\n\n    Parameters\n    ----------\n    query : str\n        The input query or prompt.\n    context : list[str]\n        Additional context information for the generation.\n\n    Returns\n    -------\n    str\n        Generated text based on query and context.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/generation/#rago.generation.GroqGen","title":"GroqGen","text":"<pre><code>GroqGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Groq generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using the Groq AP.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.GroqGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using the Groq AP.</p> Source code in <code>src/rago/generation/groq.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text using the Groq AP.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    api_params = (\n        self.api_params if self.api_params else self.default_api_params\n    )\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = dict(\n        model=self.model_name,\n        messages=messages,\n        max_completion_tokens=self.output_max_length,\n        temperature=self.temperature,\n        **api_params,\n    )\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n\n    response = self.model.chat.completions.create(**model_params)\n    self.logs['model_params'] = model_params\n\n    if hasattr(response, 'choices') and isinstance(response.choices, list):\n        return cast(str, response.choices[0].message.content.strip())\n\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/#rago.generation.HuggingFaceGen","title":"HuggingFaceGen","text":"<pre><code>HuggingFaceGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>HuggingFaceGen.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate the text from the query and augmented context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.HuggingFaceGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate the text from the query and augmented context.</p> Source code in <code>src/rago/generation/hugging_face.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate the text from the query and augmented context.\"\"\"\n    with torch.no_grad():\n        input_text = self.prompt_template.format(\n            query=query, context=' '.join(context)\n        )\n        input_ids = self.tokenizer.encode(\n            input_text,\n            return_tensors='pt',\n            truncation=True,\n            max_length=512,\n        ).to(self.device_name)\n\n        api_params = (\n            self.api_params if self.api_params else self.default_api_params\n        )\n\n        model_params = dict(\n            inputs=input_ids,\n            max_length=self.output_max_length,\n            pad_token_id=self.tokenizer.eos_token_id,\n            **api_params,\n        )\n\n        outputs = self.model.generate(**model_params)\n\n        self.logs['model_params'] = model_params\n\n        response = self.tokenizer.decode(\n            outputs[0], skip_special_tokens=True\n        )\n\n    if self.device_name == 'cuda':\n        torch.cuda.empty_cache()\n\n    return str(response)\n</code></pre>"},{"location":"api/generation/#rago.generation.HuggingFaceInfGen","title":"HuggingFaceInfGen","text":"<pre><code>HuggingFaceInfGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>HuggingFaceGen with InferenceClient.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate the text from the query and augmented context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.HuggingFaceInfGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate the text from the query and augmented context.</p> Source code in <code>src/rago/generation/hugging_face_inf.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate the text from the query and augmented context.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n    if self.system_message:\n        input_text = f'{self.system_message}\\n{input_text}'\n\n    api_params = self.api_params or self.default_api_params\n\n    self.logs['model_params'] = {\n        'model': self.model_name,\n        'inputs': input_text,\n        'parameters': api_params,\n    }\n    generated_text = self.client.text_generation(\n        prompt=input_text,\n        model=self.model_name,\n        max_new_tokens=api_params['max_new_tokens'],\n        temperature=api_params['temperature'],\n    )\n\n    return generated_text.strip()\n</code></pre>"},{"location":"api/generation/#rago.generation.LlamaGen","title":"LlamaGen","text":"<pre><code>LlamaGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Llama Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Llama model with language support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.LlamaGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Llama model with language support.</p> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Llama model with language support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    # Detect and set the language code for multilingual models (optional)\n    language = str(detect(query)) or 'en'\n    self.tokenizer.lang_code = language\n\n    api_params = (\n        self.api_params if self.api_params else self.default_api_params\n    )\n\n    # Generate the response with adjusted parameters\n\n    model_params = dict(\n        text_inputs=input_text,\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n        eos_token_id=self.tokenizer.eos_token_id,\n        **api_params,\n    )\n    response = self.generator(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    # Extract and return the answer only\n    answer = str(response[0].get('generated_text', ''))\n    # Strip off any redundant text after the answer itself\n    return answer.split('Answer:')[-1].strip()\n</code></pre>"},{"location":"api/generation/#rago.generation.OllamaGen","title":"OllamaGen","text":"<pre><code>OllamaGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Ollama Generation class for local inference via ollama-python.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text by sending a prompt to the local Ollama model.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.OllamaGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text by sending a prompt to the local Ollama model.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The user query.</p> </li> <li> <code>context</code>               (<code>list[str]</code>)           \u2013            <p>Augmented context strings.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The generated response text.</p> </li> </ul> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"\n    Generate text by sending a prompt to the local Ollama model.\n\n    Parameters\n    ----------\n    query : str\n        The user query.\n    context : list[str]\n        Augmented context strings.\n\n    Returns\n    -------\n    str\n        The generated response text.\n    \"\"\"\n    input_text = self.prompt_template.format(\n        query=query,\n        context=' '.join(context),\n    )\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    params = {\n        'model': self.model_name,\n        'messages': messages,\n        **(self.api_params or {}),\n    }\n    response = self.model.chat(**params)\n    return str(response.message.content).strip()\n</code></pre>"},{"location":"api/generation/#rago.generation.OllamaOpenAIGen","title":"OllamaOpenAIGen","text":"<pre><code>OllamaOpenAIGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>OpenAIGen</code></p> <p>OllamaGen via the Ollama Python client.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using OpenAI's API with dynamic model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.OllamaOpenAIGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using OpenAI's API with dynamic model support.</p> Source code in <code>src/rago/generation/openai.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text using OpenAI's API with dynamic model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = dict(\n        model=self.model_name,\n        messages=messages,\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n        **self.api_params,\n    )\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n\n    response = self.model.chat.completions.create(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    has_choices = hasattr(response, 'choices')\n\n    if has_choices and isinstance(response.choices, list):\n        return cast(str, response.choices[0].message.content.strip())\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/#rago.generation.OpenAIGen","title":"OpenAIGen","text":"<pre><code>OpenAIGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>OpenAI generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using OpenAI's API with dynamic model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.OpenAIGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using OpenAI's API with dynamic model support.</p> Source code in <code>src/rago/generation/openai.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text using OpenAI's API with dynamic model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = dict(\n        model=self.model_name,\n        messages=messages,\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n        **self.api_params,\n    )\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n\n    response = self.model.chat.completions.create(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    has_choices = hasattr(response, 'choices')\n\n    if has_choices and isinstance(response.choices, list):\n        return cast(str, response.choices[0].message.content.strip())\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/#rago.generation.PhiGen","title":"PhiGen","text":"<pre><code>PhiGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Phi Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Phi model with context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.PhiGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Phi model with context.</p> Source code in <code>src/rago/generation/phi.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Phi model with context.\"\"\"\n    full_prompt = f'{query}\\nContext: {\" \".join(context)}'\n\n    inputs = self.tokenizer(\n        full_prompt, return_tensors='pt', return_attention_mask=True\n    ).to(self.model.device)\n\n    model_params = dict(\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n        top_p=self.default_api_params['top_p'],\n        num_return_sequences=self.default_api_params[\n            'num_return_sequences'\n        ],\n    )\n\n    self.logs['model_params'] = model_params\n\n    outputs = self.model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        **model_params,\n    )\n\n    answer: str = self.tokenizer.decode(\n        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n    )\n\n    return answer.strip()\n</code></pre>"},{"location":"api/generation/#rago.generation.TogetherGen","title":"TogetherGen","text":"<pre><code>TogetherGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Together AI generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Together AI's API.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.TogetherGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Together AI's API.</p> Source code in <code>src/rago/generation/together.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Together AI's API.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    api_params = self.api_params or self.default_api_params\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = {\n        'model': self.model_name,\n        'messages': messages,\n        'max_tokens': self.output_max_length,\n        'temperature': self.temperature,\n        **api_params,\n    }\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n        response = self.model.chat.completions.create(**model_params)\n        self.logs['model_params'] = model_params\n        return cast(BaseModel, response)\n\n    response = self.model.chat.completions.create(**model_params)\n    self.logs['model_params'] = model_params\n    return cast(str, response.choices[0].message.content.strip())\n</code></pre>"},{"location":"api/generation/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/generation/base/#rago.generation.base","title":"base","text":"<p>Base classes for generation.</p> <p>Classes:</p> <ul> <li> <code>GenerationBase</code>           \u2013            <p>Generic Generation class.</p> </li> </ul>"},{"location":"api/generation/base/#rago.generation.base.GenerationBase","title":"GenerationBase","text":"<pre><code>GenerationBase(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Generic Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text with optional language parameter.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/base/#rago.generation.base.GenerationBase.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text with optional language parameter.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The input query or prompt.</p> </li> <li> <code>context</code>               (<code>list[str]</code>)           \u2013            <p>Additional context information for the generation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on query and context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text with optional language parameter.\n\n    Parameters\n    ----------\n    query : str\n        The input query or prompt.\n    context : list[str]\n        Additional context information for the generation.\n\n    Returns\n    -------\n    str\n        Generated text based on query and context.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/generation/cohere/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> cohere","text":""},{"location":"api/generation/cohere/#rago.generation.cohere","title":"cohere","text":"<p>CohereGen class for text generation using Cohere's API.</p> <p>Classes:</p> <ul> <li> <code>CohereGen</code>           \u2013            <p>Cohere generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/cohere/#rago.generation.cohere.CohereGen","title":"CohereGen","text":"<pre><code>CohereGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Cohere generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Cohere's API.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/cohere/#rago.generation.cohere.CohereGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Cohere's API.</p> Source code in <code>src/rago/generation/cohere.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Cohere's API.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n    api_params = self.api_params or self.default_api_params\n\n    if self.structured_output:\n        messages = []\n        # Explicit instruction to generate JSON output.\n        system_instruction = (\n            'Generate a JSON object that strictly follows the provided  '\n            'JSON schema. Do not include any additional text.'\n        )\n        if self.system_message:\n            system_instruction += ' ' + self.system_message\n        messages.append({'role': 'system', 'content': system_instruction})\n        messages.append({'role': 'user', 'content': input_text})\n\n        response_format_config = {\n            'type': 'json_object',\n            'json_schema': (\n                self.structured_output\n                if isinstance(self.structured_output, dict)\n                else self.structured_output.model_json_schema()\n            ),\n        }\n        model_params = {\n            'messages': messages,\n            'max_tokens': self.output_max_length,\n            'temperature': self.temperature,\n            'model': self.model_name,\n            'response_format': response_format_config,\n            **api_params,\n        }\n\n        response = self.model.client.chat(**model_params)\n        self.logs['model_params'] = model_params\n        json_text = response.message.content[0].text\n        parsed_dict = json.loads(json_text)\n        parsed_model = self.structured_output(**parsed_dict)\n        return parsed_model\n\n    if self.system_message:\n        messages = [\n            {'role': 'system', 'content': self.system_message},\n            {'role': 'user', 'content': input_text},\n        ]\n        model_params = {\n            'model': self.model_name,\n            'messages': messages,\n            'max_tokens': self.output_max_length,\n            'temperature': self.temperature,\n            **api_params,\n        }\n        response = self.model.chat(**model_params)\n        self.logs['model_params'] = model_params\n        return cast(str, response.text)\n\n    model_params = {\n        'model': self.model_name,\n        'prompt': input_text,\n        'max_tokens': self.output_max_length,\n        'temperature': self.temperature,\n        **api_params,\n    }\n    response = self.model.generate(**model_params)\n    self.logs['model_params'] = model_params\n    return cast(str, response.generations[0].text.strip())\n</code></pre>"},{"location":"api/generation/deepseek/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> deepseek","text":""},{"location":"api/generation/deepseek/#rago.generation.deepseek","title":"deepseek","text":"<p>DeepSeek generation module.</p> <p>Classes:</p> <ul> <li> <code>DeepSeekGen</code>           \u2013            <p>DeepSeek Generation class.</p> </li> </ul>"},{"location":"api/generation/deepseek/#rago.generation.deepseek.DeepSeekGen","title":"DeepSeekGen","text":"<pre><code>DeepSeekGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>DeepSeek Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using DeepSeek model with chat template.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/deepseek/#rago.generation.deepseek.DeepSeekGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using DeepSeek model with chat template.</p> Source code in <code>src/rago/generation/deepseek.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using DeepSeek model with chat template.\"\"\"\n    messages = [\n        {\n            'role': 'user',\n            'content': f'{query}\\nContext: {\" \".join(context)}',\n        }\n    ]\n\n    input_tensor = self.tokenizer.apply_chat_template(\n        messages, add_generation_prompt=True, return_tensors='pt'\n    ).to(self.model.device)\n\n    model_params = dict(\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n    )\n\n    self.logs['model_params'] = model_params\n\n    outputs = self.model.generate(input_tensor, **model_params)\n\n    answer: str = str(\n        self.tokenizer.decode(\n            outputs[0][input_tensor.shape[1] :], skip_special_tokens=True\n        )\n    )\n\n    return answer.strip()\n</code></pre>"},{"location":"api/generation/fireworks/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> fireworks","text":""},{"location":"api/generation/fireworks/#rago.generation.fireworks","title":"fireworks","text":"<p>FireworksGen class for text generation using Fireworks API.</p> <p>Classes:</p> <ul> <li> <code>FireworksGen</code>           \u2013            <p>Fireworks AI generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/fireworks/#rago.generation.fireworks.FireworksGen","title":"FireworksGen","text":"<pre><code>FireworksGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Fireworks AI generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Fireworks AI's API.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/fireworks/#rago.generation.fireworks.FireworksGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Fireworks AI's API.</p> Source code in <code>src/rago/generation/fireworks.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Fireworks AI's API.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    api_params = self.api_params or self.default_api_params\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = {\n        'model': self.model_name,\n        'messages': messages,\n        'max_tokens': self.output_max_length,\n        'temperature': self.temperature,\n        **api_params,\n    }\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n        response = self.model.chat.completions.create(**model_params)\n        self.logs['model_params'] = model_params\n        return cast(BaseModel, response)\n\n    response = self.model.chat.completions.create(**model_params)\n    self.logs['model_params'] = model_params\n    return cast(str, response.choices[0].message.content.strip())\n</code></pre>"},{"location":"api/generation/gemini/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> gemini","text":""},{"location":"api/generation/gemini/#rago.generation.gemini","title":"gemini","text":"<p>GeminiGen class for text generation using Google's Gemini model.</p> <p>Classes:</p> <ul> <li> <code>GeminiGen</code>           \u2013            <p>Gemini generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/gemini/#rago.generation.gemini.GeminiGen","title":"GeminiGen","text":"<pre><code>GeminiGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Gemini generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Gemini model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/gemini/#rago.generation.gemini.GeminiGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Gemini model support.</p> Source code in <code>src/rago/generation/gemini.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Gemini model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.structured_output:\n        models_params_gen = {'contents': input_text}\n        response = self.model.generate_content(**models_params_gen)\n        self.logs['model_params'] = models_params_gen\n        return cast(str, response.text.strip())\n\n    api_params = (\n        self.api_params if self.api_params else self.default_api_params\n    )\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = {\n        'messages': messages,\n        'response_model': self.structured_output,\n        **api_params,\n    }\n\n    response = self.model.create(\n        **model_params,\n    )\n\n    self.logs['model_params'] = model_params\n\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/groq/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> groq","text":""},{"location":"api/generation/groq/#rago.generation.groq","title":"groq","text":"<p>Groq class for text generation.</p> <p>Classes:</p> <ul> <li> <code>GroqGen</code>           \u2013            <p>Groq generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/groq/#rago.generation.groq.GroqGen","title":"GroqGen","text":"<pre><code>GroqGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Groq generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using the Groq AP.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/groq/#rago.generation.groq.GroqGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using the Groq AP.</p> Source code in <code>src/rago/generation/groq.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text using the Groq AP.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    api_params = (\n        self.api_params if self.api_params else self.default_api_params\n    )\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = dict(\n        model=self.model_name,\n        messages=messages,\n        max_completion_tokens=self.output_max_length,\n        temperature=self.temperature,\n        **api_params,\n    )\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n\n    response = self.model.chat.completions.create(**model_params)\n    self.logs['model_params'] = model_params\n\n    if hasattr(response, 'choices') and isinstance(response.choices, list):\n        return cast(str, response.choices[0].message.content.strip())\n\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/hugging_face/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> hugging_face","text":""},{"location":"api/generation/hugging_face/#rago.generation.hugging_face","title":"hugging_face","text":"<p>Hugging Face classes for text generation.</p> <p>Classes:</p> <ul> <li> <code>HuggingFaceGen</code>           \u2013            <p>HuggingFaceGen.</p> </li> </ul>"},{"location":"api/generation/hugging_face/#rago.generation.hugging_face.HuggingFaceGen","title":"HuggingFaceGen","text":"<pre><code>HuggingFaceGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>HuggingFaceGen.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate the text from the query and augmented context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/hugging_face/#rago.generation.hugging_face.HuggingFaceGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate the text from the query and augmented context.</p> Source code in <code>src/rago/generation/hugging_face.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate the text from the query and augmented context.\"\"\"\n    with torch.no_grad():\n        input_text = self.prompt_template.format(\n            query=query, context=' '.join(context)\n        )\n        input_ids = self.tokenizer.encode(\n            input_text,\n            return_tensors='pt',\n            truncation=True,\n            max_length=512,\n        ).to(self.device_name)\n\n        api_params = (\n            self.api_params if self.api_params else self.default_api_params\n        )\n\n        model_params = dict(\n            inputs=input_ids,\n            max_length=self.output_max_length,\n            pad_token_id=self.tokenizer.eos_token_id,\n            **api_params,\n        )\n\n        outputs = self.model.generate(**model_params)\n\n        self.logs['model_params'] = model_params\n\n        response = self.tokenizer.decode(\n            outputs[0], skip_special_tokens=True\n        )\n\n    if self.device_name == 'cuda':\n        torch.cuda.empty_cache()\n\n    return str(response)\n</code></pre>"},{"location":"api/generation/hugging_face_inf/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> hugging_face_inf","text":""},{"location":"api/generation/hugging_face_inf/#rago.generation.hugging_face_inf","title":"hugging_face_inf","text":"<p>Hugging Face inferencing classes for text generation.</p> <p>Classes:</p> <ul> <li> <code>HuggingFaceInfGen</code>           \u2013            <p>HuggingFaceGen with InferenceClient.</p> </li> </ul>"},{"location":"api/generation/hugging_face_inf/#rago.generation.hugging_face_inf.HuggingFaceInfGen","title":"HuggingFaceInfGen","text":"<pre><code>HuggingFaceInfGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>HuggingFaceGen with InferenceClient.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate the text from the query and augmented context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/hugging_face_inf/#rago.generation.hugging_face_inf.HuggingFaceInfGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate the text from the query and augmented context.</p> Source code in <code>src/rago/generation/hugging_face_inf.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate the text from the query and augmented context.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n    if self.system_message:\n        input_text = f'{self.system_message}\\n{input_text}'\n\n    api_params = self.api_params or self.default_api_params\n\n    self.logs['model_params'] = {\n        'model': self.model_name,\n        'inputs': input_text,\n        'parameters': api_params,\n    }\n    generated_text = self.client.text_generation(\n        prompt=input_text,\n        model=self.model_name,\n        max_new_tokens=api_params['max_new_tokens'],\n        temperature=api_params['temperature'],\n    )\n\n    return generated_text.strip()\n</code></pre>"},{"location":"api/generation/llama/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> llama","text":""},{"location":"api/generation/llama/#rago.generation.llama","title":"llama","text":"<p>Llama generation module.</p> <p>Classes:</p> <ul> <li> <code>LlamaGen</code>           \u2013            <p>Llama Generation class.</p> </li> <li> <code>OllamaGen</code>           \u2013            <p>Ollama Generation class for local inference via ollama-python.</p> </li> <li> <code>OllamaOpenAIGen</code>           \u2013            <p>OllamaGen via the Ollama Python client.</p> </li> </ul>"},{"location":"api/generation/llama/#rago.generation.llama.LlamaGen","title":"LlamaGen","text":"<pre><code>LlamaGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Llama Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Llama model with language support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/llama/#rago.generation.llama.LlamaGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Llama model with language support.</p> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Llama model with language support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    # Detect and set the language code for multilingual models (optional)\n    language = str(detect(query)) or 'en'\n    self.tokenizer.lang_code = language\n\n    api_params = (\n        self.api_params if self.api_params else self.default_api_params\n    )\n\n    # Generate the response with adjusted parameters\n\n    model_params = dict(\n        text_inputs=input_text,\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n        eos_token_id=self.tokenizer.eos_token_id,\n        **api_params,\n    )\n    response = self.generator(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    # Extract and return the answer only\n    answer = str(response[0].get('generated_text', ''))\n    # Strip off any redundant text after the answer itself\n    return answer.split('Answer:')[-1].strip()\n</code></pre>"},{"location":"api/generation/llama/#rago.generation.llama.OllamaGen","title":"OllamaGen","text":"<pre><code>OllamaGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Ollama Generation class for local inference via ollama-python.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text by sending a prompt to the local Ollama model.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/llama/#rago.generation.llama.OllamaGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text by sending a prompt to the local Ollama model.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The user query.</p> </li> <li> <code>context</code>               (<code>list[str]</code>)           \u2013            <p>Augmented context strings.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The generated response text.</p> </li> </ul> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"\n    Generate text by sending a prompt to the local Ollama model.\n\n    Parameters\n    ----------\n    query : str\n        The user query.\n    context : list[str]\n        Augmented context strings.\n\n    Returns\n    -------\n    str\n        The generated response text.\n    \"\"\"\n    input_text = self.prompt_template.format(\n        query=query,\n        context=' '.join(context),\n    )\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    params = {\n        'model': self.model_name,\n        'messages': messages,\n        **(self.api_params or {}),\n    }\n    response = self.model.chat(**params)\n    return str(response.message.content).strip()\n</code></pre>"},{"location":"api/generation/llama/#rago.generation.llama.OllamaOpenAIGen","title":"OllamaOpenAIGen","text":"<pre><code>OllamaOpenAIGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>OpenAIGen</code></p> <p>OllamaGen via the Ollama Python client.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using OpenAI's API with dynamic model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/llama/#rago.generation.llama.OllamaOpenAIGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using OpenAI's API with dynamic model support.</p> Source code in <code>src/rago/generation/openai.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text using OpenAI's API with dynamic model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = dict(\n        model=self.model_name,\n        messages=messages,\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n        **self.api_params,\n    )\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n\n    response = self.model.chat.completions.create(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    has_choices = hasattr(response, 'choices')\n\n    if has_choices and isinstance(response.choices, list):\n        return cast(str, response.choices[0].message.content.strip())\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/openai/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> openai","text":""},{"location":"api/generation/openai/#rago.generation.openai","title":"openai","text":"<p>OpenAI Generation Model class for flexible GPT-based text generation.</p> <p>Classes:</p> <ul> <li> <code>OpenAIGen</code>           \u2013            <p>OpenAI generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/openai/#rago.generation.openai.OpenAIGen","title":"OpenAIGen","text":"<pre><code>OpenAIGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>OpenAI generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using OpenAI's API with dynamic model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/openai/#rago.generation.openai.OpenAIGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using OpenAI's API with dynamic model support.</p> Source code in <code>src/rago/generation/openai.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str | BaseModel:\n    \"\"\"Generate text using OpenAI's API with dynamic model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = dict(\n        model=self.model_name,\n        messages=messages,\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n        **self.api_params,\n    )\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n\n    response = self.model.chat.completions.create(**model_params)\n\n    self.logs['model_params'] = model_params\n\n    has_choices = hasattr(response, 'choices')\n\n    if has_choices and isinstance(response.choices, list):\n        return cast(str, response.choices[0].message.content.strip())\n    return cast(BaseModel, response)\n</code></pre>"},{"location":"api/generation/phi/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> phi","text":""},{"location":"api/generation/phi/#rago.generation.phi","title":"phi","text":"<p>Phi generation module.</p> <p>Classes:</p> <ul> <li> <code>PhiGen</code>           \u2013            <p>Phi Generation class.</p> </li> </ul>"},{"location":"api/generation/phi/#rago.generation.phi.PhiGen","title":"PhiGen","text":"<pre><code>PhiGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Phi Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Phi model with context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/phi/#rago.generation.phi.PhiGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Phi model with context.</p> Source code in <code>src/rago/generation/phi.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Phi model with context.\"\"\"\n    full_prompt = f'{query}\\nContext: {\" \".join(context)}'\n\n    inputs = self.tokenizer(\n        full_prompt, return_tensors='pt', return_attention_mask=True\n    ).to(self.model.device)\n\n    model_params = dict(\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n        top_p=self.default_api_params['top_p'],\n        num_return_sequences=self.default_api_params[\n            'num_return_sequences'\n        ],\n    )\n\n    self.logs['model_params'] = model_params\n\n    outputs = self.model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        **model_params,\n    )\n\n    answer: str = self.tokenizer.decode(\n        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n    )\n\n    return answer.strip()\n</code></pre>"},{"location":"api/generation/together/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> together","text":""},{"location":"api/generation/together/#rago.generation.together","title":"together","text":"<p>TogetherGen class for text generation using Together AI's API.</p> <p>Classes:</p> <ul> <li> <code>TogetherGen</code>           \u2013            <p>Together AI generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/together/#rago.generation.together.TogetherGen","title":"TogetherGen","text":"<pre><code>TogetherGen(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Together AI generation model for text generation.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Together AI's API.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n    structured_output: Optional[Type[BaseModel]] = None,\n    system_message: str = '',\n    api_params: dict[str, Any] = DEFAULT_API_PARAMS,\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize Generation class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n\n    self.model_name: str = (\n        model_name if model_name is not None else self.default_model_name\n    )\n    self.output_max_length: int = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature: float = (\n        temperature\n        if temperature is not None\n        else self.default_temperature\n    )\n\n    self.prompt_template: str = (\n        prompt_template or self.default_prompt_template\n    )\n    self.structured_output: Optional[Type[BaseModel]] = structured_output\n    if api_params is DEFAULT_API_PARAMS:\n        api_params = deepcopy(self.default_api_params or {})\n\n    self.system_message = system_message\n    self.api_params = api_params\n\n    if device not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {device} not supported. Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name: str = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/together/#rago.generation.together.TogetherGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str | BaseModel\n</code></pre> <p>Generate text using Together AI's API.</p> Source code in <code>src/rago/generation/together.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str | BaseModel:\n    \"\"\"Generate text using Together AI's API.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    api_params = self.api_params or self.default_api_params\n\n    messages = []\n    if self.system_message:\n        messages.append({'role': 'system', 'content': self.system_message})\n    messages.append({'role': 'user', 'content': input_text})\n\n    model_params = {\n        'model': self.model_name,\n        'messages': messages,\n        'max_tokens': self.output_max_length,\n        'temperature': self.temperature,\n        **api_params,\n    }\n\n    if self.structured_output:\n        model_params['response_model'] = self.structured_output\n        response = self.model.chat.completions.create(**model_params)\n        self.logs['model_params'] = model_params\n        return cast(BaseModel, response)\n\n    response = self.model.chat.completions.create(**model_params)\n    self.logs['model_params'] = model_params\n    return cast(str, response.choices[0].message.content.strip())\n</code></pre>"},{"location":"api/retrieval/","title":"Index","text":""},{"location":"api/retrieval/#rago.retrieval","title":"retrieval","text":"<p>RAG Retrieval package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for retrieval.</p> </li> <li> <code>file</code>           \u2013            <p>Base classes for retrieval.</p> </li> <li> <code>text_splitter</code>           \u2013            <p>Package for classes about text splitter.</p> </li> <li> <code>tools</code>           \u2013            <p>Tools for support retrieval classes.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>PDFPathRet</code>           \u2013            <p>PDFPathRet Retrieval class.</p> </li> <li> <code>RetrievalBase</code>           \u2013            <p>Base Retrieval class.</p> </li> <li> <code>StringRet</code>           \u2013            <p>String Retrieval class.</p> </li> </ul>"},{"location":"api/retrieval/#rago.retrieval.PDFPathRet","title":"PDFPathRet","text":"<pre><code>PDFPathRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>FilePathRet</code></p> <p>PDFPathRet Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.PDFPathRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/file.py</code> <pre><code>def get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    cache_key = self.source\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(Iterable[str], cached)\n\n    text = extract_text_from_pdf(self.source)\n\n    self.logs['text'] = text\n\n    result = self.splitter.split(text)\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.RetrievalBase","title":"RetrievalBase","text":"<pre><code>RetrievalBase(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Base Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.RetrievalBase.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>@abstractmethod\ndef get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.StringRet","title":"StringRet","text":"<pre><code>StringRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RetrievalBase</code></p> <p>String Retrieval class.</p> <p>This is a very generic class that assumes that the input (source) is already a list of strings.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the sources.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.StringRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the sources.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the sources.\"\"\"\n    return cast(list[str], self.source)\n</code></pre>"},{"location":"api/retrieval/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/retrieval/base/#rago.retrieval.base","title":"base","text":"<p>Base classes for retrieval.</p> <p>Classes:</p> <ul> <li> <code>RetrievalBase</code>           \u2013            <p>Base Retrieval class.</p> </li> <li> <code>StringRet</code>           \u2013            <p>String Retrieval class.</p> </li> </ul>"},{"location":"api/retrieval/base/#rago.retrieval.base.RetrievalBase","title":"RetrievalBase","text":"<pre><code>RetrievalBase(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RagoBase</code></p> <p>Base Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/base/#rago.retrieval.base.RetrievalBase.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>@abstractmethod\ndef get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/base/#rago.retrieval.base.StringRet","title":"StringRet","text":"<pre><code>StringRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RetrievalBase</code></p> <p>String Retrieval class.</p> <p>This is a very generic class that assumes that the input (source) is already a list of strings.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the sources.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/base/#rago.retrieval.base.StringRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the sources.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the sources.\"\"\"\n    return cast(list[str], self.source)\n</code></pre>"},{"location":"api/retrieval/file/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> file","text":""},{"location":"api/retrieval/file/#rago.retrieval.file","title":"file","text":"<p>Base classes for retrieval.</p> <p>Classes:</p> <ul> <li> <code>FilePathRet</code>           \u2013            <p>File Retrieval class.</p> </li> <li> <code>PDFPathRet</code>           \u2013            <p>PDFPathRet Retrieval class.</p> </li> </ul>"},{"location":"api/retrieval/file/#rago.retrieval.file.FilePathRet","title":"FilePathRet","text":"<pre><code>FilePathRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>RetrievalBase</code></p> <p>File Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/file/#rago.retrieval.file.FilePathRet.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>@abstractmethod\ndef get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/file/#rago.retrieval.file.PDFPathRet","title":"PDFPathRet","text":"<pre><code>PDFPathRet(\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n)\n</code></pre> <p>               Bases: <code>FilePathRet</code></p> <p>PDFPathRet Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the source.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(\n    self,\n    source: Any,\n    splitter: TextSplitterBase = LangChainTextSplitter(\n        'RecursiveCharacterTextSplitter'\n    ),\n    api_key: str = '',\n    cache: Optional[Cache] = None,\n    logs: dict[str, Any] = DEFAULT_LOGS,\n) -&gt; None:\n    \"\"\"Initialize the Retrieval class.\"\"\"\n    if logs is DEFAULT_LOGS:\n        logs = {}\n    super().__init__(api_key=api_key, cache=cache, logs=logs)\n    self.source = source\n    self.splitter = splitter\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/file/#rago.retrieval.file.PDFPathRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; Iterable[str]\n</code></pre> <p>Get the data from the source.</p> Source code in <code>src/rago/retrieval/file.py</code> <pre><code>def get(self, query: str = '') -&gt; Iterable[str]:\n    \"\"\"Get the data from the source.\"\"\"\n    cache_key = self.source\n    cached = self._get_cache(cache_key)\n    if cached is not None:\n        return cast(Iterable[str], cached)\n\n    text = extract_text_from_pdf(self.source)\n\n    self.logs['text'] = text\n\n    result = self.splitter.split(text)\n\n    self._save_cache(cache_key, result)\n\n    return result\n</code></pre>"},{"location":"api/retrieval/text_splitter/","title":"Index","text":""},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter","title":"text_splitter","text":"<p>Package for classes about text splitter.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>The base classes for text splitter.</p> </li> <li> <code>langchain</code>           \u2013            <p>Support langchain text splitter.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>LangChainTextSplitter</code>           \u2013            <p>LangChain Text Splitter class.</p> </li> <li> <code>TextSplitterBase</code>           \u2013            <p>The base text splitter class.</p> </li> </ul>"},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter.LangChainTextSplitter","title":"LangChainTextSplitter","text":"<pre><code>LangChainTextSplitter(\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n)\n</code></pre> <p>               Bases: <code>TextSplitterBase</code></p> <p>LangChain Text Splitter class.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split text into smaller chunks for processing.</p> </li> </ul> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>def __init__(\n    self,\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n) -&gt; None:\n    \"\"\"Initialize the text splitter class.\"\"\"\n    self.chunk_size = chunk_size or self.default_chunk_size\n    self.chunk_overlap = chunk_overlap or self.default_chunk_overlap\n    self.splitter_name = splitter_name or self.default_splitter_name\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter.LangChainTextSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; list[str]\n</code></pre> <p>Split text into smaller chunks for processing.</p> Source code in <code>src/rago/retrieval/text_splitter/langchain.py</code> <pre><code>def split(self, text: str) -&gt; list[str]:\n    \"\"\"Split text into smaller chunks for processing.\"\"\"\n    text_splitter = self.splitter(\n        chunk_size=self.chunk_size,\n        chunk_overlap=self.chunk_overlap,\n        length_function=len,\n        is_separator_regex=True,\n    )\n    return cast(List[str], text_splitter.split_text(text))\n</code></pre>"},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter.TextSplitterBase","title":"TextSplitterBase","text":"<pre><code>TextSplitterBase(\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n)\n</code></pre> <p>The base text splitter class.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split a text into chunks.</p> </li> </ul> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>def __init__(\n    self,\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n) -&gt; None:\n    \"\"\"Initialize the text splitter class.\"\"\"\n    self.chunk_size = chunk_size or self.default_chunk_size\n    self.chunk_overlap = chunk_overlap or self.default_chunk_overlap\n    self.splitter_name = splitter_name or self.default_splitter_name\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/text_splitter/#rago.retrieval.text_splitter.TextSplitterBase.split","title":"split  <code>abstractmethod</code>","text":"<pre><code>split(text: str) -&gt; Iterable[str]\n</code></pre> <p>Split a text into chunks.</p> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>@abstractmethod\ndef split(self, text: str) -&gt; Iterable[str]:\n    \"\"\"Split a text into chunks.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/text_splitter/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/retrieval/text_splitter/base/#rago.retrieval.text_splitter.base","title":"base","text":"<p>The base classes for text splitter.</p> <p>Classes:</p> <ul> <li> <code>TextSplitterBase</code>           \u2013            <p>The base text splitter class.</p> </li> </ul>"},{"location":"api/retrieval/text_splitter/base/#rago.retrieval.text_splitter.base.TextSplitterBase","title":"TextSplitterBase","text":"<pre><code>TextSplitterBase(\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n)\n</code></pre> <p>The base text splitter class.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split a text into chunks.</p> </li> </ul> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>def __init__(\n    self,\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n) -&gt; None:\n    \"\"\"Initialize the text splitter class.\"\"\"\n    self.chunk_size = chunk_size or self.default_chunk_size\n    self.chunk_overlap = chunk_overlap or self.default_chunk_overlap\n    self.splitter_name = splitter_name or self.default_splitter_name\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/text_splitter/base/#rago.retrieval.text_splitter.base.TextSplitterBase.split","title":"split  <code>abstractmethod</code>","text":"<pre><code>split(text: str) -&gt; Iterable[str]\n</code></pre> <p>Split a text into chunks.</p> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>@abstractmethod\ndef split(self, text: str) -&gt; Iterable[str]:\n    \"\"\"Split a text into chunks.\"\"\"\n    return []\n</code></pre>"},{"location":"api/retrieval/text_splitter/langchain/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> langchain","text":""},{"location":"api/retrieval/text_splitter/langchain/#rago.retrieval.text_splitter.langchain","title":"langchain","text":"<p>Support langchain text splitter.</p> <p>Classes:</p> <ul> <li> <code>LangChainTextSplitter</code>           \u2013            <p>LangChain Text Splitter class.</p> </li> </ul>"},{"location":"api/retrieval/text_splitter/langchain/#rago.retrieval.text_splitter.langchain.LangChainTextSplitter","title":"LangChainTextSplitter","text":"<pre><code>LangChainTextSplitter(\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n)\n</code></pre> <p>               Bases: <code>TextSplitterBase</code></p> <p>LangChain Text Splitter class.</p> <p>Methods:</p> <ul> <li> <code>split</code>             \u2013              <p>Split text into smaller chunks for processing.</p> </li> </ul> Source code in <code>src/rago/retrieval/text_splitter/base.py</code> <pre><code>def __init__(\n    self,\n    splitter_name: str = '',\n    chunk_size: int = 500,\n    chunk_overlap: int = 100,\n) -&gt; None:\n    \"\"\"Initialize the text splitter class.\"\"\"\n    self.chunk_size = chunk_size or self.default_chunk_size\n    self.chunk_overlap = chunk_overlap or self.default_chunk_overlap\n    self.splitter_name = splitter_name or self.default_splitter_name\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/retrieval/text_splitter/langchain/#rago.retrieval.text_splitter.langchain.LangChainTextSplitter.split","title":"split","text":"<pre><code>split(text: str) -&gt; list[str]\n</code></pre> <p>Split text into smaller chunks for processing.</p> Source code in <code>src/rago/retrieval/text_splitter/langchain.py</code> <pre><code>def split(self, text: str) -&gt; list[str]:\n    \"\"\"Split text into smaller chunks for processing.\"\"\"\n    text_splitter = self.splitter(\n        chunk_size=self.chunk_size,\n        chunk_overlap=self.chunk_overlap,\n        length_function=len,\n        is_separator_regex=True,\n    )\n    return cast(List[str], text_splitter.split_text(text))\n</code></pre>"},{"location":"api/retrieval/tools/","title":"Index","text":""},{"location":"api/retrieval/tools/#rago.retrieval.tools","title":"tools","text":"<p>Tools for support retrieval classes.</p> <p>Modules:</p> <ul> <li> <code>pdf</code>           \u2013            <p>PDF tools.</p> </li> </ul>"},{"location":"api/retrieval/tools/pdf/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> pdf","text":""},{"location":"api/retrieval/tools/pdf/#rago.retrieval.tools.pdf","title":"pdf","text":"<p>PDF tools.</p> <p>Functions:</p> <ul> <li> <code>extract_text_from_pdf</code>             \u2013              <p>Extract text from a PDF file using pypdf.</p> </li> <li> <code>is_pdf</code>             \u2013              <p>Check if a file is a PDF by reading its header.</p> </li> </ul>"},{"location":"api/retrieval/tools/pdf/#rago.retrieval.tools.pdf.extract_text_from_pdf","title":"extract_text_from_pdf","text":"<pre><code>extract_text_from_pdf(file_path: str) -&gt; str\n</code></pre> <p>Extract text from a PDF file using pypdf.</p> <p>The result is the same as the one returned by PyPDFLoader.</p> Source code in <code>src/rago/retrieval/tools/pdf.py</code> <pre><code>def extract_text_from_pdf(file_path: str) -&gt; str:\n    \"\"\"\n    Extract text from a PDF file using pypdf.\n\n    The result is the same as the one returned by PyPDFLoader.\n    \"\"\"\n    reader = PdfReader(file_path)\n    pages = []\n    for page in reader.pages:\n        text = page.extract_text()\n        if text:\n            pages.append(text)\n    return ' '.join(pages)\n</code></pre>"},{"location":"api/retrieval/tools/pdf/#rago.retrieval.tools.pdf.is_pdf","title":"is_pdf","text":"<pre><code>is_pdf(file_path: str | Path) -&gt; bool\n</code></pre> <p>Check if a file is a PDF by reading its header.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>Path to the file to be checked.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the file is a PDF, False otherwise.</p> </li> </ul> Source code in <code>src/rago/retrieval/tools/pdf.py</code> <pre><code>def is_pdf(file_path: str | Path) -&gt; bool:\n    \"\"\"\n    Check if a file is a PDF by reading its header.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the file to be checked.\n\n    Returns\n    -------\n    bool\n        True if the file is a PDF, False otherwise.\n    \"\"\"\n    try:\n        with open(file_path, 'rb') as file:\n            header = file.read(4)\n            return header == b'%PDF'\n    except IOError:\n        return False\n</code></pre>"}]}