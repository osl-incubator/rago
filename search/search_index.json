{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#rago","title":"Rago","text":"<p>Rago is a lightweight framework for RAG</p> <ul> <li>License: BSD 3 Clause</li> <li>Documentation: https://rago.github.io</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li> <p>The security of our code: Bandit is a powerful tool that we use in our Python   project to ensure its security. This tool analyzes the code and detects   potential vulnerabilities. Some of the key features of Bandit are its ease of   use, its ability to integrate with other tools, and its support for multiple   Python versions. If you want to know about bandit you can check its   documentation.</p> </li> <li> <p>Finds unused code: Vulture is   useful for cleaning up and finding errors in large code bases in Python.</p> </li> <li> <p>Complexity of functions and modules: We use   McCabe to identify the complexity in our   Python code that may be difficult to maintain or understand. By identifying   complex code at the outset, we as developers can refactor it to make it easier   to maintain and understand. In summary, McCabe helps us to improve the quality   of our code and make it easier to maintain. If you would like to learn more   about McCabe and code complexity, you can visit   McCabe - Code Complexity Checker.   This tool is included with Flake8.</p> </li> <li> <p>TODO</p> </li> </ul>"},{"location":"#credits","title":"Credits","text":"<p>This package was created with Cookiecutter and the osl-incubator/scicookie project template.</p>"},{"location":"changelog/","title":"Release Notes","text":""},{"location":"changelog/#040-2024-10-31","title":"0.4.0 (2024-10-31)","text":""},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix packaging issues with torch and fix issues with langdetect (#10) (e4ade58)</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Implement Gemini Model Integration for Augmentation and Generation (#16) (b418c9b)</li> <li>model: Integrate GPT-4 text generation in Rago (#13) (42078a3)</li> </ul>"},{"location":"changelog/#030-2024-10-24","title":"0.3.0 (2024-10-24)","text":""},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Fix packaging for gpu and cpu (#8) (42e0716)</li> <li>Improve the usage of typeguard (#5) (25782c1)</li> </ul>"},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li>Add parameter for selecting device (cpu and gpu) (#6) (552d912)</li> <li>Implement type checking with typeguard (#4) (ae7958e)</li> </ul>"},{"location":"changelog/#020-2024-10-23","title":"0.2.0 (2024-10-23)","text":""},{"location":"changelog/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>fix the semantic release workflow (#3) (b238e46)</li> </ul>"},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li>Add support for llama 3.2 generation (#2) (770dd0c)</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>In order to be able to contribute, it is important that you understand the project layout.</p> <p>This project uses the src layout, which means that the package code is located at <code>./src/rago</code>.</p> <p>For my information, check the official documentation: https://packaging.python.org/en/latest/discussions/src-layout-vs-flat-layout/</p> <p>In addition, you should know that to build our package we use Poetry, it's a Python package management tool that simplifies the process of building and publishing Python packages. It allows us to easily manage dependencies, virtual environments and package versions. Poetry also includes features such as dependency resolution, lock files and publishing to PyPI. Overall, Poetry streamlines the process of managing Python packages, making it easier for us to create and share our code with others.</p> <p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/osl-incubator/rago.git/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \u201cbug\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \u201cenhancement\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>Rago could always use more documentation, whether as part of the official Rago docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/osl-incubator/rago.git/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are   welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started","text":"<p>Ready to contribute? Here\u2019s how to set up <code>rago</code> for local development.</p> <ol> <li>Fork the <code>rago</code> repo on GitHub.</li> <li>Clone your fork locally and change to the directory of your project:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/rago.git\n$ cd rago/\n</code></pre> <p>Also, create a remote to the upstream repository, you will need that later:</p> <pre><code>$ git remote add upstream https://github.com/osl-incubator/rago.git\n$ git fetch --all\n</code></pre>"},{"location":"contributing/#prepare-and-use-virtual-environment","title":"Prepare and use virtual environment","text":"<p>If you don't have yet conda installed in your machine, you can check the installation steps here: conda-forge/miniforge?tab=readme-ov-file#download After that, ensure that conda is already available in your terminal session and run:</p> <pre><code>$ conda env create env create --file conda/dev.yaml\n$ conda activate rago\n</code></pre> <p>Note: you can use <code>mamba env create</code> instead, if you have it already installed, in order to boost the installation step.</p>"},{"location":"contributing/#install-the-dependencies","title":"Install the dependencies","text":"<p>Now, you can already install the dependencies for the project:</p> <pre><code>$ poetry install\n</code></pre> <ul> <li><code>rago</code> uses a set of <code>pre-commit</code> hooks to improve code quality. The hooks can   be installed locally using:</li> </ul> <pre><code>$ pre-commit install\n</code></pre> <p>This would run the checks every time a <code>git commit</code> is executed locally. Usually, the verification will only run on the files modified by that commit, but the verification can also be triggered for all the files using:</p> <pre><code>$ pre-commit run --all-files\n</code></pre> <p>If you would like to skip the failing checks and push the code for further discussion, use the <code>--no-verify</code> option with <code>git commit</code>.</p> <p>This project uses <code>pytest</code> as a testing tool. <code>pytest</code> is responsible for testing the code, whose configuration is available in pyproject.toml. Additionally, this project also uses <code>pytest-cov</code> to calculate the coverage of these unit tests. For more information, check the section about tests later in this document.</p>"},{"location":"contributing/#commit-your-changes-and-push-your-branch-to-github","title":"Commit your changes and push your branch to GitHub","text":"<pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ul> <li>Submit a pull request through the GitHub website.</li> </ul>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put your    new functionality into a function with a docstring, and add the feature to    the list in README.rst.</li> <li>The pull request should work for Python &gt;= 3.8.</li> </ol>"},{"location":"contributing/#running-tests-locally","title":"Running tests locally","text":"<p>The tests can be executed using the <code>test</code> dependencies of <code>rago</code> in the following way:</p> <pre><code>$ python -m pytest\n</code></pre>"},{"location":"contributing/#running-tests-with-coverage-locally","title":"Running tests with coverage locally","text":"<p>The coverage value can be obtained while running the tests using <code>pytest-cov</code> in the following way:</p> <pre><code>$ python -m pytest --cov=rago tests/\n</code></pre> <p>A much more detailed guide on testing with <code>pytest</code> is available here.</p>"},{"location":"contributing/#automation-tasks-with-makim","title":"Automation Tasks with Makim","text":"<p>This project uses <code>makim</code> as an automation tool. Please, check the <code>.makim.yaml</code> file to check all the tasks available or run:</p> <pre><code>$ makim --help\n</code></pre>"},{"location":"contributing/#release","title":"Release","text":"<p>This project uses semantic-release in order to cut a new release based on the commit-message.</p>"},{"location":"contributing/#commit-message-format","title":"Commit message format","text":"<p>semantic-release uses the commit messages to determine the consumer impact of changes in the codebase. Following formalized conventions for commit messages, semantic-release automatically determines the next semantic version number, generates a changelog and publishes the release.</p> <p>By default, semantic-release uses Angular Commit Message Conventions. The commit message format can be changed with the <code>preset</code> or <code>config</code> options_ of the @semantic-release/commit-analyzer and @semantic-release/release-notes-generator plugins.</p> <p>Tools such as commitizen or commitlint can be used to help contributors and enforce valid commit messages.</p> <p>The table below shows which commit message gets you which release type when <code>semantic-release</code> runs (using the default configuration):</p> Commit message Release type <code>fix(pencil): stop graphite breaking when pressure is applied</code> Fix Release <code>feat(pencil): add 'graphiteWidth' option</code> Feature Release <code>perf(pencil): remove graphiteWidth option</code> Chore <code>feat(pencil)!: The graphiteWidth option has been removed</code> Breaking Release <p>Note: For a breaking change release, uses <code>!</code> at the end of the message prefix.</p> <p>source: https://github.com/semantic-release/semantic-release/blob/master/README.md#commit-message-format</p> <p>As this project uses the <code>squash and merge</code> strategy, ensure to apply the commit message format to the PR's title.</p>"},{"location":"example/","title":"Rago","text":"In\u00a0[\u00a0]: Copied! <pre>import rago\n</pre> import rago In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/#rago","title":"Rago\u00b6","text":"<p>Rago is Python library that aims to do ...</p>"},{"location":"example/#getting-started","title":"Getting Started\u00b6","text":"<p>First, check our documentation about the installation.</p> <p>Now, let's import our library:</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install Rago, run this command in your terminal:</p> <pre><code>pip install rago\n</code></pre> <p>This is the preferred method to install Rago, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>The sources for Rago can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>git clone https://github.com/osl-incubator/rago.git\n</code></pre> <p>Or download the tarball:</p> <pre><code>curl -OJL https://github.com/osl-incubator/rago.git/tarball/main\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>poetry install\n</code></pre>"},{"location":"api/","title":"Index","text":""},{"location":"api/#rago","title":"rago","text":"<p>Rago.</p> <p>Modules:</p> <ul> <li> <code>augmented</code>           \u2013            <p>Augmented package.</p> </li> <li> <code>core</code>           \u2013            <p>Rago is Retrieval Augmented Generation lightweight framework.</p> </li> <li> <code>db</code>           \u2013            <p>Rago DB package.</p> </li> <li> <code>generation</code>           \u2013            <p>RAG Generation package.</p> </li> <li> <code>retrieval</code>           \u2013            <p>RAG Retrieval package.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>Rago</code>           \u2013            <p>RAG class.</p> </li> </ul>"},{"location":"api/#rago.Rago","title":"Rago","text":"<pre><code>Rago(retrieval: RetrievalBase, augmented: AugmentedBase, generation: GenerationBase)\n</code></pre> <p>RAG class.</p> <p>Parameters:</p> <ul> <li> <code>retrieval</code>               (<code>RetrievalBase</code>)           \u2013            <p>The retrieval component used to fetch relevant data based on the query.</p> </li> <li> <code>augmented</code>               (<code>AugmentedBase</code>)           \u2013            <p>The augmentation module responsible for enriching the retrieved data.</p> </li> <li> <code>generation</code>               (<code>GenerationBase</code>)           \u2013            <p>The text generation model used to generate a response based on the query and augmented data.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>prompt</code>             \u2013              <p>Run the pipeline for a specific prompt.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def __init__(\n    self,\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n) -&gt; None:\n    \"\"\"Initialize the RAG structure.\n\n    Parameters\n    ----------\n    retrieval : RetrievalBase\n        The retrieval component used to fetch relevant data based\n        on the query.\n    augmented : AugmentedBase\n        The augmentation module responsible for enriching the\n        retrieved data.\n    generation : GenerationBase\n        The text generation model used to generate a response based\n        on the query and augmented data.\n    \"\"\"\n    self.retrieval = retrieval\n    self.augmented = augmented\n    self.generation = generation\n</code></pre>"},{"location":"api/#rago.Rago.prompt","title":"prompt","text":"<pre><code>prompt(query: str, device: str = 'auto') -&gt; str\n</code></pre> <p>Run the pipeline for a specific prompt.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query or prompt from the user.</p> </li> <li> <code>device</code>               (<code>str (default 'auto')</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device for generation (e.g., 'auto', 'cpu', 'cuda'), by default 'auto'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on the query and augmented data.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def prompt(self, query: str, device: str = 'auto') -&gt; str:\n    \"\"\"Run the pipeline for a specific prompt.\n\n    Parameters\n    ----------\n    query : str\n        The query or prompt from the user.\n    device : str (default 'auto')\n        Device for generation (e.g., 'auto', 'cpu', 'cuda'), by\n        default 'auto'.\n\n    Returns\n    -------\n    str\n        Generated text based on the query and augmented data.\n    \"\"\"\n    ret_data = self.retrieval.get(query)\n    aug_data = self.augmented.search(query, ret_data)\n    gen_data: str = self.generation.generate(query, context=aug_data)\n\n    return gen_data\n</code></pre>"},{"location":"api/#rago.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the program version.</p> Source code in <code>src/rago/__init__.py</code> <pre><code>def get_version() -&gt; str:\n    \"\"\"Return the program version.\"\"\"\n    try:\n        return importlib_metadata.version(__name__)\n    except importlib_metadata.PackageNotFoundError:  # pragma: no cover\n        return '0.4.0'  # semantic-release\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li> rago<ul> <li> augmented<ul> <li> base</li> <li> gemini</li> <li> hugging_face</li> <li> openai</li> </ul> </li> <li> core</li> <li> db<ul> <li> base</li> <li> faiss</li> </ul> </li> <li> generation<ul> <li> base</li> <li> gemini</li> <li> hugging_face</li> <li> llama</li> <li> openai</li> </ul> </li> <li> retrieval<ul> <li> base</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/core/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> core","text":""},{"location":"api/core/#rago.core","title":"core","text":"<p>Rago is Retrieval Augmented Generation lightweight framework.</p> <p>Classes:</p> <ul> <li> <code>Rago</code>           \u2013            <p>RAG class.</p> </li> </ul>"},{"location":"api/core/#rago.core.Rago","title":"Rago","text":"<pre><code>Rago(retrieval: RetrievalBase, augmented: AugmentedBase, generation: GenerationBase)\n</code></pre> <p>RAG class.</p> <p>Parameters:</p> <ul> <li> <code>retrieval</code>               (<code>RetrievalBase</code>)           \u2013            <p>The retrieval component used to fetch relevant data based on the query.</p> </li> <li> <code>augmented</code>               (<code>AugmentedBase</code>)           \u2013            <p>The augmentation module responsible for enriching the retrieved data.</p> </li> <li> <code>generation</code>               (<code>GenerationBase</code>)           \u2013            <p>The text generation model used to generate a response based on the query and augmented data.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>prompt</code>             \u2013              <p>Run the pipeline for a specific prompt.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def __init__(\n    self,\n    retrieval: RetrievalBase,\n    augmented: AugmentedBase,\n    generation: GenerationBase,\n) -&gt; None:\n    \"\"\"Initialize the RAG structure.\n\n    Parameters\n    ----------\n    retrieval : RetrievalBase\n        The retrieval component used to fetch relevant data based\n        on the query.\n    augmented : AugmentedBase\n        The augmentation module responsible for enriching the\n        retrieved data.\n    generation : GenerationBase\n        The text generation model used to generate a response based\n        on the query and augmented data.\n    \"\"\"\n    self.retrieval = retrieval\n    self.augmented = augmented\n    self.generation = generation\n</code></pre>"},{"location":"api/core/#rago.core.Rago.prompt","title":"prompt","text":"<pre><code>prompt(query: str, device: str = 'auto') -&gt; str\n</code></pre> <p>Run the pipeline for a specific prompt.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query or prompt from the user.</p> </li> <li> <code>device</code>               (<code>str (default 'auto')</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device for generation (e.g., 'auto', 'cpu', 'cuda'), by default 'auto'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on the query and augmented data.</p> </li> </ul> Source code in <code>src/rago/core.py</code> <pre><code>def prompt(self, query: str, device: str = 'auto') -&gt; str:\n    \"\"\"Run the pipeline for a specific prompt.\n\n    Parameters\n    ----------\n    query : str\n        The query or prompt from the user.\n    device : str (default 'auto')\n        Device for generation (e.g., 'auto', 'cpu', 'cuda'), by\n        default 'auto'.\n\n    Returns\n    -------\n    str\n        Generated text based on the query and augmented data.\n    \"\"\"\n    ret_data = self.retrieval.get(query)\n    aug_data = self.augmented.search(query, ret_data)\n    gen_data: str = self.generation.generate(query, context=aug_data)\n\n    return gen_data\n</code></pre>"},{"location":"api/augmented/","title":"Index","text":""},{"location":"api/augmented/#rago.augmented","title":"augmented","text":"<p>Augmented package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for the augmented step.</p> </li> <li> <code>gemini</code>           \u2013            <p>GeminiAug class for query augmentation using Google's Gemini Model.</p> </li> <li> <code>hugging_face</code>           \u2013            <p>Classes for augmentation with hugging face.</p> </li> <li> <code>openai</code>           \u2013            <p>OpenAIAug class for query augmentation using OpenAI API.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>AugmentedBase</code>           \u2013            <p>Define the base structure for Augmented classes.</p> </li> <li> <code>GeminiAug</code>           \u2013            <p>GeminiAug class for query augmentation using Gemini API.</p> </li> <li> <code>HuggingFaceAug</code>           \u2013            <p>Class for augmentation with Hugging Face.</p> </li> <li> <code>OpenAIAug</code>           \u2013            <p>OpenAIAug class for query augmentation using OpenAI API.</p> </li> </ul>"},{"location":"api/augmented/#rago.augmented.AugmentedBase","title":"AugmentedBase","text":"<pre><code>AugmentedBase(model_name: str = '', api_key: str = '', db: DBBase = FaissDB(), k: int = 0, temperature: float = 0.5, prompt_template: str = '', result_separator: str = '\\n', output_max_length: int = 500)\n</code></pre> <p>Define the base structure for Augmented classes.</p> <p>Methods:</p> <ul> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    db: DBBase = FaissDB(),\n    k: int = 0,\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    result_separator: str = '\\n',\n    output_max_length: int = 500,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    self.db = db\n    self.api_key = api_key\n\n    self.k = k or self.default_k\n    self.model_name = model_name or self.default_model_name\n    self.temperature = temperature or self.default_temperature\n    self.result_separator = (\n        result_separator or self.default_result_separator\n    )\n    self.prompt_template = prompt_template or self.default_prompt_template\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.AugmentedBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(query: str, documents: Any, k: int = 0) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self,\n    query: str,\n    documents: Any,\n    k: int = 0,\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/#rago.augmented.GeminiAug","title":"GeminiAug","text":"<pre><code>GeminiAug(model_name: str = '', api_key: str = '', db: DBBase = FaissDB(), k: int = 0, temperature: float = 0.5, prompt_template: str = '', result_separator: str = '\\n', output_max_length: int = 500)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>GeminiAug class for query augmentation using Gemini API.</p> <p>Methods:</p> <ul> <li> <code>search</code>             \u2013              <p>Augment the query by expanding or rephrasing it using Gemini.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    db: DBBase = FaissDB(),\n    k: int = 0,\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    result_separator: str = '\\n',\n    output_max_length: int = 500,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    self.db = db\n    self.api_key = api_key\n\n    self.k = k or self.default_k\n    self.model_name = model_name or self.default_model_name\n    self.temperature = temperature or self.default_temperature\n    self.result_separator = (\n        result_separator or self.default_result_separator\n    )\n    self.prompt_template = prompt_template or self.default_prompt_template\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.GeminiAug.search","title":"search","text":"<pre><code>search(query: str, documents: list[str], k: int = 0) -&gt; list[str]\n</code></pre> <p>Augment the query by expanding or rephrasing it using Gemini.</p> Source code in <code>src/rago/augmented/gemini.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], k: int = 0\n) -&gt; list[str]:\n    \"\"\"Augment the query by expanding or rephrasing it using Gemini.\"\"\"\n    k = k or self.k\n    prompt = self.prompt_template.format(\n        query=query, context=' '.join(documents), k=k\n    )\n\n    response = genai.GenerativeModel(self.model_name).generate_content(\n        prompt\n    )\n\n    augmented_query = str(\n        response.text.strip()\n        if hasattr(response, 'text')\n        else response[0].text.strip()\n    )\n    return augmented_query.split(self.result_separator)[:k]\n</code></pre>"},{"location":"api/augmented/#rago.augmented.HuggingFaceAug","title":"HuggingFaceAug","text":"<pre><code>HuggingFaceAug(model_name: str = '', api_key: str = '', db: DBBase = FaissDB(), k: int = 0, temperature: float = 0.5, prompt_template: str = '', result_separator: str = '\\n', output_max_length: int = 500)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Hugging Face.</p> <p>Methods:</p> <ul> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    db: DBBase = FaissDB(),\n    k: int = 0,\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    result_separator: str = '\\n',\n    output_max_length: int = 500,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    self.db = db\n    self.api_key = api_key\n\n    self.k = k or self.default_k\n    self.model_name = model_name or self.default_model_name\n    self.temperature = temperature or self.default_temperature\n    self.result_separator = (\n        result_separator or self.default_result_separator\n    )\n    self.prompt_template = prompt_template or self.default_prompt_template\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.HuggingFaceAug.search","title":"search","text":"<pre><code>search(query: str, documents: Any, k: int = 0) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/hugging_face.py</code> <pre><code>def search(self, query: str, documents: Any, k: int = 0) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    document_encoded = self.model.encode(documents)\n    query_encoded = self.model.encode([query])\n    k = k if k &gt; 0 else self.k\n\n    self.db.embed(document_encoded)\n\n    _, indices = self.db.search(query_encoded, k=k)\n\n    retrieved_docs = [documents[i] for i in indices]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/#rago.augmented.OpenAIAug","title":"OpenAIAug","text":"<pre><code>OpenAIAug(model_name: str = '', api_key: str = '', db: DBBase = FaissDB(), k: int = 0, temperature: float = 0.5, prompt_template: str = '', result_separator: str = '\\n', output_max_length: int = 500)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>OpenAIAug class for query augmentation using OpenAI API.</p> <p>Methods:</p> <ul> <li> <code>search</code>             \u2013              <p>Augment the query by expanding or rephrasing it using OpenAI.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    db: DBBase = FaissDB(),\n    k: int = 0,\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    result_separator: str = '\\n',\n    output_max_length: int = 500,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    self.db = db\n    self.api_key = api_key\n\n    self.k = k or self.default_k\n    self.model_name = model_name or self.default_model_name\n    self.temperature = temperature or self.default_temperature\n    self.result_separator = (\n        result_separator or self.default_result_separator\n    )\n    self.prompt_template = prompt_template or self.default_prompt_template\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/#rago.augmented.OpenAIAug.search","title":"search","text":"<pre><code>search(query: str, documents: list[str], k: int = 0) -&gt; list[str]\n</code></pre> <p>Augment the query by expanding or rephrasing it using OpenAI.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], k: int = 0\n) -&gt; list[str]:\n    \"\"\"Augment the query by expanding or rephrasing it using OpenAI.\"\"\"\n    k = k or self.k\n    prompt = self.prompt_template.format(\n        query=query, context=' '.join(documents), k=k\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    response = self.model.chat.completions.create(\n        model=self.model_name,\n        messages=[{'role': 'user', 'content': prompt}],\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n    )\n\n    augmented_query = cast(\n        str, response.choices[0].message.content.strip()\n    )\n    return augmented_query.split(self.result_separator)[:k]\n</code></pre>"},{"location":"api/augmented/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/augmented/base/#rago.augmented.base","title":"base","text":"<p>Base classes for the augmented step.</p> <p>Classes:</p> <ul> <li> <code>AugmentedBase</code>           \u2013            <p>Define the base structure for Augmented classes.</p> </li> </ul>"},{"location":"api/augmented/base/#rago.augmented.base.AugmentedBase","title":"AugmentedBase","text":"<pre><code>AugmentedBase(model_name: str = '', api_key: str = '', db: DBBase = FaissDB(), k: int = 0, temperature: float = 0.5, prompt_template: str = '', result_separator: str = '\\n', output_max_length: int = 500)\n</code></pre> <p>Define the base structure for Augmented classes.</p> <p>Methods:</p> <ul> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    db: DBBase = FaissDB(),\n    k: int = 0,\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    result_separator: str = '\\n',\n    output_max_length: int = 500,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    self.db = db\n    self.api_key = api_key\n\n    self.k = k or self.default_k\n    self.model_name = model_name or self.default_model_name\n    self.temperature = temperature or self.default_temperature\n    self.result_separator = (\n        result_separator or self.default_result_separator\n    )\n    self.prompt_template = prompt_template or self.default_prompt_template\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/base/#rago.augmented.base.AugmentedBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(query: str, documents: Any, k: int = 0) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self,\n    query: str,\n    documents: Any,\n    k: int = 0,\n) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/augmented/gemini/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> gemini","text":""},{"location":"api/augmented/gemini/#rago.augmented.gemini","title":"gemini","text":"<p>GeminiAug class for query augmentation using Google's Gemini Model.</p> <p>Classes:</p> <ul> <li> <code>GeminiAug</code>           \u2013            <p>GeminiAug class for query augmentation using Gemini API.</p> </li> </ul>"},{"location":"api/augmented/gemini/#rago.augmented.gemini.GeminiAug","title":"GeminiAug","text":"<pre><code>GeminiAug(model_name: str = '', api_key: str = '', db: DBBase = FaissDB(), k: int = 0, temperature: float = 0.5, prompt_template: str = '', result_separator: str = '\\n', output_max_length: int = 500)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>GeminiAug class for query augmentation using Gemini API.</p> <p>Methods:</p> <ul> <li> <code>search</code>             \u2013              <p>Augment the query by expanding or rephrasing it using Gemini.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    db: DBBase = FaissDB(),\n    k: int = 0,\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    result_separator: str = '\\n',\n    output_max_length: int = 500,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    self.db = db\n    self.api_key = api_key\n\n    self.k = k or self.default_k\n    self.model_name = model_name or self.default_model_name\n    self.temperature = temperature or self.default_temperature\n    self.result_separator = (\n        result_separator or self.default_result_separator\n    )\n    self.prompt_template = prompt_template or self.default_prompt_template\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/gemini/#rago.augmented.gemini.GeminiAug.search","title":"search","text":"<pre><code>search(query: str, documents: list[str], k: int = 0) -&gt; list[str]\n</code></pre> <p>Augment the query by expanding or rephrasing it using Gemini.</p> Source code in <code>src/rago/augmented/gemini.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], k: int = 0\n) -&gt; list[str]:\n    \"\"\"Augment the query by expanding or rephrasing it using Gemini.\"\"\"\n    k = k or self.k\n    prompt = self.prompt_template.format(\n        query=query, context=' '.join(documents), k=k\n    )\n\n    response = genai.GenerativeModel(self.model_name).generate_content(\n        prompt\n    )\n\n    augmented_query = str(\n        response.text.strip()\n        if hasattr(response, 'text')\n        else response[0].text.strip()\n    )\n    return augmented_query.split(self.result_separator)[:k]\n</code></pre>"},{"location":"api/augmented/hugging_face/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> hugging_face","text":""},{"location":"api/augmented/hugging_face/#rago.augmented.hugging_face","title":"hugging_face","text":"<p>Classes for augmentation with hugging face.</p> <p>Classes:</p> <ul> <li> <code>HuggingFaceAug</code>           \u2013            <p>Class for augmentation with Hugging Face.</p> </li> </ul>"},{"location":"api/augmented/hugging_face/#rago.augmented.hugging_face.HuggingFaceAug","title":"HuggingFaceAug","text":"<pre><code>HuggingFaceAug(model_name: str = '', api_key: str = '', db: DBBase = FaissDB(), k: int = 0, temperature: float = 0.5, prompt_template: str = '', result_separator: str = '\\n', output_max_length: int = 500)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>Class for augmentation with Hugging Face.</p> <p>Methods:</p> <ul> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    db: DBBase = FaissDB(),\n    k: int = 0,\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    result_separator: str = '\\n',\n    output_max_length: int = 500,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    self.db = db\n    self.api_key = api_key\n\n    self.k = k or self.default_k\n    self.model_name = model_name or self.default_model_name\n    self.temperature = temperature or self.default_temperature\n    self.result_separator = (\n        result_separator or self.default_result_separator\n    )\n    self.prompt_template = prompt_template or self.default_prompt_template\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/hugging_face/#rago.augmented.hugging_face.HuggingFaceAug.search","title":"search","text":"<pre><code>search(query: str, documents: Any, k: int = 0) -&gt; list[str]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/augmented/hugging_face.py</code> <pre><code>def search(self, query: str, documents: Any, k: int = 0) -&gt; list[str]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    document_encoded = self.model.encode(documents)\n    query_encoded = self.model.encode([query])\n    k = k if k &gt; 0 else self.k\n\n    self.db.embed(document_encoded)\n\n    _, indices = self.db.search(query_encoded, k=k)\n\n    retrieved_docs = [documents[i] for i in indices]\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/augmented/openai/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> openai","text":""},{"location":"api/augmented/openai/#rago.augmented.openai","title":"openai","text":"<p>OpenAIAug class for query augmentation using OpenAI API.</p> <p>Classes:</p> <ul> <li> <code>OpenAIAug</code>           \u2013            <p>OpenAIAug class for query augmentation using OpenAI API.</p> </li> </ul>"},{"location":"api/augmented/openai/#rago.augmented.openai.OpenAIAug","title":"OpenAIAug","text":"<pre><code>OpenAIAug(model_name: str = '', api_key: str = '', db: DBBase = FaissDB(), k: int = 0, temperature: float = 0.5, prompt_template: str = '', result_separator: str = '\\n', output_max_length: int = 500)\n</code></pre> <p>               Bases: <code>AugmentedBase</code></p> <p>OpenAIAug class for query augmentation using OpenAI API.</p> <p>Methods:</p> <ul> <li> <code>search</code>             \u2013              <p>Augment the query by expanding or rephrasing it using OpenAI.</p> </li> </ul> Source code in <code>src/rago/augmented/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    db: DBBase = FaissDB(),\n    k: int = 0,\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    result_separator: str = '\\n',\n    output_max_length: int = 500,\n) -&gt; None:\n    \"\"\"Initialize AugmentedBase.\"\"\"\n    self.db = db\n    self.api_key = api_key\n\n    self.k = k or self.default_k\n    self.model_name = model_name or self.default_model_name\n    self.temperature = temperature or self.default_temperature\n    self.result_separator = (\n        result_separator or self.default_result_separator\n    )\n    self.prompt_template = prompt_template or self.default_prompt_template\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n\n    self.model = None\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/augmented/openai/#rago.augmented.openai.OpenAIAug.search","title":"search","text":"<pre><code>search(query: str, documents: list[str], k: int = 0) -&gt; list[str]\n</code></pre> <p>Augment the query by expanding or rephrasing it using OpenAI.</p> Source code in <code>src/rago/augmented/openai.py</code> <pre><code>def search(\n    self, query: str, documents: list[str], k: int = 0\n) -&gt; list[str]:\n    \"\"\"Augment the query by expanding or rephrasing it using OpenAI.\"\"\"\n    k = k or self.k\n    prompt = self.prompt_template.format(\n        query=query, context=' '.join(documents), k=k\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    response = self.model.chat.completions.create(\n        model=self.model_name,\n        messages=[{'role': 'user', 'content': prompt}],\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n    )\n\n    augmented_query = cast(\n        str, response.choices[0].message.content.strip()\n    )\n    return augmented_query.split(self.result_separator)[:k]\n</code></pre>"},{"location":"api/db/","title":"Index","text":""},{"location":"api/db/#rago.db","title":"db","text":"<p>Rago DB package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for database.</p> </li> <li> <code>faiss</code>           \u2013            <p>Module for faiss database.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>DBBase</code>           \u2013            <p>Base class for vector database.</p> </li> <li> <code>FaissDB</code>           \u2013            <p>Faiss Database.</p> </li> </ul>"},{"location":"api/db/#rago.db.DBBase","title":"DBBase","text":"<p>Base class for vector database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search a query from documents.</p> </li> </ul>"},{"location":"api/db/#rago.db.DBBase.embed","title":"embed  <code>abstractmethod</code>","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/db/base.py</code> <pre><code>@abstractmethod\ndef embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/db/#rago.db.DBBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(query_encoded: Any, k: int = 2) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search a query from documents.</p> Source code in <code>src/rago/db/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self, query_encoded: Any, k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search a query from documents.\"\"\"\n    ...\n</code></pre>"},{"location":"api/db/#rago.db.FaissDB","title":"FaissDB","text":"<p>               Bases: <code>DBBase</code></p> <p>Faiss Database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul>"},{"location":"api/db/#rago.db.FaissDB.embed","title":"embed","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/db/faiss.py</code> <pre><code>def embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    self.index = faiss.IndexFlatL2(documents.shape[1])\n    self.index.add(documents)\n</code></pre>"},{"location":"api/db/#rago.db.FaissDB.search","title":"search","text":"<pre><code>search(query_encoded: Any, k: int = 2) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/db/faiss.py</code> <pre><code>def search(\n    self, query_encoded: Any, k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    distances, indices = self.index.search(query_encoded, k)\n    return distances, indices[0]\n</code></pre>"},{"location":"api/db/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/db/base/#rago.db.base","title":"base","text":"<p>Base classes for database.</p> <p>Classes:</p> <ul> <li> <code>DBBase</code>           \u2013            <p>Base class for vector database.</p> </li> </ul>"},{"location":"api/db/base/#rago.db.base.DBBase","title":"DBBase","text":"<p>Base class for vector database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search a query from documents.</p> </li> </ul>"},{"location":"api/db/base/#rago.db.base.DBBase.embed","title":"embed  <code>abstractmethod</code>","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/db/base.py</code> <pre><code>@abstractmethod\ndef embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    ...\n</code></pre>"},{"location":"api/db/base/#rago.db.base.DBBase.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(query_encoded: Any, k: int = 2) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search a query from documents.</p> Source code in <code>src/rago/db/base.py</code> <pre><code>@abstractmethod\ndef search(\n    self, query_encoded: Any, k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search a query from documents.\"\"\"\n    ...\n</code></pre>"},{"location":"api/db/faiss/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> faiss","text":""},{"location":"api/db/faiss/#rago.db.faiss","title":"faiss","text":"<p>Module for faiss database.</p> <p>Classes:</p> <ul> <li> <code>FaissDB</code>           \u2013            <p>Faiss Database.</p> </li> </ul>"},{"location":"api/db/faiss/#rago.db.faiss.FaissDB","title":"FaissDB","text":"<p>               Bases: <code>DBBase</code></p> <p>Faiss Database.</p> <p>Methods:</p> <ul> <li> <code>embed</code>             \u2013              <p>Embed the documents into the database.</p> </li> <li> <code>search</code>             \u2013              <p>Search an encoded query into vector database.</p> </li> </ul>"},{"location":"api/db/faiss/#rago.db.faiss.FaissDB.embed","title":"embed","text":"<pre><code>embed(documents: Any) -&gt; None\n</code></pre> <p>Embed the documents into the database.</p> Source code in <code>src/rago/db/faiss.py</code> <pre><code>def embed(self, documents: Any) -&gt; None:\n    \"\"\"Embed the documents into the database.\"\"\"\n    self.index = faiss.IndexFlatL2(documents.shape[1])\n    self.index.add(documents)\n</code></pre>"},{"location":"api/db/faiss/#rago.db.faiss.FaissDB.search","title":"search","text":"<pre><code>search(query_encoded: Any, k: int = 2) -&gt; tuple[Iterable[float], Iterable[int]]\n</code></pre> <p>Search an encoded query into vector database.</p> Source code in <code>src/rago/db/faiss.py</code> <pre><code>def search(\n    self, query_encoded: Any, k: int = 2\n) -&gt; tuple[Iterable[float], Iterable[int]]:\n    \"\"\"Search an encoded query into vector database.\"\"\"\n    distances, indices = self.index.search(query_encoded, k)\n    return distances, indices[0]\n</code></pre>"},{"location":"api/generation/","title":"Index","text":""},{"location":"api/generation/#rago.generation","title":"generation","text":"<p>RAG Generation package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for generation.</p> </li> <li> <code>gemini</code>           \u2013            <p>GeminiGen class for text generation using Google's Gemini model.</p> </li> <li> <code>hugging_face</code>           \u2013            <p>Hugging Face classes for text generation.</p> </li> <li> <code>llama</code>           \u2013            <p>Llama generation module.</p> </li> <li> <code>openai</code>           \u2013            <p>OpenAI Generation Model class for flexible GPT-based text generation.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>GeminiGen</code>           \u2013            <p>Gemini generation model for text generation.</p> </li> <li> <code>GenerationBase</code>           \u2013            <p>Generic Generation class.</p> </li> <li> <code>HuggingFaceGen</code>           \u2013            <p>HuggingFaceGen.</p> </li> <li> <code>LlamaGen</code>           \u2013            <p>Llama Generation class.</p> </li> <li> <code>OpenAIGen</code>           \u2013            <p>OpenAI generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/#rago.generation.GeminiGen","title":"GeminiGen","text":"<pre><code>GeminiGen(model_name: str = '', api_key: str = '', temperature: float = 0.5, prompt_template: str = '', output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Gemini generation model for text generation.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            </li> <li> <code>prompt_template</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>output_max_length</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum length of the generated output.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            </li> </ul> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Gemini model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize Generation class.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    api_key : str\n    temperature : float\n    prompt_template: str\n    output_max_length : int\n        Maximum length of the generated output.\n    device: str (default=auto)\n    \"\"\"\n    self.api_key = api_key\n    self.model_name = model_name or self.default_model_name\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature = temperature or self.default_temperature\n\n    self.prompt_template = prompt_template or self.default_prompt_template\n\n    if self.device_name not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {self.device_name} not supported. '\n            'Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.GeminiGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Gemini model support.</p> Source code in <code>src/rago/generation/gemini.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Gemini model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    response = self.model.generate_content(input_text)\n    return cast(str, response.text.strip())\n</code></pre>"},{"location":"api/generation/#rago.generation.GenerationBase","title":"GenerationBase","text":"<pre><code>GenerationBase(model_name: str = '', api_key: str = '', temperature: float = 0.5, prompt_template: str = '', output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>Generic Generation class.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            </li> <li> <code>prompt_template</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>output_max_length</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum length of the generated output.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            </li> </ul> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text with optional language parameter.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize Generation class.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    api_key : str\n    temperature : float\n    prompt_template: str\n    output_max_length : int\n        Maximum length of the generated output.\n    device: str (default=auto)\n    \"\"\"\n    self.api_key = api_key\n    self.model_name = model_name or self.default_model_name\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature = temperature or self.default_temperature\n\n    self.prompt_template = prompt_template or self.default_prompt_template\n\n    if self.device_name not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {self.device_name} not supported. '\n            'Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.GenerationBase.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text with optional language parameter.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The input query or prompt.</p> </li> <li> <code>context</code>               (<code>list[str]</code>)           \u2013            <p>Additional context information for the generation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on query and context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str:\n    \"\"\"Generate text with optional language parameter.\n\n    Parameters\n    ----------\n    query : str\n        The input query or prompt.\n    context : list[str]\n        Additional context information for the generation.\n\n    Returns\n    -------\n    str\n        Generated text based on query and context.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/generation/#rago.generation.HuggingFaceGen","title":"HuggingFaceGen","text":"<pre><code>HuggingFaceGen(model_name: str = '', api_key: str = '', temperature: float = 0.5, prompt_template: str = '', output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>HuggingFaceGen.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            </li> <li> <code>prompt_template</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>output_max_length</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum length of the generated output.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            </li> </ul> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate the text from the query and augmented context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize Generation class.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    api_key : str\n    temperature : float\n    prompt_template: str\n    output_max_length : int\n        Maximum length of the generated output.\n    device: str (default=auto)\n    \"\"\"\n    self.api_key = api_key\n    self.model_name = model_name or self.default_model_name\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature = temperature or self.default_temperature\n\n    self.prompt_template = prompt_template or self.default_prompt_template\n\n    if self.device_name not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {self.device_name} not supported. '\n            'Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.HuggingFaceGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate the text from the query and augmented context.</p> Source code in <code>src/rago/generation/hugging_face.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate the text from the query and augmented context.\"\"\"\n    with torch.no_grad():\n        input_text = self.prompt_template.format(\n            query=query, context=' '.join(context)\n        )\n        input_ids = self.tokenizer.encode(\n            input_text,\n            return_tensors='pt',\n            truncation=True,\n            max_length=512,\n        ).to(self.device_name)\n\n        outputs = self.model.generate(\n            input_ids,\n            max_length=self.output_max_length,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n        response = self.tokenizer.decode(\n            outputs[0], skip_special_tokens=True\n        )\n\n    if self.device_name == 'cuda':\n        torch.cuda.empty_cache()\n\n    return str(response)\n</code></pre>"},{"location":"api/generation/#rago.generation.LlamaGen","title":"LlamaGen","text":"<pre><code>LlamaGen(model_name: str = 'meta-llama/Llama-3.2-1B', api_key: str = '', temperature: float = 0.5, output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Llama Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Llama model with language support.</p> </li> </ul> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = 'meta-llama/Llama-3.2-1B',\n    api_key: str = '',\n    temperature: float = 0.5,\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize LlamaGen.\"\"\"\n    if not model_name.startswith('meta-llama/'):\n        raise Exception(\n            f'The given model name {model_name} is not provided by meta.'\n        )\n\n    super().__init__(\n        model_name=model_name,\n        api_key=api_key,\n        temperature=temperature,\n        output_max_length=output_max_length,\n        device=device,\n    )\n\n    self.tokenizer = AutoTokenizer.from_pretrained(\n        model_name, token=api_key\n    )\n\n    self.model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        token=api_key,\n        torch_dtype=torch.float16\n        if self.device_name == 'cuda'\n        else torch.float32,\n    )\n\n    self.generator = pipeline(\n        'text-generation',\n        model=self.model,\n        tokenizer=self.tokenizer,\n        device=0 if self.device_name == 'cuda' else -1,\n    )\n</code></pre>"},{"location":"api/generation/#rago.generation.LlamaGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Llama model with language support.</p> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Llama model with language support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    # Detect and set the language code for multilingual models (optional)\n    language = str(detect(query)) or 'en'\n    self.tokenizer.lang_code = language\n\n    # Generate the response with adjusted parameters\n    response = self.generator(\n        input_text,\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n        top_k=50,\n        top_p=0.95,\n        num_return_sequences=1,\n        eos_token_id=self.tokenizer.eos_token_id,\n    )\n\n    # Extract and return the answer only\n    answer = str(response[0].get('generated_text', ''))\n    # Strip off any redundant text after the answer itself\n    return answer.split('Answer:')[-1].strip()\n</code></pre>"},{"location":"api/generation/#rago.generation.OpenAIGen","title":"OpenAIGen","text":"<pre><code>OpenAIGen(model_name: str = '', api_key: str = '', temperature: float = 0.5, prompt_template: str = '', output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>OpenAI generation model for text generation.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            </li> <li> <code>prompt_template</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>output_max_length</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum length of the generated output.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            </li> </ul> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using OpenAI's API with dynamic model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize Generation class.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    api_key : str\n    temperature : float\n    prompt_template: str\n    output_max_length : int\n        Maximum length of the generated output.\n    device: str (default=auto)\n    \"\"\"\n    self.api_key = api_key\n    self.model_name = model_name or self.default_model_name\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature = temperature or self.default_temperature\n\n    self.prompt_template = prompt_template or self.default_prompt_template\n\n    if self.device_name not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {self.device_name} not supported. '\n            'Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/#rago.generation.OpenAIGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using OpenAI's API with dynamic model support.</p> Source code in <code>src/rago/generation/openai.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str:\n    \"\"\"Generate text using OpenAI's API with dynamic model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    response = self.model.chat.completions.create(\n        model=self.model_name,\n        messages=[{'role': 'user', 'content': input_text}],\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n        top_p=0.9,\n        frequency_penalty=0.5,\n        presence_penalty=0.3,\n    )\n\n    return cast(str, response.choices[0].message.content.strip())\n</code></pre>"},{"location":"api/generation/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/generation/base/#rago.generation.base","title":"base","text":"<p>Base classes for generation.</p> <p>Classes:</p> <ul> <li> <code>GenerationBase</code>           \u2013            <p>Generic Generation class.</p> </li> </ul>"},{"location":"api/generation/base/#rago.generation.base.GenerationBase","title":"GenerationBase","text":"<pre><code>GenerationBase(model_name: str = '', api_key: str = '', temperature: float = 0.5, prompt_template: str = '', output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>Generic Generation class.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            </li> <li> <code>prompt_template</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>output_max_length</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum length of the generated output.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            </li> </ul> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text with optional language parameter.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize Generation class.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    api_key : str\n    temperature : float\n    prompt_template: str\n    output_max_length : int\n        Maximum length of the generated output.\n    device: str (default=auto)\n    \"\"\"\n    self.api_key = api_key\n    self.model_name = model_name or self.default_model_name\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature = temperature or self.default_temperature\n\n    self.prompt_template = prompt_template or self.default_prompt_template\n\n    if self.device_name not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {self.device_name} not supported. '\n            'Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/base/#rago.generation.base.GenerationBase.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text with optional language parameter.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The input query or prompt.</p> </li> <li> <code>context</code>               (<code>list[str]</code>)           \u2013            <p>Additional context information for the generation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Generated text based on query and context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str:\n    \"\"\"Generate text with optional language parameter.\n\n    Parameters\n    ----------\n    query : str\n        The input query or prompt.\n    context : list[str]\n        Additional context information for the generation.\n\n    Returns\n    -------\n    str\n        Generated text based on query and context.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/generation/gemini/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> gemini","text":""},{"location":"api/generation/gemini/#rago.generation.gemini","title":"gemini","text":"<p>GeminiGen class for text generation using Google's Gemini model.</p> <p>Classes:</p> <ul> <li> <code>GeminiGen</code>           \u2013            <p>Gemini generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/gemini/#rago.generation.gemini.GeminiGen","title":"GeminiGen","text":"<pre><code>GeminiGen(model_name: str = '', api_key: str = '', temperature: float = 0.5, prompt_template: str = '', output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Gemini generation model for text generation.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            </li> <li> <code>prompt_template</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>output_max_length</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum length of the generated output.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            </li> </ul> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Gemini model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize Generation class.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    api_key : str\n    temperature : float\n    prompt_template: str\n    output_max_length : int\n        Maximum length of the generated output.\n    device: str (default=auto)\n    \"\"\"\n    self.api_key = api_key\n    self.model_name = model_name or self.default_model_name\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature = temperature or self.default_temperature\n\n    self.prompt_template = prompt_template or self.default_prompt_template\n\n    if self.device_name not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {self.device_name} not supported. '\n            'Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/gemini/#rago.generation.gemini.GeminiGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Gemini model support.</p> Source code in <code>src/rago/generation/gemini.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Gemini model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    response = self.model.generate_content(input_text)\n    return cast(str, response.text.strip())\n</code></pre>"},{"location":"api/generation/hugging_face/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> hugging_face","text":""},{"location":"api/generation/hugging_face/#rago.generation.hugging_face","title":"hugging_face","text":"<p>Hugging Face classes for text generation.</p> <p>Classes:</p> <ul> <li> <code>HuggingFaceGen</code>           \u2013            <p>HuggingFaceGen.</p> </li> </ul>"},{"location":"api/generation/hugging_face/#rago.generation.hugging_face.HuggingFaceGen","title":"HuggingFaceGen","text":"<pre><code>HuggingFaceGen(model_name: str = '', api_key: str = '', temperature: float = 0.5, prompt_template: str = '', output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>HuggingFaceGen.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            </li> <li> <code>prompt_template</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>output_max_length</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum length of the generated output.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            </li> </ul> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate the text from the query and augmented context.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize Generation class.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    api_key : str\n    temperature : float\n    prompt_template: str\n    output_max_length : int\n        Maximum length of the generated output.\n    device: str (default=auto)\n    \"\"\"\n    self.api_key = api_key\n    self.model_name = model_name or self.default_model_name\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature = temperature or self.default_temperature\n\n    self.prompt_template = prompt_template or self.default_prompt_template\n\n    if self.device_name not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {self.device_name} not supported. '\n            'Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/hugging_face/#rago.generation.hugging_face.HuggingFaceGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate the text from the query and augmented context.</p> Source code in <code>src/rago/generation/hugging_face.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate the text from the query and augmented context.\"\"\"\n    with torch.no_grad():\n        input_text = self.prompt_template.format(\n            query=query, context=' '.join(context)\n        )\n        input_ids = self.tokenizer.encode(\n            input_text,\n            return_tensors='pt',\n            truncation=True,\n            max_length=512,\n        ).to(self.device_name)\n\n        outputs = self.model.generate(\n            input_ids,\n            max_length=self.output_max_length,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n        response = self.tokenizer.decode(\n            outputs[0], skip_special_tokens=True\n        )\n\n    if self.device_name == 'cuda':\n        torch.cuda.empty_cache()\n\n    return str(response)\n</code></pre>"},{"location":"api/generation/llama/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> llama","text":""},{"location":"api/generation/llama/#rago.generation.llama","title":"llama","text":"<p>Llama generation module.</p> <p>Classes:</p> <ul> <li> <code>LlamaGen</code>           \u2013            <p>Llama Generation class.</p> </li> </ul>"},{"location":"api/generation/llama/#rago.generation.llama.LlamaGen","title":"LlamaGen","text":"<pre><code>LlamaGen(model_name: str = 'meta-llama/Llama-3.2-1B', api_key: str = '', temperature: float = 0.5, output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>Llama Generation class.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using Llama model with language support.</p> </li> </ul> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = 'meta-llama/Llama-3.2-1B',\n    api_key: str = '',\n    temperature: float = 0.5,\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize LlamaGen.\"\"\"\n    if not model_name.startswith('meta-llama/'):\n        raise Exception(\n            f'The given model name {model_name} is not provided by meta.'\n        )\n\n    super().__init__(\n        model_name=model_name,\n        api_key=api_key,\n        temperature=temperature,\n        output_max_length=output_max_length,\n        device=device,\n    )\n\n    self.tokenizer = AutoTokenizer.from_pretrained(\n        model_name, token=api_key\n    )\n\n    self.model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        token=api_key,\n        torch_dtype=torch.float16\n        if self.device_name == 'cuda'\n        else torch.float32,\n    )\n\n    self.generator = pipeline(\n        'text-generation',\n        model=self.model,\n        tokenizer=self.tokenizer,\n        device=0 if self.device_name == 'cuda' else -1,\n    )\n</code></pre>"},{"location":"api/generation/llama/#rago.generation.llama.LlamaGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using Llama model with language support.</p> Source code in <code>src/rago/generation/llama.py</code> <pre><code>def generate(self, query: str, context: list[str]) -&gt; str:\n    \"\"\"Generate text using Llama model with language support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    # Detect and set the language code for multilingual models (optional)\n    language = str(detect(query)) or 'en'\n    self.tokenizer.lang_code = language\n\n    # Generate the response with adjusted parameters\n    response = self.generator(\n        input_text,\n        max_new_tokens=self.output_max_length,\n        do_sample=True,\n        temperature=self.temperature,\n        top_k=50,\n        top_p=0.95,\n        num_return_sequences=1,\n        eos_token_id=self.tokenizer.eos_token_id,\n    )\n\n    # Extract and return the answer only\n    answer = str(response[0].get('generated_text', ''))\n    # Strip off any redundant text after the answer itself\n    return answer.split('Answer:')[-1].strip()\n</code></pre>"},{"location":"api/generation/openai/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> openai","text":""},{"location":"api/generation/openai/#rago.generation.openai","title":"openai","text":"<p>OpenAI Generation Model class for flexible GPT-based text generation.</p> <p>Classes:</p> <ul> <li> <code>OpenAIGen</code>           \u2013            <p>OpenAI generation model for text generation.</p> </li> </ul>"},{"location":"api/generation/openai/#rago.generation.openai.OpenAIGen","title":"OpenAIGen","text":"<pre><code>OpenAIGen(model_name: str = '', api_key: str = '', temperature: float = 0.5, prompt_template: str = '', output_max_length: int = 500, device: str = 'auto')\n</code></pre> <p>               Bases: <code>GenerationBase</code></p> <p>OpenAI generation model for text generation.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            </li> <li> <code>prompt_template</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            </li> <li> <code>output_max_length</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum length of the generated output.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            </li> </ul> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate text using OpenAI's API with dynamic model support.</p> </li> </ul> Source code in <code>src/rago/generation/base.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    model_name: str = '',\n    api_key: str = '',\n    temperature: float = 0.5,\n    prompt_template: str = '',\n    output_max_length: int = 500,\n    device: str = 'auto',\n) -&gt; None:\n    \"\"\"Initialize Generation class.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the model to use.\n    api_key : str\n    temperature : float\n    prompt_template: str\n    output_max_length : int\n        Maximum length of the generated output.\n    device: str (default=auto)\n    \"\"\"\n    self.api_key = api_key\n    self.model_name = model_name or self.default_model_name\n    self.output_max_length = (\n        output_max_length or self.default_output_max_length\n    )\n    self.temperature = temperature or self.default_temperature\n\n    self.prompt_template = prompt_template or self.default_prompt_template\n\n    if self.device_name not in ['cpu', 'cuda', 'auto']:\n        raise Exception(\n            f'Device {self.device_name} not supported. '\n            'Options: cpu, cuda, auto.'\n        )\n\n    cuda_available = torch.cuda.is_available()\n    self.device_name = (\n        'cpu' if device == 'cpu' or not cuda_available else 'cuda'\n    )\n    self.device = torch.device(self.device_name)\n\n    self._validate()\n    self._setup()\n</code></pre>"},{"location":"api/generation/openai/#rago.generation.openai.OpenAIGen.generate","title":"generate","text":"<pre><code>generate(query: str, context: list[str]) -&gt; str\n</code></pre> <p>Generate text using OpenAI's API with dynamic model support.</p> Source code in <code>src/rago/generation/openai.py</code> <pre><code>def generate(\n    self,\n    query: str,\n    context: list[str],\n) -&gt; str:\n    \"\"\"Generate text using OpenAI's API with dynamic model support.\"\"\"\n    input_text = self.prompt_template.format(\n        query=query, context=' '.join(context)\n    )\n\n    if not self.model:\n        raise Exception('The model was not created.')\n\n    response = self.model.chat.completions.create(\n        model=self.model_name,\n        messages=[{'role': 'user', 'content': input_text}],\n        max_tokens=self.output_max_length,\n        temperature=self.temperature,\n        top_p=0.9,\n        frequency_penalty=0.5,\n        presence_penalty=0.3,\n    )\n\n    return cast(str, response.choices[0].message.content.strip())\n</code></pre>"},{"location":"api/retrieval/","title":"Index","text":""},{"location":"api/retrieval/#rago.retrieval","title":"retrieval","text":"<p>RAG Retrieval package.</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for retrieval.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>RetrievalBase</code>           \u2013            <p>Base Retrieval class.</p> </li> <li> <code>StringRet</code>           \u2013            <p>String Retrieval class.</p> </li> </ul>"},{"location":"api/retrieval/#rago.retrieval.RetrievalBase","title":"RetrievalBase","text":"<pre><code>RetrievalBase(sources: Any)\n</code></pre> <p>Base Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the sources.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(self, sources: Any) -&gt; None:\n    self.sources = sources\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.RetrievalBase.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(query: str = '') -&gt; Any\n</code></pre> <p>Get the data from the sources.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>@abstractmethod\ndef get(self, query: str = '') -&gt; Any:\n    \"\"\"Get the data from the sources.\"\"\"\n    ...\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.StringRet","title":"StringRet","text":"<pre><code>StringRet(sources: Any)\n</code></pre> <p>               Bases: <code>RetrievalBase</code></p> <p>String Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the sources.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(self, sources: Any) -&gt; None:\n    self.sources = sources\n</code></pre>"},{"location":"api/retrieval/#rago.retrieval.StringRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; list[str]\n</code></pre> <p>Get the data from the sources.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def get(self, query: str = '') -&gt; list[str]:\n    \"\"\"Get the data from the sources.\"\"\"\n    return cast(list[str], self.sources)\n</code></pre>"},{"location":"api/retrieval/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"api/retrieval/base/#rago.retrieval.base","title":"base","text":"<p>Base classes for retrieval.</p> <p>Classes:</p> <ul> <li> <code>RetrievalBase</code>           \u2013            <p>Base Retrieval class.</p> </li> <li> <code>StringRet</code>           \u2013            <p>String Retrieval class.</p> </li> </ul>"},{"location":"api/retrieval/base/#rago.retrieval.base.RetrievalBase","title":"RetrievalBase","text":"<pre><code>RetrievalBase(sources: Any)\n</code></pre> <p>Base Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the sources.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(self, sources: Any) -&gt; None:\n    self.sources = sources\n</code></pre>"},{"location":"api/retrieval/base/#rago.retrieval.base.RetrievalBase.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(query: str = '') -&gt; Any\n</code></pre> <p>Get the data from the sources.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>@abstractmethod\ndef get(self, query: str = '') -&gt; Any:\n    \"\"\"Get the data from the sources.\"\"\"\n    ...\n</code></pre>"},{"location":"api/retrieval/base/#rago.retrieval.base.StringRet","title":"StringRet","text":"<pre><code>StringRet(sources: Any)\n</code></pre> <p>               Bases: <code>RetrievalBase</code></p> <p>String Retrieval class.</p> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Get the data from the sources.</p> </li> </ul> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def __init__(self, sources: Any) -&gt; None:\n    self.sources = sources\n</code></pre>"},{"location":"api/retrieval/base/#rago.retrieval.base.StringRet.get","title":"get","text":"<pre><code>get(query: str = '') -&gt; list[str]\n</code></pre> <p>Get the data from the sources.</p> Source code in <code>src/rago/retrieval/base.py</code> <pre><code>def get(self, query: str = '') -&gt; list[str]:\n    \"\"\"Get the data from the sources.\"\"\"\n    return cast(list[str], self.sources)\n</code></pre>"}]}